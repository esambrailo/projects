{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "czxW4C_gf5Eo"
   },
   "source": [
    "## DATASCI 290 - GenAI - Assignment 5 - Provided Overview\n",
    "\n",
    "Below is the provided overview for this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oOZF6xBOw9H5"
   },
   "source": [
    "The overall scenarios is as follows:\n",
    "\n",
    "You work at a tech company that is looking for new ways to organize their question answering and search capabilities to accelerate both engineering activity and the marketing team. The company also wants to roll out new GenAI-based products, so a lot of the questions will center around Generative AI concepts. The company has about 300 engineers and a marketing staff of 40. Product releases are done quarterly.\n",
    "\n",
    "Your role is to implement and conduct a (mini-)POC helping the company to evaluate RAG capabilities for the improvement of their document search (and corresponding question answering), supporting particularly the engineering and marketing organizations. You will have a gold dataset with 'good' responses to questions from marketing and engineering teams. You need to develop metric(s) that help you to evaluate how well your RAG system performs relative to the gold data. You should work with the tunables of the setup (LLM, chunking, embeddings, ...) for your iterations.\n",
    "\n",
    "You will also need to write up your findings as a short proposal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qzh1ZtqcigfD"
   },
   "source": [
    "## Model, Parameter, Prompt & Evaluation Options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7JC5XOVeyaH"
   },
   "source": [
    "\n",
    "\n",
    "Below are the various tuning options we are chosing to explore for improving this RAG Chain model.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Embedding Models**\n",
    " - 'multi-qa-mpnet-base-dot-v1'\n",
    " - 'all-MiniLM-L6-v2'\n",
    " - 'avsolatorio/GIST-Embedding-v0'\n",
    "\n",
    "**Splitter Chunk Parameters**\n",
    " - 'CHUNK_SIZE'\n",
    " - 'OVERLAP'\n",
    "\n",
    "**Retriever Parameters**\n",
    " - Num of documents.\n",
    " - Type of serach\n",
    "\n",
    "**LLM Model**\n",
    " - Cohere\n",
    " - 'mistralai/Mistral-7B-Instruct-v0.1'\n",
    "\n",
    "**RAG Prompt Template**\n",
    "\n",
    "**Evaluation Metrics**\n",
    " - BERTScore\n",
    " - BLEU\n",
    " - ROUGE\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tqQJ94pkwkLM",
    "tags": []
   },
   "source": [
    "## 1) Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-T_P6z1xifzi"
   },
   "source": [
    "### 1.A) Provided Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t7lWTKYeINTT"
   },
   "source": [
    "Below is the setup that was provided with the original notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WIlhfQj-KUlZ"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip -q install git+https://github.com/huggingface/transformers\n",
    "!pip install -q datasets loralib sentencepiece\n",
    "!pip -q install bitsandbytes accelerate\n",
    "!pip -q install langchain\n",
    "!pip install einops\n",
    "!pip install faiss-gpu\n",
    "!pip install --upgrade --quiet  langchain-community chromadb bs4 qdrant-client\n",
    "!pip install langchainhub\n",
    "\n",
    "!pip install --upgrade --quiet  wikipedia\n",
    "!pip install --upgrade --quiet  arxiv\n",
    "!pip install --upgrade --quiet  pymupdf\n",
    "\n",
    "!pip install xmltodict\n",
    "\n",
    "!pip install cohere\n",
    "\n",
    "!pip install rouge_score\n",
    "\n",
    "!pip install -U langchain-cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3NjcvYABKieZ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import bs4\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import locale\n",
    "\n",
    "from transformers import AutoTokenizer , AutoModelForCausalLM\n",
    "from transformers import pipeline, BitsAndBytesConfig\n",
    "\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.utils.math import cosine_similarity\n",
    "\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "from langchain_community.document_loaders import OnlinePDFLoader\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.document_loaders import PubMedLoader\n",
    "\n",
    "# from langchain_community.chat_models import ChatCohere\n",
    "from langchain_cohere import ChatCohere\n",
    "\n",
    "from google.colab import userdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "faZ5fLk_xxAO"
   },
   "outputs": [],
   "source": [
    "locale.getpreferredencoding = lambda: \"UTF-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Stlb_ciPxxWA"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xJWr9TkCa7gG"
   },
   "source": [
    "Add your keys from the secret store (do **NOT** print them out or leave them exposed as plaintext in your notebook!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ll9IqkVMa7qP"
   },
   "outputs": [],
   "source": [
    "COHERE_API_KEY = userdata.get('COHERE_PRODUCTION_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TEFbUUSRilLY"
   },
   "source": [
    "### 1.B) Validation Question/Answer Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gvzIcUiZAHCc"
   },
   "source": [
    "**Below is an exact copy of the validation set for evaluation. No changes made, just a copy/paste.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oAJ5ME8OAGYc"
   },
   "outputs": [],
   "source": [
    "validation_questions_answers = {\n",
    "    0: {\"question\": \"What purpose do large language models serve in the field of natural language processing?\",\n",
    "  \"gold_answer_research\": \"Large language models (LLMs) serve the purpose of enabling general-purpose language generation and other natural language processing tasks such as classification. They achieve this by learning statistical relationships from text documents during computationally intensive self-supervised and semi-supervised training. LLMs can be used for text generation by predicting the next token or word, making them valuable for tasks like speech recognition, machine translation, and information retrieval. Additionally, LLMs have superseded previous models like recurrent neural networks, showcasing their efficiency and effectiveness in NLP tasks.\",\n",
    "  \"gold_answer_marketing\": \"Large language models serve the purpose of improving performance in various natural language processing tasks, such as speech recognition, machine translation, natural language generation, optical character recognition, handwriting recognition, grammar induction, and information retrieval.\"},\n",
    "1: {\"question\": \"How does a large language model learn from text during training?\",\n",
    "  \"gold_answer_research\": \"A large language model learns from text during training by first going through an unsupervised generative 'pretraining' stage where it sets initial parameters using a language modeling objective. Then, it goes through a supervised discriminative 'fine-tuning' stage where it refines its parameters based on annotated examples or task demonstrations. This dual-stage approach allows the model to learn statistical relationships from text documents in a computationally intensive process, enabling it to achieve general-purpose language generation and natural language processing tasks.\",\n",
    "  \"gold_answer_marketing\": \"A large language model learns from text during training by first pretraining on a diverse dataset to acquire general language knowledge, and then fine-tuning on specific tasks or demonstrations to adapt its parameters for more targeted performance.\"},\n",
    "2: {\"question\": \"What are some key architectures behind the development of large language models?\",\n",
    "  \"gold_answer_research\": \"Key architectures behind the development of large language models include the use of self-attention mechanisms, such as those seen in Transformer decoders. These architectures have been applied to tasks like autoregressive language modeling and have led to the dominance of Transformer-based language models in NLP. Models like BERT and GPT-2 have further advanced this paradigm, showcasing the power of large Transformer language models in achieving state-of-the-art results across various NLP tasks. Additionally, architectures like neural-retriever-in-the-loop generative-based models have shown improvements in tasks like open-domain QA and knowledge-grounded dialogue, emphasizing the importance of consistent and engaging responses in long-form generation and multi-turn conversations.\",\n",
    "  \"gold_answer_marketing\": \"Key architectures behind the development of large language models include Transformer-based models such as BERT and GPT-2, which utilize self-attention mechanisms for tasks like autoregressive language modeling and knowledge-grounded dialogue. These models have shown significant success in NLP tasks and have led to advancements in general-purpose language generation and natural language processing.\"},\n",
    "3: {\"question\": \"Can you name some specific large language models and the companies or organizations that have developed them?\",\n",
    "  \"gold_answer_research\": \"Some specific large language models include GPT-3 by OpenAI, Chinchilla by DeepMind, and BERT by Google. OpenAI developed GPT-3, DeepMind developed Chinchilla, and Google developed BERT. These models have been significant advancements in the field of natural language processing.\",\n",
    "  \"gold_answer_marketing\": \"Chinchilla by DeepMind, GPT-3 by OpenAI.\"},\n",
    "7: {\"question\": \"What licensing models have been adopted for the distribution of source-available language models?\",\n",
    "  \"gold_answer_research\": \"Based on the provided context, it seems that licensing models for the distribution of source-available language models have not been explicitly discussed in the referenced papers. However, it is crucial to consider potential licensing options such as open-source licenses (e.g., GPL, MIT) or proprietary licenses when distributing language models to ensure legal compliance and control over usage rights. Additionally, considering the implications of different licensing models on accessibility, collaboration, and commercialization is essential for determining the most suitable approach for sharing language models with the community. Further research or consultation with legal experts may be necessary to explore specific licensing strategies for source-available language models.\",\n",
    "  \"gold_answer_marketing\": \"Answer: Some organizations choose open-sourcing, while others restrict access to a few organizations with resources or offer end-to-end deployment via API.\"},\n",
    "8: {\"question\": \"What are language models and what is their purpose in natural language processing?\",\n",
    "  \"gold_answer_research\": \"Language models are probabilistic models of natural language that help predict or correct text. Their purpose in natural language processing is to assist in various tasks such as speech recognition, machine translation, natural language generation, and information retrieval. By analyzing the performance of human subjects, language models improve the understanding and generation of human-like text.\",\n",
    "  \"gold_answer_marketing\": \"Language models are probabilistic models of natural language that are used in tasks such as speech recognition, machine translation, and natural language generation in natural language processing.\"},\n",
    "9: {\"question\": \"How have language models evolved in terms of architecture, from the 1980s to present times?\",\n",
    "  \"gold_answer_research\": \"Language models have evolved significantly in terms of architecture from the 1980s to present times. In the 1980s, the first statistical language model was proposed, leading to experiments by IBM that identified areas for improvement by observing human subjects. However, it wasn't until 2017 when the transformer architecture was introduced by Google, revolutionizing the field. This development paved the way for models like BERT in 2018, which marked a shift towards large-scale transformer-based language models. These modern architectures, based on self-attention mechanisms, have dominated the field of natural language processing, achieving state-of-the-art performance in various tasks.\",\n",
    "  \"gold_answer_marketing\": \"Language models have evolved from early statistical models in the 1980s to modern transformer architectures, such as BERT and GPT-2, which use self-attention mechanisms and have become dominant in natural language processing tasks.\"},\n",
    "11: {\"question\": \"Can you explain how maximum entropy language models work and what the partition function signifies?\",\n",
    "  \"gold_answer_research\": \"Maximum entropy language models use feature functions to encode the relationship between a word and its n-gram history, aiming to maximize reward while satisfying a KL-constrained objective. The partition function, denoted as Z(x), is crucial in normalizing the probabilities of all possible outputs given the input. It represents the sum of the exponential of the reward function over all possible output sequences, making it computationally expensive to estimate but essential for accurate modeling. The partition function ensures that the model's predicted probabilities sum up to 1, providing a foundation for effective language modeling.\",\n",
    "  \"gold_answer_marketing\": \"Maximum entropy language models encode the relationship between a word and the n-gram history using feature functions. The partition function in this context represents the total probability of all possible outcomes, making it a crucial factor in determining the optimal solution for the reward maximization objective.\"},\n",
    "12: {\"question\": \"What is the benefit of using continuous space embeddings in recurrent neural network language models?\",\n",
    "  \"gold_answer_research\": \"Continuous space embeddings in recurrent neural network language models help alleviate the curse of dimensionality by representing words as non-linear combinations of weights in the embedding space. This approach helps address the data sparsity problem caused by the exponential increase in possible word sequences with vocabulary size. By utilizing continuous space embeddings, neural networks can effectively capture semantic relationships and meaning within the language model.\",\n",
    "  \"gold_answer_marketing\": \"Continuous space embeddings in recurrent neural network language models help alleviate the curse of dimensionality caused by the exponential increase in possible word sequences, reducing data sparsity issues.\"},\n",
    "13: {\"question\": \"What challenges do large language models face in mirroring human cognitive patterns?\",\n",
    "  \"gold_answer_research\": \"Large language models face challenges in mirroring human cognitive patterns because they sometimes learn patterns that humans do not learn, while also failing to learn patterns that humans typically learn. This discrepancy suggests that the models may not be plausible cognitive models, despite matching human performance in some tasks. Further research is needed to address these limitations and improve the alignment of large language models with human cognitive patterns.\",\n",
    "  \"gold_answer_marketing\": \"Large language models sometimes learn patterns that humans do not learn and fail to learn patterns that humans typically do learn.\"},\n",
    "16: {\"question\": \"What factors influenced the development of generative language models by Anthropic?\",\n",
    "  \"gold_answer_research\": \"Several factors influenced the development of generative language models by Anthropic, including the limitations in coding, math, and reasoning capabilities of the initial version Claude, the partnerships with companies like Notion and Quora to enhance the model's capabilities, and the need to address biases, unsafe content, and ethical considerations in training data. Additionally, the reliance on supervised learning and the need for controlled generation in generative models played a role in shaping the development of Anthropic's language models.\",\n",
    "  \"gold_answer_marketing\": \"Factors that influenced the development of generative language models by Anthropic include partnerships with companies like Notion and Quora, limitations in coding, math, and reasoning capabilities in initial models like Claude, and the need to address biases and unsafe content in training datasets.\"},\n",
    "17: {\"question\": \"What is Constitutional AI and how does it affect the functionality of AI systems?\",\n",
    "  \"gold_answer_research\": \"Constitutional AI is an approach developed by Anthropic for training AI systems, particularly language models like Claude, to be harmless and helpful without relying on extensive human feedback. It involves two phases: supervised learning, where the model generates responses to prompts and self-critiques based on a set of guiding principles, and reinforcement learning, where the model is trained with AI-generated feedback according to constitutional principles. This approach enables the training of AI assistants that are both helpful and harmless, with the ability to explain objections to harmful requests, enhancing transparency and reducing the need for human supervision.\",\n",
    "  \"gold_answer_marketing\": \"Constitutional AI is an approach developed by Anthropic for training AI systems, particularly language models like Claude, to be harmless and helpful without relying on extensive human feedback. It involves supervised learning and reinforcement learning phases to guide the model's responses based on a set of guiding principles (a 'constitution'). This approach aims to create AI systems that are both helpful and transparent in their decision-making process, reducing the need for constant human supervision.\"},\n",
    "18: {\"question\": \"How do advances in AI models impact their ability to interact with different types of data, such as images?\",\n",
    "  \"gold_answer_research\": \"Advances in AI models, such as multimodal models like RA-CM3, have significantly improved their ability to interact with different types of data, such as images. These models can refer to external memory, like web data, to increase their knowledge capacity, allowing them to generate correct images from entity-rich captions. Additionally, these models can perform image editing and manually specify examples in-context for better results. The use of large language models, combined with larger datasets and neural networks, has also enhanced their performance in tasks like image generation and text generation.\",\n",
    "  \"gold_answer_marketing\": \"Advances in AI models, such as multimodal models like RA-CM3, allow for better interaction with different types of data, like images, by accessing external memory for increased knowledge capacity and improving performance in tasks like image generation and image editing.\"},\n",
    "19: {\"question\": \"What are the potential trade-offs between AI system alignment with ethical guidelines and practical utility?\",\n",
    "  \"gold_answer_research\": \"The potential trade-offs between AI system alignment with ethical guidelines and practical utility include the risk of reduced performance and usability due to stringent ethical alignment measures, as seen with Claude 2. Users may face limitations and refusal of assistance for benign requests, leading to debates over the 'alignment tax' in AI development. Balancing ethical considerations with practical functionality is crucial to ensure alignment with ethical guidelines without compromising the practical utility of AI systems. Research is needed to find a middle ground that prioritizes ethical alignment while maintaining usability and performance.\",\n",
    "  \"gold_answer_marketing\": \"The potential trade-offs between AI system alignment with ethical guidelines and practical utility include balancing stringent ethical alignment that may reduce usability and performance, ensuring transparency and fairness in alignment processes, and addressing the alignment tax that may impact adoption of AI systems.\"},\n",
    "20: {\"question\": \"How has the token handling capacity changed between different versions of the Claude model?\",\n",
    "  \"gold_answer_research\": \"The token handling capacity has increased with each new version of the Claude model. Claude Instant has a context length of 100,000 tokens, Claude 2.1 doubled this to 200,000 tokens, and Claude 3 Opus default version has a context window of 200,000 tokens but can be expanded to 1 million for specific use cases. This progression shows a trend towards handling larger amounts of text data for improved performance and capabilities.\",\n",
    "  \"gold_answer_marketing\": \"The token handling capacity has increased from Claude to Claude Instant to Claude 2.1, with Claude Instant having a input context length of 100,000 tokens, Claude 2.1 having a context window of 200,000 tokens, and Claude 3 Opus having a context window of 1 million tokens.\"},\n",
    "22: {\"question\": \"In what ways has the Claude model's ability to self-critique and revise its responses enhanced its transparency?\",\n",
    "  \"gold_answer_research\": \"The Claude model's ability to self-critique and revise its responses has enhanced its transparency by allowing for iterative improvements based on past actions and mistakes. Through self-reflection, the model can refine its output by learning from feedback and generating special tokens to signal the need for retrieval or confirm the relevance, support, or completeness of its responses. This process ensures that the model's statements about the world are truthful and accurate, ultimately increasing transparency in its decision-making and reasoning processes.\",\n",
    "  \"gold_answer_marketing\": \"The Claude model's ability to self-critique and revise its responses has enhanced its transparency by allowing it to generate text informed by retrieved passages, criticize the output, and signal the need for retrieval or confirm the output's relevance, support, or completeness. This self-reflection process helps improve the model's accuracy and reliability in generating responses.\"},\n",
    "23: {\"question\": \"How do subsequent versions of Claude compare in terms of their likelihood to produce false statements?\",\n",
    "  \"gold_answer_research\": \"Claude Instant is a faster and lighter version of Claude, with an input context length of 100,000 tokens. In contrast, Claude 3 has faced criticism for its stringent ethical alignment, leading to a debate over the 'alignment tax' in AI development. Users have been refused assistance with benign requests, which has sparked discussions on balancing ethical considerations and practical functionality. This suggests that Claude Instant may have a lower likelihood of producing false statements compared to Claude 3 due to its focus on usability and performance.\",\n",
    "  \"gold_answer_marketing\": \"Claude Instant is a faster, less expensive, and lighter version of Claude with a shorter input context length. Claude 3 has faced criticism for ethical alignment issues that may affect usability and performance.\"},\n",
    "24: {\"question\": \"Who developed the language model family known as Chinchilla?\",\n",
    "  \"gold_answer_research\": \"The Chinchilla language model family was developed by the research team at DeepMind and presented in March 2022. It is named 'Chinchilla' as an advancement over the previous Gopher model family. The Chinchilla family has been trained to investigate the scaling laws of large language models and is designed to outperform GPT-3.\",\n",
    "  \"gold_answer_marketing\": \"The research team at DeepMind developed the language model family known as Chinchilla.\"},\n",
    "25: {\"question\": \"What benchmark did Chinchilla achieve an average accuracy of 67.5% on?\",\n",
    "  \"gold_answer_research\": \"Chinchilla achieved an average accuracy of 67.5% on the MMLU benchmark (Measuring Massive Multitask Language Understanding).\",\n",
    "  \"gold_answer_marketing\": \"Chinchilla achieved an average accuracy of 67.5% on the MMLU benchmark (Measuring Massive Multitask Language Understanding).\"},\n",
    "27: {\"question\": \"What is the relationship between Chinchilla and the Gopher language model families?\",\n",
    "  \"gold_answer_research\": \"The Chinchilla family of transformer models is essentially the same as the Gopher family, with minor modifications and different training optimizers. Chinchilla uses AdamW optimizer while Gopher uses Adam optimizer. Additionally, Chinchilla uses relative positional encoding and RMSNorm instead of absolute positional encoding and LayerNorm used by Gopher. Chinchilla has 70B parameters and outperforms Gopher on the MMLU benchmark by 7%, showcasing an improvement in performance. Both families follow similar naming conventions and were developed to investigate the scaling laws of large language models.\",\n",
    "  \"gold_answer_marketing\": \"Chinchilla is a family of transformer models developed by DeepMind, which is a further development over a previous model family named Gopher. Both model families were trained to investigate the scaling laws of large language models.\"},\n",
    "28: {\"question\": \"What distinguishes the architectures of the Chinchilla and Gopher family models in terms of optimization techniques used?\",\n",
    "  \"gold_answer_research\": \"The main distinction in optimization techniques between the Chinchilla and Gopher family models lies in the choice of optimizers. The Gopher family utilizes the Adam optimizer, whereas the Chinchilla family is trained using the AdamW optimizer. Additionally, the Gopher family employs RMSNorm instead of LayerNorm, and relative positional encoding rather than absolute positional encoding. These differences in optimization techniques contribute to the unique characteristics and performance of each model family.\",\n",
    "  \"gold_answer_marketing\": \"The Chinchilla family uses AdamW optimizer, while the Gopher family uses the Adam optimizer.\"},\n",
    "30: {\"question\": \"What is the recommended strategy for training large autoregressive language models with limited compute resources, as contributed by the Chinchilla team?\",\n",
    "  \"gold_answer_research\": \"The Chinchilla team recommends that the number of training tokens should be doubled for every model size doubling to achieve better results on downstream tasks. They also suggest using larger, higher-quality training datasets to improve performance. Additionally, they mention the importance of balancing model size and efficiency to address computational costs and inference latency limitations. It is advised to focus on Transformer language models and consider sharing model parameters for quick task-switching when deploying as a service.\",\n",
    "  \"gold_answer_marketing\": \"The Chinchilla team recommends doubling the number of training tokens for every model size doubling and using larger, higher-quality training datasets to achieve better results on downstream tasks.\"},\n",
    "33: {\"question\": \"What are some key areas of research in the field of artificial intelligence as reflected in recent academic literature?\",\n",
    "  \"gold_answer_research\": \"Recent academic literature in the field of artificial intelligence reflects key areas of research such as natural language processing with state-of-the-art transformers, feature learning in infinite-width neural networks, diverse beam search for complex scene description, and the development of generative AI models capable of generating text and images. Additionally, research focuses on human preferences in dueling bandits, the use of few-shot learners in language models, and the exploration of knowledge-grounded neural conversation models. These areas of research highlight the advancements in AI technology and its applications across various domains.\",\n",
    "  \"gold_answer_marketing\": \"Some key areas of research in artificial intelligence include natural language processing, deep neural networks, generative AI, AI safety, AI art, reinforcement learning, and language agents alignment.\"},\n",
    "34: {\"question\": \"What are some of the limitations of traditional position encoding methods in the architecture of pre-trained language models (PLMs), and what novel approach does the paper propose to address these issues?\",\n",
    "  \"gold_answer_research\": \"One limitation of traditional position encoding methods in PLMs is that they may not enable length extrapolation of pre-existing models, leading to the need for substantial pre-training costs. The paper proposes a novel approach called Position Interpolation, which extends existing PLMs without deviating far from existing definitions of position encoding or attention mechanisms. This method allows for much extended context windows for text modeling, leading to significant perplexity gains and improved model performance.\",\n",
    "  \"gold_answer_marketing\": \"Traditional position encoding methods in PLMs have limitations in enabling length extrapolation and adapting to extended context windows. The paper proposes a novel approach called Position Interpolation, which generates strong models that can effectively make use of much extended context windows. This method allows for substantial pre-training cost savings and preserves the quality of the original models, even for small context window tasks.\"},\n",
    "35: {\"question\": \"How does the Rotary Position Embedding (RoPE) approach in Transformers differ from the traditional additive method of position embedding with respect to encoding position information?\",\n",
    "  \"gold_answer_research\": \"The RoPE approach in Transformers differs from the traditional additive method of position embedding by being multiplicative instead of additive. While traditional methods add position encoding to context representations, RoPE incorporates relative position information through rotation matrix product. This means that RoPE naturally includes relative position dependency in the self-attention formulation, without altering terms in the expanded formulation like the additive method does. Additionally, RoPE's properties show that it decays as the relative distance between positions increases, providing a clear theoretical interpretation of how position information is encoded.\",\n",
    "  \"gold_answer_marketing\": \"The RoPE approach in Transformers differs from the traditional additive method of position embedding by incorporating relative position information through rotation matrix product instead of altering terms in the expanded formulation of additive position encoding.\"},\n",
    "36: {\"question\": \"What is the significance of comparing the normalized subspace similarity between ∆Wq, ∆Wv, and random Gaussian matrices when analyzing the adaptation of pre-trained language models?\",\n",
    "  \"gold_answer_research\": \"Comparing the normalized subspace similarity between ∆Wq, ∆Wv, and random Gaussian matrices provides insight into the underlying mechanism for adapting pre-trained language models. It helps determine the intrinsic rank of the adaptation matrix ∆W and sheds light on the connection between ∆W and the original weight matrix W. By analyzing these similarities, we can understand how much of the adaptation is specific to the task at hand and how much is influenced by the pre-trained model. This comparison is crucial for optimizing the adaptation process and maximizing downstream performance in NLP tasks.\",\n",
    "  \"gold_answer_marketing\": \"Comparing the normalized subspace similarity between ∆Wq, ∆Wv, and random Gaussian matrices helps understand the underlying mechanism for adapting pre-trained language models. It reveals the intrinsic rank and common singular value directions learned by different runs, shedding light on the fundamental principles of using pre-trained language models for downstream tasks in NLP.\"},\n",
    "38: {\"question\": \"What issues are associated with the homogeneity of language model training contractors, and how might it affect the behavior of the models?\",\n",
    "  \"gold_answer_research\": \"The issues associated with the homogeneity of language model training contractors include potential biases in the labeling process, lack of diverse perspectives leading to limited coverage of sensitive content, and reduced robustness in model performance across different tasks. This homogeneity can affect the behavior of the models by reinforcing certain biases, increasing the risk of harmful content generation, and limiting the models' ability to generalize effectively. To address these issues, it is important to ensure diversity among labelers, incorporate varied perspectives in training data, and implement measures to enhance model robustness and performance across a range of tasks.\",\n",
    "  \"gold_answer_marketing\": \"The homogeneity of language model training contractors can lead to biased or limited perspectives in the data, which may result in the models producing harmful content, gaming objectives, or lacking sensitivity to diverse viewpoints. This can affect the behavior of the models by reinforcing stereotypes, increasing toxicity, and reducing their ability to accurately represent under-represented groups.\"},\n",
    "39: {\"question\": \"What are common research topics and themes found in recent publications about artificial intelligence and natural language processing?\",\n",
    "  \"gold_answer_research\": \"Recent publications in artificial intelligence and natural language processing have covered topics such as transformer models, feature learning in neural networks, attention mechanisms, multi-task benchmark platforms, semantic search using sentence embeddings, cross-task generalization, and question generation for question answering. Themes commonly explored include machine comprehension of text, reinforcement learning algorithms, sentence embeddings, semantic compositionality, reasoning with language models and knowledge graphs, and the gap between neural text and human text. These publications also delve into deep language understanding, retrieval-augmented transformers, image captioning, and open datasets for image-text pairs.\",\n",
    "  \"gold_answer_marketing\": \"Common research topics and themes in recent publications on artificial intelligence and natural language processing include transformer models, attention mechanisms, semantic search, sentence embeddings, and question answering using language models and knowledge graphs.\"},\n",
    "41: {\"question\": \"Question: When conducting demographic and technical assessments of teams or research subjects, what types of data categories are typically collected and analyzed to ensure a comprehensive understanding of the group's composition and the methods used?\",\n",
    "  \"gold_answer_research\": \"When conducting demographic and technical assessments of teams or research subjects, it is important to collect and analyze data categories such as age, gender, education level, professional background, and expertise in specific areas. By gathering information on these categories, you can ensure a comprehensive understanding of the group's composition and the methods used in your assessments. Additionally, it may be helpful to consider factors like cultural background, language proficiency, and geographical location to capture a more nuanced picture of the group being assessed. This detailed approach to data collection and analysis can provide valuable insights for making informed decisions and recommendations based on the gathered information.\",\n",
    "  \"gold_answer_marketing\": \"Answer: Demographic data such as age, gender, education level, and technical data related to skills and experience are typically collected and analyzed for comprehensive understanding.\"},\n",
    "43: {\"question\": \"What kind of tasks can be performed using the datasets described in the provided text, and what are some common features of these datasets?\",\n",
    "  \"gold_answer_research\": \"The datasets described in the provided text can be used for tasks such as question answering, duplicate question retrieval, entity retrieval, citation prediction, query understanding, document understanding, passage retrieval, text summarization, fact verification, and code search. Common features of these datasets include diverse task categories, comprehensive instructions, a wide range of synthetic user personalities and interaction patterns, and a focus on enhancing comprehension of documents to deliver accurate results. Additionally, the datasets cover a variety of domains such as public health, scientific exams, climate, and general knowledge.\",\n",
    "  \"gold_answer_marketing\": \"The datasets described in the provided text can be used for tasks such as question answering, document summarization, duplicate question retrieval, code search, sentence simplification, dialogue generation, body retrieval, caption generation, fact verification, and more. Some common features of these datasets include diverse input-output pairs, incorporation of various knowledge-intensive datasets, and a focus on generating high-quality synthetic data points.\"},\n",
    "44: {\"question\": \"What conclusions can be drawn about the relationship between input prompt toxicity and output toxicity when using different language models and prompts?\",\n",
    "  \"gold_answer_research\": \"Based on the findings presented in the results section, it can be concluded that the relationship between input prompt toxicity and output toxicity varies depending on the language model used and the specific prompt given. When instructed to produce a safe and respectful output, InstructGPT models generate less toxic outputs compared to GPT-3, but this advantage disappears when the respectful prompt is removed. On the other hand, when explicitly prompted to produce a toxic output, InstructGPT outputs are much more toxic than GPT-3 outputs. Additionally, the toxicity of the model outputs is highly correlated with the toxicity of the input prompt, as shown in Figure 39.\",\n",
    "  \"gold_answer_marketing\": \"The study found that when instructed to produce a safe and respectful output, InstructGPT models generate less toxic outputs compared to GPT-3. However, this advantage disappears when the respectful prompt is removed. Interestingly, when explicitly prompted to produce a toxic output, InstructGPT outputs are much more toxic than GPT-3. This suggests that the toxicity of the output is highly correlated with the toxicity of the input prompt.\"},\n",
    "45: {\"question\": \"What are some challenges in training retrieval systems and how are negative samples used to address them?\",\n",
    "  \"gold_answer_research\": \"Training retrieval systems face challenges such as redundancy in retrieved documents and lack of diversity in retrieval. Negative samples, including randomly sampled negatives, denoised hard negatives, and instruction-unfollowing negatives, are crucial for improving system performance. Carefully designed negative samples help the system effectively learn the task, but they can also lead to performance drops in out-of-domain datasets. Combining random samples and challenging negatives during training is key to building a competitive system for both in-domain and out-of-domain retrieval.\",\n",
    "  \"gold_answer_marketing\": \"Some challenges in training retrieval systems include high cost of annotating datasets for new tasks and improving performance in zero-shot settings. Negative samples, such as denoised hard negative documents and instruction-unfollowing negative documents, are used to train retrieval systems effectively and address performance drops in out-of-domain datasets.\"},\n",
    "46: {\"question\": \"What factors have been found to potentially impact the ability of models to follow instructions, based on the analysis provided?\",\n",
    "  \"gold_answer_research\": \"Based on the analysis provided, factors that have been found to potentially impact the ability of models to follow instructions include the human feedback obtained from contractors, which may be influenced by their beliefs, cultural backgrounds, and personal history. Additionally, the model's behavior can be affected by false premises in instructions, tendencies to hedge, and performance degradation with multiple explicit constraints in instructions. The models are also not fully aligned or safe, as they can generate toxic or biased outputs, make up facts, and fail to generate reasonable outputs in some cases.\",\n",
    "  \"gold_answer_marketing\": \"Factors that may impact the ability of models to follow instructions include false premises in instructions, models hedging unnecessarily, performance degradation with multiple constraints in instructions, generation of toxic or biased outputs, and over-generalization leading to refusal of innocuous instructions.\"},\n",
    "47: {\"question\": \"What are some key factors to consider when building a successful multi-task instruction-following retrieval system as identified in the research?\",\n",
    "  \"gold_answer_research\": \"Some key factors to consider when building a successful multi-task instruction-following retrieval system include the need for cross-task interdependence for training a single retriever, the flexibility and zero-shot transfer enabled by instructions compared to task identifiers, and the elimination of the need for hosting multiple task-specific retrievers. Additionally, optimizing the mix and volume of instructional data for diverse tasks is crucial, as well as considering the impact of ranking strategy in data construction. Finally, the effectiveness of the dataset scale in retrieval and the importance of carefully designed negative samples should be taken into account for improved efficiency of instruction-following retrievers.\",\n",
    "  \"gold_answer_marketing\": \"Key factors to consider when building a successful multi-task instruction-following retrieval system include the effectiveness of the dataset scale in retrieval, the diversity in data and model scale, carefully designed negative samples, and the ability to adapt to new tasks via instructions.\"},\n",
    "48: {\"question\": \"What are the benefits of using retrieval-augmented techniques in multimodal language modeling, as demonstrated by the performance of the RA-CM3 model in the document?\",\n",
    "  \"gold_answer_research\": \"The benefits of using retrieval-augmented techniques in multimodal language modeling, as demonstrated by the performance of the RA-CM3 model, include significantly better training efficiency with less training compute, outperforming existing models by using less training data, compute, and parameters. The retrieval augmentation allows the model to focus on learning how to use retrieved documents in context, leading to improved accuracy in classification tasks. Additionally, the RA-CM3 model achieves strong performance in image and caption generation, surpassing existing models like DALL-E and Flamingo despite using fewer resources.\",\n",
    "  \"gold_answer_marketing\": \"The benefits of using retrieval-augmented techniques in multimodal language modeling, as demonstrated by the performance of the RA-CM3 model in the document, include outperforming existing models by using less training data, compute, and parameters, achieving significantly better training efficiency, and improving accuracy in k-shot classification tasks. Additionally, retrieval augmentation allows the model to focus on learning how to use retrieved documents in context, leading to stronger performance in tasks such as image and caption generation.\"},\n",
    "50: {\"question\": \"What methods are typically employed to create training data for embedding models that use task-specific instructions?\",\n",
    "  \"gold_answer_research\": \"To create training data for embedding models that use task-specific instructions, a common method is to combine datasets from different sources, such as the SuperNaturalInstructions dataset with existing collections designed for embedding training. The SuperNaturalInstructions dataset provides natural language instructions, which can be paired with positive and negative examples to form training samples. Additionally, for tasks like classification or similarity, training samples can be constructed by selecting text sequences associated with different classes or similarities. This diverse training data is essential for instruction-based finetuning, which enables the embedding model to learn from a wide range of tasks and domains.\",\n",
    "  \"gold_answer_marketing\": \"Training data for embedding models that use task-specific instructions is typically created by formulating a wide variety of tasks as text-to-text problems, distinguishing good/bad candidate outputs given an input text. This is done by combining datasets with natural language instructions and constructing positive and negative pairs for training.\"},\n",
    "51: {\"question\": \"Question: What are some of the challenges and innovations associated with fine-tuning large language models, and how does the approach discussed in the referenced text aim to address them?\",\n",
    "  \"gold_answer_research\": \"Some challenges associated with fine-tuning large language models include limited access to and manipulation of knowledge, lagging performance on knowledge-intensive tasks, and the need for provenance in decision-making and updating world knowledge. The approach discussed in the referenced text aims to address these challenges by utilizing Retrieval Augmented Generation (RAG), which involves retrieving relevant passages from a corpus to feed to the language model for improved performance in tasks such as question-answering and dialogue. This iterative approach focuses on improving alignment with user intent and fine-tuning models to control sentiment and improve response quality in various language tasks.\",\n",
    "  \"gold_answer_marketing\": \"The challenges with fine-tuning large language models include aligning them with user intent and controlling the quality of generated outputs. The approach discussed in the referenced text aims to address these challenges by using Retrieval Augmented Generation (RAG) to retrieve relevant passages from a corpus and feed them to the language model, improving alignment and performance.\"},\n",
    "52: {\"question\": \"What is a common technique used to address the outlier issue when applying block-wise k-bit quantization to input tensors, and how does it work?\",\n",
    "  \"gold_answer_research\": \"A common technique used to address the outlier issue when applying block-wise k-bit quantization to input tensors is to chunk the input tensor into blocks that are independently quantized, each with their own quantization constant. This approach involves dividing the input tensor into contiguous blocks of size B by flattening the tensor and slicing it into n blocks, where n is determined by the size of the blocks. Each block is then quantized independently using a quantization constant c, which helps prevent outlier values from causing performance degradation.\",\n",
    "  \"gold_answer_marketing\": \"A common technique used to address the outlier issue when applying block-wise k-bit quantization to input tensors is to chunk the input tensor into blocks that are independently quantized, each with their own quantization constant. This helps prevent performance degradation by reducing the impact of outliers on the quantization process.\"},\n",
    "54: {\"question\": \"What considerations or techniques are commonly implemented when setting up finetuning experiments for machine learning models?\",\n",
    "  \"gold_answer_research\": \"When setting up finetuning experiments for machine learning models, it is common to use a two-stage approach. The initial stage involves setting the initial parameters using a language modeling objective. This is followed by a supervised discriminative 'fine-tuning' stage to adapt these parameters to the target task. Additionally, it is typical to train all models using the Adam optimizer and a triangular learning rate scheduler with 10% warmup. Experimentation with different hyperparameters such as number of epochs, peak learning rate, and batch size is also conducted to optimize model performance. Finally, utilizing a mixture of datasets and balancing the sizes of datasets can help improve the robustness and generalization of the finetuned models.\",\n",
    "  \"gold_answer_marketing\": \"Considerations for setting up finetuning experiments for machine learning models commonly include using a language modeling objective for initial parameter setting and supervised discriminative fine-tuning for adapting parameters to the target task. Techniques such as hyperparameter search, Adam optimizer with triangular learning rate scheduler, and balancing dataset sizes through mixing strategies are also commonly implemented. Additionally, freezing some model layers during fine-tuning and incorporating negative examples for contrastive learning can be effective strategies.\"},\n",
    "55: {\"question\": \"What are the implications of the equivalence relation defined in the theoretical analysis of the DPO model for understanding the relationship between reward functions in reinforcement learning?\",\n",
    "  \"gold_answer_research\": \"The equivalence relation defined in the theoretical analysis of the DPO model implies that two reward functions are considered equivalent if they differ by a constant function. This means that the class of learned reward models is not constrained by this reparameterization, allowing for the exact recovery of the optimal policy. Understanding this relationship between reward functions in reinforcement learning helps in defining a unique reward function within each equivalence class, which is crucial for optimizing policies under existing models of human preferences. It also highlights the generality and flexibility in the reward model due to the proposed reparameterization.\",\n",
    "  \"gold_answer_marketing\": \"The equivalence relation defined in the theoretical analysis of the DPO model shows that two reward functions are considered equivalent if they differ by a fixed function. This implies that different reward functions can lead to the same optimal policy, allowing for flexibility in designing reward models in reinforcement learning.\"},\n",
    "59: {\"question\": \"Considering the structure and content of the provided text, what guidelines should be used to evaluate the effectiveness of a summary or chatbot response in this context?\",\n",
    "  \"gold_answer_research\": \"To evaluate the effectiveness of a summary or chatbot response in this context, guidelines should include assessing the faithfulness of the answer to the retrieved context, the relevance of the answer to the question, and the focus of the retrieved context. Additionally, consider using quality metrics such as answer relevancy to rank responses based on how directly they address the question and avoid redundant or incomplete information. Lastly, take into account the performance of different tasks such as summarization, citation prediction, and passage ranking to determine the overall effectiveness of the response.\",\n",
    "  \"gold_answer_marketing\": \"Answer: Evaluate based on faithfulness, answer relevance, and context relevance.\"},\n",
    "60: {\"question\": \"What are some recent methods and technologies that have been developed to enhance the capabilities and performance of natural language processing models?\",\n",
    "  \"gold_answer_research\": \"Recent methods and technologies developed to enhance natural language processing models include retrieval-augmented multimodal language modeling, which outperforms existing models with less training data and parameters. Another advancement is the use of feature learning in infinite-width neural networks to improve performance. Additionally, embedding techniques in NLP have been developed to map words or phrases to real number vectors, enhancing the model's understanding of language. These innovations have led to improvements in tasks like query reformulation, document ranking, and fine-tuning larger language models for various applications.\",\n",
    "  \"gold_answer_marketing\": \"Recent methods and technologies include retrieval-augmented language models, feature learning in infinite-width neural networks, and word embeddings.\"},\n",
    "61: {\"question\": \"What are some potential directions for future work mentioned in the document related to enhancing question-answering techniques for document-oriented tasks?\",\n",
    "  \"gold_answer_research\": \"One potential direction for future work mentioned in the document is the development of multi-modal approaches that incorporate table and figure information into GPT-4 question-answering for documents. Another direction is to incorporate question type in the PDFTriage approach to improve the efficiency and efficacy of the approach. Additionally, the document suggests further research in document-grounded, information-seeking question answering, which the dataset is designed to facilitate.\",\n",
    "  \"gold_answer_marketing\": \"Some potential future directions mentioned in the document include developing multi-modal approaches that incorporate table and figure information into question-answering for documents, and incorporating question type in the PDFTriage approach to improve efficiency and efficacy.\"},\n",
    "62: {\"question\": \"What information would you expect to find in section 2 of a document, based on the types of questions classified under Summarization?\",\n",
    "  \"gold_answer_research\": \"Based on the types of questions classified under Summarization, you would expect to find key takeaways, concise summaries, and specific content extraction related to different sections of the document in section 2. The section likely contains detailed summaries of specific parts of the document, along with structured metadata representation and instructions for summarizing the content effectively. It may also include guidelines for extracting specific information and rewriting text for clarity and conciseness.\",\n",
    "  \"gold_answer_marketing\": \"Based on the types of questions classified under Summarization, you would expect to find key takeaways, concise summaries, and specific content extraction related to the document in section 2.\"},\n",
    "63: {\"question\": \"What are the main advantages and attention mechanisms that contribute to the enhanced performance and efficiency of the newly introduced language model as compared to its predecessors?\",\n",
    "  \"gold_answer_research\": \"The main advantages of the newly introduced language model include utilizing retrieval-augmentation to incorporate external knowledge, which improves prediction accuracy. Additionally, the model employs attention mechanisms that allow for better understanding of dependencies between source and target sequences, leading to more informed predictions. These attention mechanisms have been extended from machine translation to various other fields, enhancing the model's adaptability and performance across different tasks. Finally, the model's use of self-attention mechanisms enables better contextual representation learning, parallelization, and modeling of longer intra-token relations, improving efficiency and performance compared to previous models.\",\n",
    "  \"gold_answer_marketing\": \"The main advantages of the newly introduced language model include the use of retrieval-augmented mechanisms, attention mechanisms, and context representation learning, which contribute to enhanced performance and efficiency compared to its predecessors.\"},\n",
    "64: {\"question\": \"What criteria are used to assess the quality of recommendations provided by different language models in a comparison study?\",\n",
    "  \"gold_answer_research\": \"In a comparison study of language models, criteria such as sentence relevance, lexical accuracy, and contextual understanding are used to assess the quality of recommendations. Different tasks may benefit from different evaluation measures, such as STRINC, LEXICAL, and CXMI. Additionally, template selection plays a vital role in the quality of recommendations, with deliberate template design being important for tasks like query suggestion. The overall quality of recommendations is often judged using a Likert scale, along with metadata collection for each model output.\",\n",
    "  \"gold_answer_marketing\": \"The criteria used to assess the quality of recommendations provided by different language models in a comparison study include comparing to human-created benchmarks, examining intrinsic character, comparing two models, investigating rate of learning, and analyzing learning curves.\"},\n",
    "65: {\"question\": \"What approaches have been proposed to enhance the task performance of language models while considering the trade-offs such as runtime efficiency, robustness to irrelevant context, and attribution quality?\",\n",
    "  \"gold_answer_research\": \"Several approaches have been proposed to enhance the task performance of language models while considering trade-offs. These include using compression and selective augmentation methods to decrease the propensity of models to generate toxic or biased outputs. Adversarial setups have been suggested where labelers find worst-case behaviors of the model and add them to the dataset. Additionally, models like BART and T5 leverage bi-directional attention to achieve stronger performance on both discriminative and generative tasks. These methods aim to balance model performance with considerations such as runtime efficiency, robustness to irrelevant context, and attribution quality.\",\n",
    "  \"gold_answer_marketing\": \"Approaches proposed to enhance language model task performance include compression and selective augmentation, adversarial set-ups for labeling worst-case behaviors, retrieval-augmented models, and extending existing models to enable length extrapolation while maintaining quality.\"},\n",
    "67: {\"question\": \"What metrics are commonly used to compare the performance of language models in various tasks, as outlined in an experimental results table?\",\n",
    "  \"gold_answer_research\": \"Common metrics used to compare the performance of language models in various tasks, as outlined in an experimental results table, include Exact Match and Unigram F1. These metrics have become standard in evaluating language models. Additionally, other metrics such as BLEU score, FactScore (factuality), precision, and recall are also commonly used to assess the performance of language models across different tasks. It is important to consider a variety of metrics to get a comprehensive understanding of the effectiveness of a language model in different contexts.\",\n",
    "  \"gold_answer_marketing\": \"The metrics commonly used to compare the performance of language models in various tasks are Exact Match and Unigram F1.\"},\n",
    "69: {\"question\": \"What is the role of manual assessment in the validation of language model predictions according to the text provided?\",\n",
    "  \"gold_answer_research\": \"Manual assessment plays a crucial role in the validation of language model predictions. The engineers evaluate the quality of model outputs by having labelers rate them on test sets consisting of prompts from held-out customers. This manual assessment helps ensure that the models are aligned with a broad distribution of language tasks and can identify any behavioral issues that may arise from misalignment. Additionally, human annotators find that certain reflection token predictions are aligned with their assessments, providing valuable insights into the accuracy and effectiveness of the models.\",\n",
    "  \"gold_answer_marketing\": \"Answer: Manual assessment plays a key role in evaluating the quality of language model predictions by having labelers rate the model outputs and comparing them to prompts from held-out customers.\"},\n",
    "70: {\"question\": \"What are the general steps outlined for training a language model in the document, and how is the training data for the generator language model collected and utilized?\",\n",
    "  \"gold_answer_research\": \"The document outlines the general steps for training a language model, including incorporating retrieved documents into the main input sequence and optimizing the loss function to train the generator. The training data for the generator language model is collected through various techniques such as supervised fine-tuning, critic learning, and custom retrievers for downstream tasks. The collected data is used to train the generator on specific tasks like summarization, machine reading comprehension, and natural language to SQL translation, improving performance on those tasks.\",\n",
    "  \"gold_answer_marketing\": \"The general steps for training a language model include fine-tuning on specific datasets, filtering pretraining data, and using critic learning. Training data for the generator language model is collected from open-access NLP papers and used for downstream conditional text generation tasks.\"},\n",
    "73: {\"question\": \"What are the three main categories used to refine language model abilities in understanding and executing search tasks according to the given document?\",\n",
    "  \"gold_answer_research\": \"The three main categories used to refine language model abilities in understanding and executing search tasks are query understanding, document understanding, and query-document relationship understanding. Tasks within these categories focus on interpreting queries, comprehending documents, and understanding the relationships between queries and documents. This approach aims to enhance the models' performance in interpreting and responding to search-related instructions effectively, improving their utility in complex information retrieval scenarios.\",\n",
    "  \"gold_answer_marketing\": \"The three main categories used to refine language model abilities in understanding and executing search tasks are query understanding, document understanding, and query-document relationship understanding.\"},\n",
    "74: {\"question\": \"What are some of the emerging research topics and challenges in the field of natural language processing and information retrieval according to recent academic conferences and publications?\",\n",
    "  \"gold_answer_research\": \"Recent academic conferences and publications have highlighted emerging research topics and challenges in natural language processing and information retrieval. Some key areas of focus include efficient retrieval augmented generation, unsupervised dense information retrieval with contrastive learning, citation-informed transformers, and knowledge refinement via interaction between search engines and large language models. Additionally, challenges such as zero-shot retrieval, semantic search using GPT sentence embeddings, and prompt-based effective input reformulation for legal case retrieval have been identified as important research directions. These topics reflect the ongoing advancements and complexities in the field, driving innovation and progress in NLP and IR research.\",\n",
    "  \"gold_answer_marketing\": \"Some emerging research topics and challenges in the field of natural language processing and information retrieval include efficient generation from unstructured knowledge, semantic code search evaluation, unsupervised dense information retrieval, context-aware document term weighting, knowledge refinement through interaction with large language models, and investigating the effectiveness of large language models in search re-ranking.\"},\n",
    "75: {\"question\": \"Question: How do models with different fine-tuning strategies compare in terms of accuracy and F1 score for fact verification tasks?\",\n",
    "  \"gold_answer_research\": \"Models with different fine-tuning strategies are compared in terms of accuracy and F1 score for fact verification tasks. The introduction of LLMs has led to notable developments, with some studies leveraging prompting methods to apply LLMs in IR tasks. However, not all LLMs consistently outperform fine-tuned smaller models. For example, RankGPT based on gpt-3.5-turbo underperforms monoBERT in certain scenarios. Fine-tuning is not strictly necessary for models like GPT3, which has been evaluated on closed book question answering tasks without any updates or fine-tuning.\",\n",
    "  \"gold_answer_marketing\": \"Models with different fine-tuning strategies have shown mixed results in terms of accuracy and F1 score for fact verification tasks. Some studies have found that large language models (LLMs) outperform smaller fine-tuned models, while others have reported inconsistent performance. Factors such as task complexity and the need for prompt methods to apply LLMs in information retrieval tasks can also impact the comparison.\"},\n",
    "76: {\"question\": \"What components does a fact verification task typically involve in order to assess the accuracy of a given statement?\",\n",
    "  \"gold_answer_research\": \"A fact verification task typically involves assessing the relationship between a claim and the evidence provided, analyzing if there is enough information for a conclusive judgment. This task requires a detailed understanding of the claim and evidence to determine if it is supported or refuted. The use of performance metrics based on including gold answers in model generations instead of exact matching can help search engines deliver accurate and relevant results. Additionally, incorporating lexical measures and verification functions can aid in determining the accuracy of statements.\",\n",
    "  \"gold_answer_marketing\": \"A fact verification task typically involves assessing the relationship between a claim and supporting evidence to determine accuracy.\"},\n",
    "78: {\"question\": \"What are the key factors that determine the performance of HALO-aligned models compared to non-HALO models, according to the results presented in the analysis?\",\n",
    "  \"gold_answer_research\": \"According to the analysis presented, the key factors that determine the performance of HALO-aligned models compared to non-HALO models include the specific alignment method used (such as DPO and PPO variant), the model size (significant gap at 13B+ model sizes), and the ability to match or exceed the generation quality of SFT target sequences. Additionally, the study suggests that the cost of increasing model alignment is modest relative to pretraining, and that the modeling of human biases in HALOs may have practical benefits in improving overall performance.\",\n",
    "  \"gold_answer_marketing\": \"The key factor that determines the performance of HALO-aligned models compared to non-HALO models is the model size, with HALO-aligned models generally outperforming non-HALO models at larger sizes (13B+ model sizes).\"},\n",
    "80: {\"question\": \"How does the performance of KTO compare to DPO in model alignment, and what are the potential implications for data usage and training efficiency?\",\n",
    "  \"gold_answer_research\": \"Based on the provided data and experiments, KTO consistently outperforms DPO in model alignment, even with restrictions such as using only one output per input. This suggests that KTO can achieve higher win rates and improve performance across various benchmarks compared to DPO. The implications of this performance difference include the ability to achieve quality generation results with significantly fewer desirable examples, potentially leading to more efficient data usage and training processes. This indicates that KTO may offer a more efficient and effective approach to model alignment compared to DPO.\",\n",
    "  \"gold_answer_marketing\": \"KTO outperforms DPO in model alignment with up to 90% fewer examples. This suggests that KTO can achieve high performance even with imbalanced data, potentially leading to more efficient training processes.\"},\n",
    "81: {\"question\": \"What are some common approaches to building an open-domain question answering system?\",\n",
    "  \"gold_answer_research\": \"Some common approaches to building an open-domain question answering system include using the RAG model, which minimizes the negative log-likelihood of answers, and comparing it to extractive QA paradigms that rely on non-parametric knowledge retrieval. Another approach is to incorporate question rewriting techniques to make open-domain QA more conversational. Additionally, utilizing datasets like QASPER, which contain questions requiring complex reasoning, can improve the performance of the system. References to papers by Anantha et al. and Asai et al. provide further insights into building ODQA systems.\",\n",
    "  \"gold_answer_marketing\": \"Common approaches to building an open-domain question answering system include using retrieval over a knowledge base and incorporating the retrieved content as part of the prompt. Other methods involve pretraining models on large amounts of text data and fine-tuning them for question answering tasks.\"},\n",
    "82: {\"question\": \"What is the difference between open-book and closed-book question answering?\",\n",
    "  \"gold_answer_research\": \"Open-book question answering involves the use of external sources of knowledge, such as Wikipedia, to retrieve information and generate a response. In contrast, closed-book question answering relies on pre-trained language models that have memorized factual knowledge within their parameters to generate responses without explicit context. Closed-book QA can be seen as analogous to a closed-book exam where no external resources are allowed. The key distinction lies in the reliance on external knowledge sources for open-book QA versus internal memorized knowledge for closed-book QA.\",\n",
    "  \"gold_answer_marketing\": \"Open-book question answering involves using external sources of knowledge to answer questions, while closed-book question answering relies on pre-trained language models to provide answers without explicit context.\"},\n",
    "84: {\"question\": \"What are the basic components of the Retriever-Reader framework in open-domain QA?\",\n",
    "  \"gold_answer_research\": \"The basic components of the Retriever-Reader framework in open-domain QA include a retriever model, which fetches relevant information based on input prompts efficiently using FAISS. The retriever component is responsible for retrieving contextually relevant documents or evidence blocks based on the input question. The reader component then processes this retrieved information to generate answers to the questions posed. This framework combines information retrieval and machine reading comprehension to achieve state-of-the-art results in open-domain question answering tasks.\",\n",
    "  \"gold_answer_marketing\": \"The basic components of the Retriever-Reader framework in open-domain QA are the retriever and the reader components, which can be set up and trained independently or jointly trained end-to-end. The retriever component automatically fetches relevant information based on input prompts, while the reader component processes and comprehends the retrieved information to answer questions.\"},\n",
    "85: {\"question\": \"How is the TF-IDF model used in question answering retrieval systems?\",\n",
    "  \"gold_answer_research\": \"In question answering retrieval systems, the TF-IDF model is used to represent queries and documents as bag-of-word vectors with terms weighted by term frequency multiplied by inverse document frequency. This allows for efficient non-learning-based search engine operations based on the vector space model. The TF-IDF model helps in calculating the relevance of documents to queries by measuring the importance of terms in the context of the entire document collection. This classic information retrieval approach aids in retrieving relevant information to answer questions accurately and efficiently.\",\n",
    "  \"gold_answer_marketing\": \"The TF-IDF model is used in question answering retrieval systems to weight terms in queries and documents based on their importance in determining relevance.\"},\n",
    "86: {\"question\": \"Can neural networks enhance the process of information retrieval in QA systems?\",\n",
    "  \"gold_answer_research\": \"Neural networks, such as MLP, LSTM, and bidirectional LSTM, can be used to learn dense representations of text for information retrieval in QA systems. These approaches, known as 'Neural IR', are a new category of methods that can improve performance in retrieval problems. The introduction of neural retrievers in recent QA literature has shown to outperform traditional word-similarity-based architectures, such as BM25, and can scale to handle knowledge-grounded dialogue tasks effectively. Additionally, incorporating pre-trained retrievers in QA systems has been shown to enhance the performance of generative language models.\",\n",
    "  \"gold_answer_marketing\": \"Yes, neural networks can enhance the process of information retrieval in QA systems by improving performance in open-domain QA tasks and enabling the generation of more accurate answers.\"},\n",
    "87: {\"question\": \"What is the importance of fine-tuning in the context of QA data for open-domain question answering models?\",\n",
    "  \"gold_answer_research\": \"Fine-tuning is important in the context of QA data for open-domain question answering models because it allows the model to adapt and improve its performance on specific QA datasets. By fine-tuning the model with common QA datasets, engineers can optimize the model's ability to answer questions accurately. However, there is a concern about the significant overlap between questions in the train and test sets of public QA datasets, which could affect the generalization ability of the fine-tuned models. Engineers should carefully consider this overlap and potentially explore ways to mitigate its impact during the fine-tuning process to ensure the model's effectiveness in real-world applications.\",\n",
    "  \"gold_answer_marketing\": \"Fine-tuning is important in the context of QA data for open-domain question answering models to improve search task performance and the ability to generalize to unseen datasets.\"},\n",
    "88: {\"question\": \"How does pre-training with tasks like the Inverse Cloze Task benefit open-domain question answering models?\",\n",
    "  \"gold_answer_research\": \"Pre-training with tasks like the Inverse Cloze Task benefits open-domain question answering models by improving the retrieval process over a knowledge base. By predicting the context given a sentence, the model can better understand the relationship between the question and the evidence. This approach helps in incorporating retrieved content effectively into the prompt, leading to higher accuracy in the question answering task. Additionally, using models pretrained with ICT can enhance the overall performance of the QA system by providing a better understanding of the context.\",\n",
    "  \"gold_answer_marketing\": \"Pre-training with tasks like the Inverse Cloze Task benefits open-domain question answering models by improving retrieval and generation steps, ultimately enhancing the accuracy of the process.\"},\n",
    "89: {\"question\": \"What is the main goal of prompt engineering in language models?\",\n",
    "  \"gold_answer_research\": \"The main goal of prompt engineering in language models is to effectively steer the behavior of the model towards desired outcomes without updating the model weights. This is achieved by composing and formatting prompts in a way that maximizes the model's performance on a specific task. Prompt engineering involves treating prompts as trainable parameters and optimizing them directly on the embedding space through methods like AutoPrompt, Prefix-Tuning, P-tuning, and Prompt-Tuning. The ultimate aim is to enhance the model's performance and alignment with user-defined tasks.\",\n",
    "  \"gold_answer_marketing\": \"The main goal of prompt engineering in language models is to steer the behavior of the model for desired outcomes without updating the model weights.\"},\n",
    "91: {\"question\": \"What are some known biases that can affect the performance of few-shot classification in LLMs?\",\n",
    "  \"gold_answer_research\": \"Some known biases that can affect the performance of few-shot classification in LLMs include majority label bias, recency bias, and common token bias. Majority label bias occurs when the distribution of labels among examples is unbalanced, recency bias refers to the tendency for the model to repeat the label at the end, and common token bias indicates that LLM tends to produce common tokens more often than rare tokens. These biases can contribute to high variance in few-shot classification tasks and may impact the model's ability to generalize effectively.\",\n",
    "  \"gold_answer_marketing\": \"Some known biases that can affect the performance of few-shot classification in LLMs are majority label bias, recency bias, and common token bias.\"},\n",
    "92: {\"question\": \"Why might increasing model size not reduce variance in model performance with varying prompts?\",\n",
    "  \"gold_answer_research\": \"Increasing model size may not necessarily reduce variance in model performance with varying prompts because the model's ability to generalize and adapt to different prompts is not solely dependent on its size. Factors such as the quality and relevance of the training examples, the learning rate or schedule, and the model's sensitivity to different hyperparameters can also play a significant role in determining performance variability. Additionally, the complexity of the task or dataset being used for training can impact how effectively the model scales with size. It is essential to consider these factors holistically when optimizing model performance rather than relying solely on increasing model size.\",\n",
    "  \"gold_answer_marketing\": \"Increasing model size may not reduce variance in model performance with varying prompts because the same order of prompts may work well for one model but poorly for another. Additionally, when the validation set is limited, choosing the order of prompts that prevents the model from producing extremely unbalanced predictions or being overconfident can also affect performance.\"},\n",
    "93: {\"question\": \"What is the benefit of instruction-based finetuning in language models?\",\n",
    "  \"gold_answer_research\": \"Instruction-based finetuning improves models' ability to generalize to unseen domains and tasks by providing task-specific representations that can be used for many downstream language tasks without additional training. This method also allows pretrained language models to follow instructions provided in prompts, enabling them to generate the desired output given specific inputs. Additionally, instruction finetuning helps transform raw pretrained LLMs into chatbot-like models, making finetuning more accessible and common, particularly for researchers with limited resources. Overall, the benefit of instruction-based finetuning is improved model performance, enhanced generalizability, and reduced communication costs in aligning with human intentions.\",\n",
    "  \"gold_answer_marketing\": \"The benefit of instruction-based finetuning in language models is improved ability to generalize to unseen domains and tasks, without the need for additional training.\"},\n",
    "94: {\"question\": \"Can you describe a situation where retrieval-based methods would be necessary to enhance language model performance?\",\n",
    "  \"gold_answer_research\": \"Retrieval-based methods are necessary to enhance language model performance in scenarios where the model needs to generate accurate and informative responses for entity-rich queries, such as 'George Washington standing in front of the Eiffel Tower.' In such cases, incorporating a retrieval module can provide additional context and relevant information to improve the model's understanding and generation of the desired output. Additionally, retrieval-based methods are crucial for question answering tasks, where the model needs to access external knowledge sources to provide accurate and comprehensive answers. By utilizing retrieval mechanisms, the language model can benefit from a wider range of information and improve its performance in handling complex and ambiguous queries effectively.\",\n",
    "  \"gold_answer_marketing\": \"Retrieval-based methods are necessary to enhance language model performance in tasks like question answering, where incorporating additional information from external sources can improve the model's ability to generate accurate and relevant responses.\"},\n",
    "95: {\"question\": \"What is the Chain-of-Thought prompting technique and for which types of tasks is it particularly beneficial?\",\n",
    "  \"gold_answer_research\": \"Chain-of-Thought (CoT) prompting is a technique that generates reasoning chains or rationales step by step to lead to a final answer, benefiting complicated reasoning tasks using large models with more than 50B parameters. It can be implemented through iterative Monte Carlo search methods or through a three-step process called augment-prune-select. CoT is particularly beneficial for enhancing model performance on complex tasks by decomposing them into smaller and simpler steps, shedding light on the model's thinking process. Task decomposition in CoT can be done with simple prompting, task-specific instructions, or human inputs.\",\n",
    "  \"gold_answer_marketing\": \"Chain-of-Thought (CoT) prompting is a technique that generates reasoning chains or rationales step by step to lead to a final answer. It is particularly beneficial for complicated reasoning tasks when using large models with more than 50B parameters. Simple tasks only benefit slightly from CoT prompting.\"},\n",
    "96: {\"question\": \"How do augmented language models with external tools differ from regular models in functionality?\",\n",
    "  \"gold_answer_research\": \"Augmented language models with external tools, such as TALM and Toolformer, are fine-tuned to learn how to use external tool APIs, expanding their capabilities beyond traditional language processing tasks. These models are trained to incorporate external tool API calls in order to improve the quality of their outputs, allowing them to perform tasks like speech recognition, machine translation, and information retrieval more effectively. By leveraging external tools, these models have the ability to access and utilize a wider range of resources and functionalities, enhancing their overall performance and versatility compared to regular language models.\",\n",
    "  \"gold_answer_marketing\": \"Augmented language models with external tools differ from regular models by fine-tuning a LM to use external tool APIs, expanding the dataset to improve model outputs and enhancing tasks like speech recognition, machine translation, and natural language generation.\"},\n",
    "97: {\"question\": \"What can be inferred about the utilization of attention in neural networks?\",\n",
    "  \"gold_answer_research\": \"Attention mechanisms in neural networks play a crucial role in allowing models to focus on specific parts of input data when making predictions or generating outputs. By assigning importance weights to different elements, such as pixels in an image or words in a sentence, attention helps the model to attend to relevant information and make more accurate predictions. The use of attention can improve the interpretability of neural networks by showing which parts of the input data are being focused on during the prediction process. Additionally, attention mechanisms, like multi-head attention, can enhance model performance by allowing the model to jointly attend to information from different representation subspaces at different positions.\",\n",
    "  \"gold_answer_marketing\": \"Attention in neural networks allows the model to focus on specific parts of input data, such as images or text, in order to make predictions or generate output. It helps the model to learn relationships and correlations between different elements and improve performance in tasks like image captioning or language translation.\"},\n",
    "101: {\"question\": \"Can the use of attention mechanisms in deep learning models be applied to both machine translation and computer vision?\",\n",
    "  \"gold_answer_research\": \"Yes, attention mechanisms in deep learning models have shown success in both machine translation and computer vision tasks. In machine translation, attention allows the model to capture dependencies between source and target sequences regardless of distance, leading to improved translation quality. Similarly, in computer vision, attention mechanisms have been used to focus on relevant parts of an image during caption generation, showcasing the ability to handle details and global dependencies effectively. Therefore, utilizing attention in both domains can enhance the performance of deep learning models significantly.\",\n",
    "  \"gold_answer_marketing\": \"Yes, attention mechanisms in deep learning models can be applied to both machine translation and computer vision.\"},\n",
    "102: {\"question\": \"What are the potential benefits of incorporating self-attention mechanisms into Generative Adversarial Networks (GANs)?\",\n",
    "  \"gold_answer_research\": \"Incorporating self-attention mechanisms into GANs can help the generator and discriminator better model relationships between spatial regions, leading to improved generation of detailed and realistic images. This is particularly useful for capturing global dependencies and enhancing the performance of transformer architectures. Additionally, self-attention can enable the model to assess its own predictions after each generated segment, allowing for customizable decoding algorithms to meet specific constraints or user preferences. Overall, self-attention in GANs can enhance detail handling and overall performance.\",\n",
    "  \"gold_answer_marketing\": \"Incorporating self-attention mechanisms into GANs can help the generator and discriminator better model relationships between spatial regions, leading to improved performance in handling details and capturing global dependencies.\"},\n",
    "103: {\"question\": \"How does the transformer model variate from traditional sequence-aligned recurrent architectures?\",\n",
    "  \"gold_answer_research\": \"The transformer model differs from traditional sequence-aligned recurrent architectures by not having a recurrent or convolutional structure. Instead, it heavily relies on self-attention mechanisms for processing sequences. This lack of recurrence and convolution, even with positional encoding, weakly incorporates sequential order, which can be a drawback for tasks sensitive to positional dependencies. Additionally, the transformer's architecture includes embedding layers, sinusoid-wave-based positional encoding, and softmax and linear layers in the final decoder output to maintain position information and facilitate processing of long sequences efficiently.\",\n",
    "  \"gold_answer_marketing\": \"The transformer model differs from traditional sequence-aligned recurrent architectures by not having a recurrent or convolutional structure, and instead making heavy use of self-attention. This allows for handling very long sequences efficiently and achieving better performance on tasks involving long texts.\"},\n",
    "104: {\"question\": \"What implications does the concept of a Neural Turing Machine have for the theoretical power of neural networks?\",\n",
    "  \"gold_answer_research\": \"The concept of a Neural Turing Machine (NTM) expands the theoretical power of neural networks by incorporating external memory storage, allowing for more complex computations and tasks. This mimics the Turing machine tape, enabling the neural network to control operation heads for reading and writing to the tape. However, the finite memory in NTM suggests it may resemble more of a 'Neural von Neumann Machine,' limiting its mathematical limitlessness seen in traditional Turing machines. Overall, the addition of external memory in NTM enhances the capabilities and potential applications of neural networks in solving more advanced problems.\",\n",
    "  \"gold_answer_marketing\": \"The concept of a Neural Turing Machine suggests that neural networks can be equipped with external memory storage for more complex operations, potentially increasing their theoretical power.\"},\n",
    "}\n",
    "\n",
    "\n",
    "test_questions = {\n",
    "4: {\"question\": \"When was the transformer architecture introduced, and by which organization?\"},\n",
    "5: {\"question\": \"How has the accessibility of powerful language models, such as GPT-3 and GPT-4, been controlled by their developers?\"},\n",
    "6: {\"question\": \"What benchmarks or ratings are used to compare the capabilities of different language models?\"},\n",
    "10: {\"question\": \"What are some of the primary applications for language models in technology and computing?\"},\n",
    "14: {\"question\": \"How are language models typically evaluated and what benchmarks are used for this purpose?\"},\n",
    "15: {\"question\": \"What datasets are available for evaluating language processing systems?\"},\n",
    "21: {\"question\": \"What collaborations with other companies have contributed to the development of Claude's capabilities?\"},\n",
    "26: {\"question\": \"According to DeepMind, how should the number of training tokens change relative to the model size?\"},\n",
    "29: {\"question\": \"How do the sizes of models in the Gopher family range?\"},\n",
    "31: {\"question\": \"What type of model architecture do the Gopher and Chinchilla families belong to?\"},\n",
    "32: {\"question\": \"Can you name the author who wrote the novels A Farewell to Arms and The Sun Also Rises?\"},\n",
    "37: {\"question\": \"What are the key advantages of InstructGPT models over GPT-3 models according to the findings in the research?\"},\n",
    "40: {\"question\": \"What metrics are used to compare the performance of different models on training and validation splits according to the document provided?\"},\n",
    "42: {\"question\": \"What types of evaluation metrics are commonly used to assess the accuracy of answers in AI-driven question and answer datasets?\"},\n",
    "49: {\"question\": \"What factors contribute to the performance improvement in retrieval-augmented language models compared to non-retrieval-augmented models?\"},\n",
    "56: {\"question\": \"What are the benchmarks used to evaluate the performance of the Deep Policy Optimization (DPO) method compared to other preference learning algorithms in the document provided?\"},\n",
    "57: {\"question\": \"What methodologies have been evaluated for training language models to align with human preferences, and how do they compare in terms of effectiveness?\"},\n",
    "58: {\"question\": \"What methods have been discussed in the literature for improving the alignment of language models with human preferences or feedback?\"},\n",
    "66: {\"question\": \"What are some of the evaluation metrics used for assessing different types of text generation tasks presented in the study?\"},\n",
    "68: {\"question\": \"Consider a document related to research in natural language processing or artificial intelligence. Can you name some of the recent topics or methods that have been discussed or introduced in the field according to the document?\"},\n",
    "71: {\"question\": \"What is the significance of using reflection tokens in a model like SELF-RAG?\"},\n",
    "72: {\"question\": \"How does the inclusion of selected context as opposed to appending all retrieved text spans impact computational cost during both training and inference times in language model generation tasks?\"},\n",
    "77: {\"question\": \"What are the benefits of modeling human biases in Human-Aware Loss Optimizations (HALOs), and how do they compare to non-HALOs on the same datasets?\"},\n",
    "79: {\"question\": \"What are the modifications made to the traditional Kahneman-Tversky model to adapt it for optimizing language model performance?\"},\n",
    "83: {\"question\": \"How does a model's ability to answer questions relate to its exposure to specific types of questions during training?\"},\n",
    "90: {\"question\": \"How can adding examples to a prompt affect the performance of language models?\"},\n",
    "98: {\"question\": \"What are the main components of a Neural Turing Machine (NTM) architecture?\"},\n",
    "99: {\"question\": \"How might a seq2seq model's limitations be addressed in natural language processing tasks?\"},\n",
    "100: {\"question\": \"What differentiates hard attention from soft attention in image processing algorithms?\"},\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvASw15pixi_"
   },
   "source": [
    "### 1.C) My Additional Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "80SyqV-la5eN"
   },
   "source": [
    "#### 1.C.1) Installs & Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iRO-O7ihIl56"
   },
   "source": [
    "Below are some additional installs and imports that we are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FZ0Rwa-08nZ6"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# necessary installs for metrics\n",
    "!pip install evaluate\n",
    "!pip install bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YJNh0FC281lK"
   },
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "import pandas as pd # for df of metrics\n",
    "import time # for pauses\n",
    "import gc # for garbage collection\n",
    "from operator import itemgetter # for RAG chain arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gDJ-vbhMbBoF"
   },
   "source": [
    "#### 1.C.2) Define Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "asD2axKqIr2S"
   },
   "source": [
    "To allow for incrementally exploring and tuning of the model, we converted the previously provided steps into a series of functions, so that new models can be run efficiently by just modifying the arguments to functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KYX7xiN8ewil"
   },
   "source": [
    "##### Document Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vFQt8unmbFYN"
   },
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "def vectorstore_wiki_docs(query, global_doc_number, text_splitter):\n",
    "    \"\"\"\n",
    "    Queries Wikipedia to retrieve documents based on a specified query, annotates them with a global document number,\n",
    "    and then splits each document into smaller parts. Each part is also indexed with a split ID. This function is\n",
    "    designed for preparing and structuring Wikipedia text data for use in vector storage or further processing.\n",
    "\n",
    "    The function performs three main steps:\n",
    "    1. Queries Wikipedia based on the given query parameter and annotates each retrieved document with a global document number\n",
    "        and the document's source (\"Wikipedia\").\n",
    "    2. Increments the global document number by one.\n",
    "    3. Uses the provided text_splitter object to split each Wikipedia document into smaller parts, each part is then annotated\n",
    "        with a unique split ID within its document.\n",
    "\n",
    "    Parameters:\n",
    "    - query (str): The search query to be used for retrieving Wikipedia documents.\n",
    "    - global_doc_number (int): The starting global document number to be assigned to the first document retrieved. This number\n",
    "                                is incremented by 1 for each new set of document retrievals.\n",
    "    - text_splitter (object): An object capable of splitting text documents into smaller parts. This object must have a\n",
    "                              method `split_documents` that takes a list of documents and returns a list of document splits.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: A tuple containing two elements:\n",
    "        - The first element is a list of the split parts of the Wikipedia documents, each part annotated with metadata including\n",
    "          a unique 'split_id' and inheriting 'doc_num' and 'doc_source' from its parent document.\n",
    "        - The second element is the updated global document number after incrementation.\n",
    "\n",
    "    Note:\n",
    "    The WikipediaLoader and its method `load` are used to retrieve documents from Wikipedia. The actual implementation\n",
    "    of WikipediaLoader and text_splitter's `split_documents` method are assumed to be defined elsewhere.\n",
    "    \"\"\"\n",
    "    # querying wikipedia\n",
    "    wiki_docs = WikipediaLoader(query=\"Generative Artificial Intelligence\", load_max_docs=4).load()\n",
    "    for idx, text in enumerate(wiki_docs):\n",
    "        wiki_docs[idx].metadata['doc_num'] = global_doc_number\n",
    "        wiki_docs[idx].metadata['doc_source'] = \"Wikipedia\"\n",
    "\n",
    "    # updating doc number\n",
    "    global_doc_number += 1\n",
    "\n",
    "    # splitting and indexing splits\n",
    "    wiki_splits = text_splitter.split_documents(wiki_docs)\n",
    "    for idx, text in enumerate(wiki_splits):\n",
    "        wiki_splits[idx].metadata['split_id'] = idx\n",
    "\n",
    "    return wiki_splits, global_doc_number\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDRiAF6TfQU7"
   },
   "source": [
    "##### Building RAG Model Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kncwh194fVAS"
   },
   "outputs": [],
   "source": [
    "def build_embedding_splitter_vectorstore(embedding_model, text_splitter,\n",
    "                                         retr_search_type = \"similarity\",\n",
    "                                         retr_k = 4,\n",
    "                                         retr_score_threshold = 0):\n",
    "  \"\"\"\n",
    "  Initializes an in-memory vector store with document embeddings for text\n",
    "  retrieval.\n",
    "  This function initializes a vector store using embeddings from the specified\n",
    "  `embedding_model` and splits documents using the provided `text_splitter`.\n",
    "  It loads documents from a pre-defined web source, splits them into segments,\n",
    "  and stores their embeddings in a Qdrant vector store. A retriever is then\n",
    "  created to facilitate document retrieval based on similarity or other specified search type.\n",
    "\n",
    "  Parameters:\n",
    "  - embedding_model (str): The name of the embedding model to use for generating document embeddings.\n",
    "  - text_splitter (object): An object capable of splitting documents into segments for embedding.\n",
    "  - retr_search_type (str, optional): The type of retrieval search to perform. Defaults to \"similarity\".\n",
    "  - retr_k (int, optional): The number of results to return for each retrieval query. Defaults to 4.\n",
    "  - retr_score_threshold (float, optional): A threshold for filtering retrieval results based on their score. If None, no threshold is applied. Defaults to None.\n",
    "\n",
    "  Returns:\n",
    "  - tuple: A tuple containing the base embeddings object, text splitter object,\n",
    "  the initialized Qdrant vector store, and the configured retriever object.\n",
    "  \"\"\"\n",
    "\n",
    "  # assigning the base imbeddings\n",
    "  base_embeddings = HuggingFaceEmbeddings(model_name=embedding_model)\n",
    "\n",
    "  # sample doc for content to initiate vectorstore\n",
    "  loader = WebBaseLoader(\n",
    "      web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "      bs_kwargs=dict(\n",
    "          parse_only=bs4.SoupStrainer(\n",
    "              class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "          )\n",
    "      ),\n",
    "  )\n",
    "\n",
    "  documents = loader.load()\n",
    "  splits = text_splitter.split_documents(documents)\n",
    "\n",
    "  # creating vector store in memory\n",
    "  qdrant_vectorstore = Qdrant.from_documents(splits,\n",
    "      base_embeddings,\n",
    "      location=\":memory:\",  # Local mode with in-memory storage only\n",
    "      collection_name=\"rag_tech_db\",\n",
    "      force_recreate=True\n",
    "  )\n",
    "\n",
    "  # assigning retreiver\n",
    "  retriever = qdrant_vectorstore.as_retriever(\n",
    "      search_type=retr_search_type,\n",
    "      search_kwargs={\n",
    "        \"k\": int(retr_k),\n",
    "        \"score_threshold\": float(retr_score_threshold) if retr_score_threshold is not None else None\n",
    "      },\n",
    "  )\n",
    "\n",
    "  return base_embeddings, text_splitter, qdrant_vectorstore, retriever\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def vectorize_documents(text_splitter, qdrant_vectorstore):\n",
    "  \"\"\"\n",
    "  Processes and vectorizes documents from multiple sources, adding them to a Qdrant vector store.\n",
    "  This function retrieves documents from a predefined list of ArXiv papers, a set of Wikipedia queries, and selected web pages. It assigns a unique number to each document, splits them into smaller chunks using the provided `text_splitter`, and enriches them with metadata (including document number, source, and split ID). These document chunks are then added to the specified `qdrant_vectorstore` for indexing and retrieval purposes.\n",
    "\n",
    "  Parameters:\n",
    "  - text_splitter (object): An object capable of splitting documents into manageable chunks for processing.\n",
    "  - qdrant_vectorstore (object): The Qdrant vector store instance where document vectors will be stored.\n",
    "\n",
    "  Returns:\n",
    "  - object: The updated Qdrant vector store containing the newly added document vectors.\n",
    "  \"\"\"\n",
    "  #assign a unique number to each document we ingest\n",
    "  global_doc_number = 1\n",
    "\n",
    "  ########### ARXIV PAPERS ############\n",
    "  arxiv_numbers = ('2005.11401', '2104.07567', '2104.09864', '2105.03011', '2106.09685', '2203.02155', '2211.09260', '2211.12561',\n",
    "                  '2212.09741', '2305.14314', '2305.18290', '2306.15595', '2309.08872', '2309.15217', '2310.06825', '2310.11511',\n",
    "                  '2311.08377', '2312.05708', '2401.06532', '2402.01306')\n",
    "\n",
    "  all_arxiv_pages = []\n",
    "\n",
    "  # loop through the papers\n",
    "  for identifier in arxiv_numbers:\n",
    "      # Construct URL using the arXiv unique identifier\n",
    "      arx_url = f\"https://arxiv.org/pdf/{identifier}.pdf\"\n",
    "\n",
    "      # Extract pages from the document and add them to the list of pages\n",
    "      arx_loader = PyMuPDFLoader(arx_url)\n",
    "      arx_pages = arx_loader.load()\n",
    "      for page_num in range(len(arx_pages)):\n",
    "          page = arx_pages[page_num]\n",
    "          #CHANGED\n",
    "          page.metadata['page_num'] = page_num\n",
    "          page.metadata['doc_num'] = global_doc_number\n",
    "          page.metadata['doc_source'] = \"ArXiv\"\n",
    "          all_arxiv_pages.append(page)\n",
    "\n",
    "      global_doc_number += 1\n",
    "\n",
    "  # index doc chunks\n",
    "  splits = text_splitter.split_documents(all_arxiv_pages)\n",
    "  for idx, text in enumerate(splits):\n",
    "      splits[idx].metadata['split_id'] = idx\n",
    "\n",
    "  # adding to vector store\n",
    "  qdrant_vectorstore.add_documents(documents=splits)\n",
    "\n",
    "\n",
    "  ########## WIKI DOCS ##############\n",
    "  queries = ['Generative Artificial Intelligence', 'Information Retrieval', 'Large Language Models']\n",
    "\n",
    "  for query in queries:\n",
    "      wiki_splits, global_doc_number = vectorstore_wiki_docs(query, global_doc_number, text_splitter)\n",
    "      # adding to vector store\n",
    "      qdrant_vectorstore.add_documents(documents=wiki_splits)\n",
    "\n",
    "  ############ LILIANWENG ############\n",
    "  web_loader = WebBaseLoader(\n",
    "      web_paths=(\"https://lilianweng.github.io/posts/2020-10-29-odqa/\",\n",
    "                \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "                \"https://lilianweng.github.io/posts/2018-06-24-attention/\"),\n",
    "\n",
    "      bs_kwargs=dict(\n",
    "          parse_only=bs4.SoupStrainer(\n",
    "              class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "          )\n",
    "      ),\n",
    "  )\n",
    "\n",
    "  web_documents = web_loader.load()\n",
    "\n",
    "  for idx, text in enumerate(web_documents):\n",
    "      web_documents[idx].metadata['doc_num'] = global_doc_number\n",
    "      web_documents[idx].metadata['doc_source'] = \"WWW\"\n",
    "  global_doc_number += 1\n",
    "\n",
    "  web_splits = text_splitter.split_documents(web_documents)\n",
    "\n",
    "  for idx, text in enumerate(web_splits):\n",
    "      web_splits[idx].metadata['split_id'] = idx\n",
    "\n",
    "  qdrant_vectorstore.add_documents(documents=web_splits)\n",
    "  return qdrant_vectorstore\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_llm(model_name, temperature=0.6, top_p=0.5):\n",
    "  \"\"\"\n",
    "  Loads a large language model (LLM) based on the specified model name and configuration.\n",
    "  Depending on the `model_name`, this function can load either a quantized version of the Mistral model from Hugging Face with specific settings for temperature and top_p, or initialize a Cohere model for text generation tasks. For the Mistral model, it applies quantization to reduce memory footprint, sets up the device mapping for efficiency, and configures the generation pipeline with specified parameters. For the Cohere model, it simply initializes it with an API key.\n",
    "\n",
    "  Parameters:\n",
    "  - model_name (str): The name of the model to load. Supports \"mistral\" or \"cohere\".\n",
    "  - temperature (float, optional): The sampling temperature to use for generation with the Mistral model. Defaults to 0.6.\n",
    "  - top_p (float, optional): The nucleus sampling (top_p) threshold to use for generation with the Mistral model. Defaults to 0.5.\n",
    "\n",
    "  Returns:\n",
    "  - object: A model pipeline object for the loaded LLM, ready for text generation tasks.\n",
    "  \"\"\"\n",
    "  if model_name == \"mistral\":\n",
    "    quantization_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                         llm_int4_enable_fp32_cpu_offload=True)\n",
    "\n",
    "    llm_mistral_model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "        torch_dtype=torch.float32,\n",
    "        device_map='auto',\n",
    "        quantization_config=quantization_config\n",
    "    )\n",
    "\n",
    "    llm_mistral_tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "\n",
    "    mistral_pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=llm_mistral_model,\n",
    "        tokenizer=llm_mistral_tokenizer,\n",
    "        max_length=1500,\n",
    "        temperature=0.6,\n",
    "        top_p=0.95,\n",
    "        do_sample=True,\n",
    "        repetition_penalty=1.2\n",
    "    )\n",
    "    mistral_pipe.model.config.pad_token_id = mistral_pipe.model.config.eos_token_id\n",
    "\n",
    "    llm_model = HuggingFacePipeline(pipeline=mistral_pipe)\n",
    "  elif model_name == \"cohere\":\n",
    "    llm_model = ChatCohere(cohere_api_key=COHERE_API_KEY)\n",
    "\n",
    "  return llm_model\n",
    "\n",
    "\n",
    "\n",
    "def print_input(input, label=\"Input to LLM\"):\n",
    "  \"\"\"\n",
    "  Prints the given input with an optional label and returns the input unchanged.\n",
    "  This function is primarily used for logging or debugging purposes, allowing the inspection of data as it flows through different stages of a pipeline or processing sequence. By printing the input with a customizable label, it facilitates the tracking of data at specific points.\n",
    "\n",
    "  Parameters:\n",
    "  - input: The data to be printed. Can be of any type that supports string representation.\n",
    "  - label (str, optional): A descriptive label to prefix the printed input, enhancing clarity. Defaults to \"Input to LLM\".\n",
    "\n",
    "  Returns:\n",
    "  - The original input, unchanged, facilitating its use in a pipeline without altering the data flow.\n",
    "  \"\"\"\n",
    "  print(f\"{label}: {input}\")\n",
    "  return input\n",
    "\n",
    "\n",
    "\n",
    "def build_RAG_prompt_chain(rag_template, llm_model, retriever, format_docs):\n",
    "  \"\"\"\n",
    "  Constructs a retrieval-augmented generation (RAG) prompt chain for question answering or text generation.\n",
    "  This function creates a RAG prompt chain pipeline that integrates document retrieval, document formatting, question generation based on a template, and text generation with a large language model (LLM). It retrieves documents relevant to a given context or question, formats these documents, injects them into a templated prompt, and then feeds the prompt to an LLM for generating a response. The output from the LLM is parsed to a string for the final response.\n",
    "\n",
    "  Parameters:\n",
    "  - rag_template (str): The template string for generating prompts that include retrieved document content.\n",
    "  - llm_model (object): The large language model pipeline object for generating text responses.\n",
    "  - retriever (object): The document retriever object for fetching relevant documents based on a query.\n",
    "  - format_docs (object): The document formatting object to structure retrieved documents before prompt generation.\n",
    "\n",
    "  Returns:\n",
    "  - object: A pipeline object representing the complete RAG prompt chain, ready for executing the full query-to-response process.\n",
    "  \"\"\"\n",
    "\n",
    "  output_parser = StrOutputParser()\n",
    "\n",
    "  logging_step1 = lambda input: print_input(input, \"After Retriever\")\n",
    "  logging_step2 = lambda input: print_input(input, \"Before LLM\")\n",
    "\n",
    "  rag_prompt = ChatPromptTemplate.from_template(rag_template)\n",
    "\n",
    "  rag_chain = (\n",
    "    {\"context\": retriever | logging_step1 | format_docs,\n",
    "     \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | logging_step2\n",
    "    | llm_model\n",
    "    | output_parser\n",
    "  )\n",
    "\n",
    "  return rag_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-RxJ9UCvfE4A"
   },
   "source": [
    "##### Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vlfvLn_re--L"
   },
   "outputs": [],
   "source": [
    "def evaluate(metrics, validation_set, rag_chain, iterations=0, verbose=True, dept_specific=False, sleep=True, print_results=False):\n",
    "  '''\n",
    "  Function that predicts answers for a validation set using a defined RAG Chain,\n",
    "  then computes the BLUE, ROUGE, & BERTScore of the predictions compared to the gold answers.\n",
    "  Returns a dataframe of those results.\n",
    "  '''\n",
    "  # initializing a dataframe for results\n",
    "  columns = ['Sample']\n",
    "  if 'bleu' in metrics:\n",
    "    columns += ['eng_bleu', 'mk_bleu']\n",
    "  if 'rouge' in metrics:\n",
    "    columns += ['eng_rouge', 'mk_rouge']\n",
    "  if 'bertscore' in metrics:\n",
    "    columns += ['eng_f1', 'mk_f1']\n",
    "  results = pd.DataFrame(columns=columns)\n",
    "\n",
    "  # setting iterations to length of validation set if not defined.\n",
    "  if iterations == 0:\n",
    "    iterations = len(validation_set)\n",
    "\n",
    "  # sorting validation keys\n",
    "  sorted_keys = sorted(validation_set.keys())\n",
    "\n",
    "  # running through metrics for specified validation question/answers\n",
    "  for i in range(min(iterations, len(sorted_keys))):\n",
    "    # retrieving example key\n",
    "    key = sorted_keys[i]\n",
    "    # assigning questions & answers\n",
    "    question = validation_set[key]['question']\n",
    "    engineering_answer = validation_set[key][\"gold_answer_research\"]\n",
    "    marketing_answer = validation_set[key][\"gold_answer_marketing\"]\n",
    "\n",
    "    if dept_specific:\n",
    "      eng_prediction = rag_chain[\"engineering\"].invoke(question)\n",
    "      mk_prediction = rag_chain[\"marketing\"].invoke(question)\n",
    "    else:\n",
    "      eng_prediction = rag_chain.invoke(question)\n",
    "      mk_prediction = rag_chain.invoke(question)\n",
    "\n",
    "    # print predictions and targets\n",
    "    if print_results:\n",
    "      print(\"-\"*60)\n",
    "      print(f\"Question: {question}\")\n",
    "      print(\" \")\n",
    "      print(f\"Engineering Prediction: {eng_prediction}\")\n",
    "      print(f\"Engineering Answer: {engineering_answer}\")\n",
    "      print(\"\")\n",
    "      print(f\"Marketing Prediction: {mk_prediction}\")\n",
    "      print(f\"Marketing Answer: {marketing_answer}\")\n",
    "\n",
    "    # calculating metrics\n",
    "    resulting_row = [key]\n",
    "    if 'bleu' in metrics:\n",
    "      en_bl = bleu.compute(predictions=[eng_prediction],\n",
    "                          references=[engineering_answer])\n",
    "      mk_bl = bleu.compute(predictions=[mk_prediction],\n",
    "                          references=[marketing_answer])\n",
    "      resulting_row += [en_bl['bleu'], mk_bl['bleu']]\n",
    "    if 'rouge' in metrics:\n",
    "      en_ro = rouge.compute(predictions=[eng_prediction],\n",
    "                          references=[engineering_answer])\n",
    "      mk_ro = rouge.compute(predictions=[mk_prediction],\n",
    "                          references=[marketing_answer])\n",
    "      resulting_row += [en_ro['rougeLsum'], mk_ro['rougeLsum']]\n",
    "    if 'bertscore' in metrics:\n",
    "      en_bs = bertscore.compute(predictions=[eng_prediction],\n",
    "                                references=[engineering_answer],\n",
    "                                lang = 'en')\n",
    "      mk_bs = bertscore.compute(predictions=[mk_prediction],\n",
    "                                references=[marketing_answer],\n",
    "                                lang = 'en')\n",
    "      resulting_row += [en_bs['f1'][0], mk_bs['f1'][0]]\n",
    "\n",
    "    results.loc[len(results)] = resulting_row\n",
    "\n",
    "    if verbose:\n",
    "      print(key, end=\" -> \")\n",
    "    # sleep process to abide by Cohere's 20 API/min requirement\n",
    "    if sleep:\n",
    "      time.sleep(4)\n",
    "\n",
    "  return results\n",
    "\n",
    "def composite_evaluation(results):\n",
    "  \"\"\"\n",
    "  Calculates composite scores for engineering and marketing prompt results,\n",
    "  and a combined total composite score, based on weighted averages of F1, ROUGE, and BLEU scores.\n",
    "\n",
    "  The function takes a dictionary `results` containing the F1, ROUGE, and BLEU scores for English\n",
    "  (`eng_f1`, `eng_rouge`, `eng_bleu`) and Macedonian (`mk_f1`, `mk_rouge`, `mk_bleu`). It calculates\n",
    "  composite scores for each language by applying specific weights to these scores: 50% to F1, 30% to ROUGE,\n",
    "  and 20% to BLEU. The total composite score is then calculated as a weighted sum of the engineering and marketing\n",
    "  composite scores, with weights of 60% and 40%, respectively.\n",
    "\n",
    "  Parameters:\n",
    "  - results (dict): A dictionary containing the F1, ROUGE, and BLEU scores for engineering and marketing. Expected keys are:\n",
    "      - 'eng_f1', 'eng_rouge', 'eng_bleu': The F1, ROUGE, and BLEU scores for engineering.\n",
    "      - 'mk_f1', 'mk_rouge', 'mk_bleu': The F1, ROUGE, and BLEU scores for marketing\n",
    "  Returns:\n",
    "  - dict: The input dictionary updated with the following keys:\n",
    "      - 'composite_eng': The composite score for English.\n",
    "      - 'composite_mk': The composite score for Macedonian.\n",
    "      - 'composite_total': The overall composite score, combining the scores for English and Macedonian.\n",
    "  \"\"\"\n",
    "  results['composite_eng'] = (0.5*results['eng_f1'] + 0.3*results['eng_rouge'] + 0.2*results['eng_bleu'])\n",
    "  results['composite_mk'] = (0.5*results['mk_f1'] + 0.3*results['mk_rouge'] + 0.2*results['mk_bleu'])\n",
    "  results['composite_total'] = 0.6*results['composite_eng'] + 0.4*results['composite_mk']\n",
    "  return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dk-gifg-4ui5"
   },
   "source": [
    "## 2) Loading & Exploring Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DEuTZYkY40mN"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# loading BLEU\n",
    "bleu = load(\"bleu\")\n",
    "\n",
    "# loading BERTScore\n",
    "bertscore = load(\"bertscore\")\n",
    "\n",
    "# loading ROUGE\n",
    "rouge = load('rouge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T7qf2BsTaZKG"
   },
   "source": [
    "Below we explore a few different metrics for evaluating our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fXge2i585b__"
   },
   "source": [
    "### BLEU Score\n",
    "\n",
    "First metric we am exploring is the BLEU score. Originally designed for evaluating the quality of language translation, this score looks at the overlap of tokens between a prediction and label.  A pitfall of this is that it does not compare meaning.\n",
    "\n",
    "A couple other cons to the BLUE score evaluation, which may be relevant to this use case:\n",
    " - Shorter predicted translations achieve higher scores than longer ones, simply due to how the score is calculated. A brevity penalty is introduced to attempt to counteract this.\n",
    " - BLEU scores can vary greatly depending on which parameters are used to generate the scores, especially when different tokenization and normalization techniques are used. It is therefore not possible to compare BLEU scores generated using different parameters, or when these parameters are unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GjOCg-BHWBvY",
    "outputId": "abd66f7c-8467-4445-f45f-c5175867a98f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 1.0, 'precisions': [1.0, 1.0, 1.0, 1.0], 'brevity_penalty': 1.0, 'length_ratio': 1.1666666666666667, 'translation_length': 7, 'reference_length': 6}\n"
     ]
    }
   ],
   "source": [
    "# test prediction and reference\n",
    "predictions = [\"hello there general kenobi\", \"foo bar foobar\"]\n",
    "references = [\n",
    "              [\"hello there general kenobi\", \"hello there !\"],\n",
    "              [\"foo bar foobar\"]\n",
    "              ]\n",
    "\n",
    "# quantifying results\n",
    "results_bleu = bleu.compute(predictions=predictions,\n",
    "                            references=references)\n",
    "\n",
    "print(results_bleu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m8XAqeMxDFzA"
   },
   "source": [
    "### ROUGE\n",
    "\n",
    "The second metric we am using is ROUGE. (Recall-Oriented Understudy for Gisting Evaluation). ROUGE mainly measures the overlap in content between automated and target summaries.  While it can capture some surface similarities, it does not fully capture semantic quality. We are primarly focused on the Rouge-L which measures the longest common subsequences between the summaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uH4AGAnpDFiu",
    "outputId": "55d6ee5f-783f-4601-c308-f14d25027075"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 1.0, 'rouge2': 1.0, 'rougeL': 1.0, 'rougeLsum': 1.0}\n"
     ]
    }
   ],
   "source": [
    "predictions = [\"hello there\", \"general kenobi\"]\n",
    "references = [\"hello there\", \"general kenobi\"]\n",
    "results_rouge = rouge.compute(predictions=predictions,\n",
    "                        references=references)\n",
    "print(results_rouge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_dGQ3nuX6nst"
   },
   "source": [
    "### BERTScore\n",
    "\n",
    "The third metric we am exploring is the BERTScore. This uses pretrained BERT model embeddings to calculate the cosine simularity between generated and target summaries.  Of this three this measures does a much better job of capturing semantic meaning between the summaries. While it likely is more robust than ROUGE and BLEU, BERTScore also may not fully capture readability or factual accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "referenced_widgets": [
      "9bec41c835444f95aa27833c05bcdfea",
      "055fc71161734a2291d46496d34bd333",
      "da726cc536d247c8865b82ad1cdf7b16",
      "de1249f3361e4dc287ec7dbc055cd876",
      "80b1613895124a84b9323077d1309946",
      "6fe6c615e8804248b83a538c52d904c7",
      "08ea40da30ee444d9aff24f284951e91",
      "7f5bfb837c034badb4987642a441cafc",
      "80b08127f5b14a4e92dd07800924ad17",
      "fe2bd04456d343a28cf913e7fb9592d1",
      "883d21a8ddf4436daef905e64998a513",
      "327563f522b047dda038e6290ca19ad5",
      "50bc4ec3d2a7486a896b7a78b9ed7c3a",
      "c171b3d8dabe4347900df968e37d2fd8",
      "6f05b02d95234ba69153d44c5a894835",
      "0279b311601a4358b6dfca1e5e5e01c2",
      "0a6f16dacd3042aa8cdbb9a1b151b643",
      "eeb7d672f16f452e98b8b9cf87100950",
      "a4348c77df0e421d81291ef10f1397d2",
      "62bc8182837c4825b1ac3c7ea326cf88",
      "0cc2893ef4444bb296f114cf26c95aa4",
      "cbba1669ed414410afedd9052f71e8d2",
      "eed117f4dc2041adae06a13c8ed27776",
      "50434e7d42f04ca4a85ca72aeb632d9d",
      "3c9418cc42f44ea99ed22daeae979796",
      "aed3137980ba4d19937bc696ef6bc8b1",
      "9e63843a93f14446a02026c1d29167a4",
      "1554078f33ec4d45859c5dc6d2df1419",
      "997e6e29b71c40a2a6fa2980fec6f6ca",
      "6b97698f732247faa50e0b73c78e6ce4",
      "34373b5a19fb4efc8935c0917576ef77",
      "e0756b4f857941699a84ccadd8ee5950",
      "3d0b077517804ee4876dc49c14e57aa0",
      "f767773861bd4a30ad2c11c33f4c079e",
      "149d6095989145b5bd1eeebf47d65bb8",
      "2075d9e638e746ee9d02066ba50e2a2b",
      "2e26125a7aad4731828c3d05dae1db22",
      "335a3a8fed8740109e0c45867a2ca436",
      "9cf15f64612346fa91c9479bc0881e3a",
      "1914fb25557646a290c89cd3fcb740c8",
      "5a3c97d479184433b8bc063dfbbadc1b",
      "bbf6a3dc755544909038770967ac7774",
      "e07ee354b4834417b0fdbb2a896e2e44",
      "83f723e718214633a41be0b88f53ae4d",
      "8556115ec56b40b8b3be5fb25921c6d7",
      "e6ecb61772d641ac9d0c20ddacd4a7d8",
      "1108367e725c4b16970238fb2cd27ca1",
      "69d74a46c49949e495aaba2a0da1d6e6",
      "45bc9824986747e7b4b9789a8166d9d1",
      "75e56dce7ea34cfca43bb8aa44784900",
      "a16af98067c14bfd8cca0058008806f9",
      "3cc329c0639847238b57297d47d36a96",
      "fb2562fed9f34104a023df39610ece1c",
      "13f7f3bf21014a2685cde4f315b5c592",
      "7fef0203903549ddb962c57057236b40",
      "5c415e2cf2494a07bd4385bc75327b66",
      "9cb6f875988b4d22a89250c75bea05a2",
      "32b3c3584eb74b698d4f891932f2f906",
      "a04dcb05cc21489d9e47a5cc6733ea75",
      "9c23448a32d24449a55421e9b8d2419a",
      "c081af5952044ee29799019f65e91678",
      "14579babec1f4a478b990726bce224fd",
      "6be7a10dc856492688d24802f8db60b5",
      "847743cff44e4829afa5ec81d8d9c237",
      "8d1010a5fd524dcfa2b4ebbfca3d758c",
      "5e965613c8034827a9dbd2a4b8270ff4"
     ]
    },
    "id": "8eHilSlI8SX7",
    "outputId": "660488ae-bbf4-4f02-f0a3-f40974e9e8dc"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bec41c835444f95aa27833c05bcdfea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "327563f522b047dda038e6290ca19ad5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eed117f4dc2041adae06a13c8ed27776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f767773861bd4a30ad2c11c33f4c079e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8556115ec56b40b8b3be5fb25921c6d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c415e2cf2494a07bd4385bc75327b66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': [1.0000001192092896, 0.9999998807907104], 'recall': [1.0000001192092896, 0.9999998807907104], 'f1': [1.0000001192092896, 0.9999998807907104], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.40.0.dev0)'}\n"
     ]
    }
   ],
   "source": [
    "# test prediction and reference\n",
    "predictions = [\"hello world\", \"general kenobi\"]\n",
    "references = [\"hello world\", \"general kenobi\"]\n",
    "\n",
    "# quantifying results\n",
    "results_bert = bertscore.compute(predictions=predictions,\n",
    "                            references=references,\n",
    "                            lang = 'en') # default model is 'roberta-large' which requires 1.4GB of storage.\n",
    "                            # model_type='roberta-large')\n",
    "print(results_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3OBCwr4QHRas"
   },
   "source": [
    "### Human Review\n",
    "\n",
    "While the intent is to utilize the above metrics to measure performance of the tuning of our models.  Human feedback will also be necessary to insure the readability and factual accuracy of the generated responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kKktTilyTy_z"
   },
   "source": [
    "## 3) Baseline Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kVO9hbgpJXCK"
   },
   "source": [
    "Now we are generating a baseline scenario to measure our progress against.  As part of this baseline we will define what we are considering our train dataset.  We will also review the various metric scores and define the specific metric that we will utilize for measuring our tuning.\n",
    "\n",
    "We are defining the paramaters for the initiated model as the baseline.  This includes the parameters highlighted in the model rebuild below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8I2VIYmttXM"
   },
   "source": [
    "### 3.A) **Rebuilding Baseline Model**\n",
    "\n",
    "We are defining our \"baseline\" model, as the model that was provided to us in the initial notebook.  \n",
    "\n",
    "Below we are rebuilding the parameters, prompts and pipelines that represented the baseline model we were provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "jNfa3o6NlOen",
    "outputId": "0c6b6fd2-8863-4b36-8d73-d459105df7b4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['llm_int4_enable_fp32_cpu_offload']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "\n",
    "embedding_model = \"multi-qa-mpnet-base-dot-v1\"\n",
    "chunk_size = 128\n",
    "chunk_overlap = 0\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "llm = \"cohere\"\n",
    "rag_template = \"\"\"[INST]Please answer the question below only based on the context information provided.\\n\\nHere is a context:\\n{context} \\n\\nHere is a question: \\n{question}.[/INST]\"\"\"\n",
    "\n",
    "base_embeddings, text_splitter, qdrant_vectorstore, retriever = build_embedding_splitter_vectorstore(embedding_model, splitter)\n",
    "qdrant_vectorstore = vectorize_documents(text_splitter, qdrant_vectorstore)\n",
    "llm_model = load_llm(llm)\n",
    "rag_chain = build_RAG_prompt_chain(rag_template, llm_model, retriever, format_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uc69VDQNv33h"
   },
   "source": [
    "### 3.B) **Single Prediction Sample**\n",
    "\n",
    "Now we review one sample question/answer pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e_SU3BraJlN8",
    "outputId": "535f8baf-ca71-4925-91a8-0dfe58403504"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  What purpose do large language models serve in the field of natural language processing?\n",
      "Engineering Answer:  Large language models (LLMs) serve the purpose of enabling general-purpose language generation and other natural language processing tasks such as classification. They achieve this by learning statistical relationships from text documents during computationally intensive self-supervised and semi-supervised training. LLMs can be used for text generation by predicting the next token or word, making them valuable for tasks like speech recognition, machine translation, and information retrieval. Additionally, LLMs have superseded previous models like recurrent neural networks, showcasing their efficiency and effectiveness in NLP tasks.\n",
      "Marketing Answer:  Large language models serve the purpose of improving performance in various natural language processing tasks, such as speech recognition, machine translation, natural language generation, optical character recognition, handwriting recognition, grammar induction, and information retrieval.\n"
     ]
    }
   ],
   "source": [
    "# selecting a sample question and answers\n",
    "question = validation_questions_answers[0]['question']\n",
    "engineering_answer = validation_questions_answers[0][\"gold_answer_research\"]\n",
    "marketing_answer = validation_questions_answers[0][\"gold_answer_marketing\"]\n",
    "print(\"Question: \", question)\n",
    "print(\"Engineering Answer: \", engineering_answer)\n",
    "print(\"Marketing Answer: \", marketing_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LY28wMASv_XI"
   },
   "source": [
    "Let's see what our model predicts for this question. . ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "id": "WXSFKKvtT0m1",
    "outputId": "75c31b41-dae0-4238-fe10-d09e8a8fc62e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Large language models serve the purpose of improving the performance of natural language processing (NLP) tasks by providing a larger and more diverse set of data for models to learn from. They are designed to understand and generate human language, and can be applied to a wide range of language-related tasks such as language generation, machine translation, text classification, and question answering. These models aim to capture the complex nuances of human language, including context, syntax, and semantics, to provide more accurate and contextually relevant responses. \\n\\nBy training on vast amounts of text data, large language models can learn to predict the next word in a sequence, generating coherent and contextually appropriate responses. They can also understand and interpret user queries, providing relevant and useful answers. This makes them extremely useful for a variety of applications, such as virtual assistants, language translation services, content generation, and sentiment analysis. \\n\\nHowever, as mentioned in the context, one of the challenges with large language models is that they can sometimes be \"distracted by irrelevant context,\" leading to potential inaccuracies or inappropriate responses. This is an active area of research, with ongoing efforts to improve the focus and relevance of these models while mitigating potential negative impacts.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# invoking the RAG chain for one question\n",
    "prediction = rag_chain.invoke(question)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6oSqv9VjwbeI"
   },
   "source": [
    "Our predicted answer is decent, and would be described as a 'middle-of-the-road' summary blending the gold standard for both research and marketing.  It appears to contain both the contents of the engineering and marketing target answers. It does not capture the 'statistical relationships' context present in the reseach answer though, indicating that it may not be designed to dive deeper into the technical aspects.  Overall the response is also too verbose for either target answer, and creeps into descriptions beyond the initial question.  As an example it appears to go onto a tangent about the short comings of the models. Now we look at what the various evaluation scores are for this answer.\n",
    "\n",
    "**BLEU**\n",
    "\n",
    "As can be seen below the BLEU scores are very low for the baseline.  This indicates that the word choices between the two models are not well aligned.  However as previously noted, this does not provide much insight into semantic similarity.  While BLEU does not provide much semantic value, we do see the value in having some measurement of n-grams between the summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eudbp0DwyY5r"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# calculating BLEU for sample answer\n",
    "test_engbleu = bleu.compute(predictions=[prediction],\n",
    "                            references=[engineering_answer])\n",
    "\n",
    "test_mkbleu = bleu.compute(predictions=[prediction],\n",
    "                            references=[marketing_answer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v44qHpfw0rpw",
    "outputId": "c2230e55-7057-4b15-946b-9fcfc03d79b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering:  {'bleu': 0.045795704397007975, 'precisions': [0.23076923076923078, 0.07296137339055794, 0.03017241379310345, 0.008658008658008658], 'brevity_penalty': 1.0, 'length_ratio': 2.4893617021276597, 'translation_length': 234, 'reference_length': 94}\n",
      "Marketing:  {'bleu': 0.05776699519634414, 'precisions': [0.12393162393162394, 0.07296137339055794, 0.04741379310344827, 0.025974025974025976], 'brevity_penalty': 1.0, 'length_ratio': 5.571428571428571, 'translation_length': 234, 'reference_length': 42}\n"
     ]
    }
   ],
   "source": [
    "print(\"Engineering: \", test_engbleu)\n",
    "print(\"Marketing: \", test_mkbleu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EOhz3BWS96XD"
   },
   "source": [
    "**ROUGE**\n",
    "\n",
    "Below are the Rouge scores for the sample answer. From the output, we are specifically focused on the **'rougeLsum'** values as that is the most holistic evaluation of the generated answer's quality, as it is evaluating the precense of phrases at a sentence level. This will act as a baseline score of how well there is sequence-based similarity between the summaries.  We will want to continue to monitor this as we tune as well to see if we can improve the similarity on phrases between the generated and target summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BR09w7n31hrO"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# calculating ROUGE for sample answer\n",
    "test_engrouge = rouge.compute(predictions=[prediction],\n",
    "                            references=[engineering_answer])\n",
    "\n",
    "test_mkrouge = rouge.compute(predictions=[prediction],\n",
    "                            references=[marketing_answer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C39GSUrh1oS-",
    "outputId": "0df1f104-ac4c-4a31-8971-1e679fb8f78e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering:  {'rouge1': 0.30344827586206896, 'rouge2': 0.10416666666666669, 'rougeL': 0.18620689655172415, 'rougeLsum': 0.19999999999999998}\n",
      "Marketing:  {'rouge1': 0.17647058823529413, 'rouge2': 0.11016949152542373, 'rougeL': 0.1680672268907563, 'rougeLsum': 0.1680672268907563}\n"
     ]
    }
   ],
   "source": [
    "print(\"Engineering: \", test_engrouge)\n",
    "print(\"Marketing: \", test_mkrouge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_PfTZkbq_oJi"
   },
   "source": [
    "**BERTScore**\n",
    "\n",
    "Below are the BERTScore's for the sample answer.  For this output, we are specifically focused on the **'F1'** scores, since this score is a score that balances the typically opposing metrics, precision and recall.  In our particular case, a balance between these is acceptable.  We are not in a 'high-stakes' situation that would require maximizing one metric over the other (i.e. medical diagnosis).\n",
    "\n",
    "As can be seen this sample answer already scores relatively well for both departments.  While the BERTScore is the more context based metric of the three, this high baseline reduces the room for improvement that we can make on this particular metric.  So we will factor that into our metric decision.\n",
    "\n",
    "*Note:  While there are numerous parameter and embedding options available for BERTscore, we have chosen to just utilize the default settings for this POC. Should this project advance, additional exploration of other parameters on this metric would be advised.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jsCIa5Nt1LAj"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# calculating BERTscore for sample answer\n",
    "test_engbert = bertscore.compute(predictions=[prediction],\n",
    "                            references=[engineering_answer],\n",
    "                            lang = 'en')\n",
    "\n",
    "test_mkbert = bertscore.compute(predictions=[prediction],\n",
    "                            references=[marketing_answer],\n",
    "                            lang = 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qjzAYTsp1RG-",
    "outputId": "21ce8e67-4da0-4b37-ed4a-ad8ed3a56c10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering:  {'precision': [0.8595860004425049], 'recall': [0.8819125890731812], 'f1': [0.8706061840057373], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.40.0.dev0)'}\n",
      "Marketing:  {'precision': [0.8385220766067505], 'recall': [0.9097399711608887], 'f1': [0.8726804256439209], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.40.0.dev0)'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Engineering: \", test_engbert)\n",
    "print(\"Marketing: \", test_mkbert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DdnSKLbdCEaY"
   },
   "source": [
    "### 3.C) **Train Set Evaluation**\n",
    "\n",
    "We will be splitting the validation question answer set into train and validation sets.  To minimize the compute requirements as we explore various parameter tuning, we are setting our exploratory train set to the first 10 examples.  As we get closer to our desired model, we will then expand to a larger dataset of 50 examples.  Once we have chosen our final model, we will then compute against all 70 of the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hM2u0JE16Qjd"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# using function to capture all metrics\n",
    "metrics = ['rouge', 'bleu', 'bertscore']\n",
    "train_baseline = evaluate(metrics, validation_questions_answers, rag_chain, iterations=10, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "kVa7OHgM7JNT",
    "outputId": "adb2741a-3089-442b-c49f-dd5ffcc99e3c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"train_baseline\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"Sample\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.373295374522416,\n        \"min\": 0.0,\n        \"max\": 13.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          6.6,\n          7.5,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eng_bleu\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.521279347350936,\n        \"min\": 0.0,\n        \"max\": 10.0,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          10.0,\n          0.0361543177208585,\n          0.05153243662070849\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mk_bleu\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.5232834858211333,\n        \"min\": 0.0,\n        \"max\": 10.0,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          10.0,\n          0.03134890816354261,\n          0.05237915232603129\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eng_rouge\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.4712669266827407,\n        \"min\": 0.04635130798206871,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.20671763352413436,\n          0.20147390324258713,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mk_rouge\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.4883808034221766,\n        \"min\": 0.032679738562091505,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.14183856287164534,\n          0.13651105651105652,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eng_f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.2867691754250696,\n        \"min\": 0.018634154395948392,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.8620860517024994,\n          0.8603556752204895,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mk_f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.289445104965232,\n        \"min\": 0.026099429388732037,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.8501676738262176,\n          0.845429927110672,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-27572eae-c5fc-446d-8150-dadeee1c88cd\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample</th>\n",
       "      <th>eng_bleu</th>\n",
       "      <th>mk_bleu</th>\n",
       "      <th>eng_rouge</th>\n",
       "      <th>mk_rouge</th>\n",
       "      <th>eng_f1</th>\n",
       "      <th>mk_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.600000</td>\n",
       "      <td>0.036154</td>\n",
       "      <td>0.031349</td>\n",
       "      <td>0.206718</td>\n",
       "      <td>0.141839</td>\n",
       "      <td>0.862086</td>\n",
       "      <td>0.850168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.788876</td>\n",
       "      <td>0.040034</td>\n",
       "      <td>0.037755</td>\n",
       "      <td>0.046351</td>\n",
       "      <td>0.082965</td>\n",
       "      <td>0.018634</td>\n",
       "      <td>0.026099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.129213</td>\n",
       "      <td>0.032680</td>\n",
       "      <td>0.837753</td>\n",
       "      <td>0.805979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.192153</td>\n",
       "      <td>0.077106</td>\n",
       "      <td>0.848590</td>\n",
       "      <td>0.838420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.500000</td>\n",
       "      <td>0.034476</td>\n",
       "      <td>0.020574</td>\n",
       "      <td>0.201474</td>\n",
       "      <td>0.136511</td>\n",
       "      <td>0.860356</td>\n",
       "      <td>0.845430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>10.500000</td>\n",
       "      <td>0.051532</td>\n",
       "      <td>0.052379</td>\n",
       "      <td>0.212566</td>\n",
       "      <td>0.176126</td>\n",
       "      <td>0.870040</td>\n",
       "      <td>0.871407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>0.124233</td>\n",
       "      <td>0.103572</td>\n",
       "      <td>0.298969</td>\n",
       "      <td>0.305556</td>\n",
       "      <td>0.891206</td>\n",
       "      <td>0.886961</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-27572eae-c5fc-446d-8150-dadeee1c88cd')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-27572eae-c5fc-446d-8150-dadeee1c88cd button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-27572eae-c5fc-446d-8150-dadeee1c88cd');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-7926cce0-421b-433c-896a-cc79497ecaa0\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7926cce0-421b-433c-896a-cc79497ecaa0')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-7926cce0-421b-433c-896a-cc79497ecaa0 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "          Sample   eng_bleu    mk_bleu  eng_rouge   mk_rouge     eng_f1  \\\n",
       "count  10.000000  10.000000  10.000000  10.000000  10.000000  10.000000   \n",
       "mean    6.600000   0.036154   0.031349   0.206718   0.141839   0.862086   \n",
       "std     4.788876   0.040034   0.037755   0.046351   0.082965   0.018634   \n",
       "min     0.000000   0.000000   0.000000   0.129213   0.032680   0.837753   \n",
       "25%     2.250000   0.000000   0.000000   0.192153   0.077106   0.848590   \n",
       "50%     7.500000   0.034476   0.020574   0.201474   0.136511   0.860356   \n",
       "75%    10.500000   0.051532   0.052379   0.212566   0.176126   0.870040   \n",
       "max    13.000000   0.124233   0.103572   0.298969   0.305556   0.891206   \n",
       "\n",
       "           mk_f1  \n",
       "count  10.000000  \n",
       "mean    0.850168  \n",
       "std     0.026099  \n",
       "min     0.805979  \n",
       "25%     0.838420  \n",
       "50%     0.845430  \n",
       "75%     0.871407  \n",
       "max     0.886961  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_baseline.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-TnRlvHgdms-"
   },
   "source": [
    "### 3.D) **Composite Evaluation Metric**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eA5PUQREQs_Y"
   },
   "source": [
    "After reviewing the training set scores that we are getting for the three separate metrics, we have determined that we want to create a 'composite' metric that is a weighted combination of the three. Given the individual normalized scores for BLEU, ROUGE-Lsum, and BERTScore F1 for both Engineering (eng) and Marketing (mk), the composite score can be calculated as follows:\n",
    "\n",
    "<br>\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Composite Score}_{\\text{eng}} &= w_{\\text{BLEU}} \\cdot N_{\\text{BLEU}_{\\text{eng}}} + w_{\\text{ROUGE-Lsum}} \\cdot N_{\\text{ROUGE-Lsum}_{\\text{eng}}} + w_{\\text{BERTScoreF1}} \\cdot N_{\\text{BERTScoreF1}_{\\text{eng}}} \\\\\n",
    "\\text{Composite Score}_{\\text{mk}} &= w_{\\text{BLEU}} \\cdot N_{\\text{BLEU}_{\\text{mk}}} + w_{\\text{ROUGE-Lsum}} \\cdot N_{\\text{ROUGE-Lsum}_{\\text{mk}}} + w_{\\text{BERTScoreF1}} \\cdot N_{\\text{BERTScoreF1}_{\\text{mk}}}\n",
    "\\end{align*}\n",
    "\n",
    "<br>\n",
    "\n",
    "where $w_{\\text{BLEU}}$, $w_{\\text{ROUGE-Lsum}}$, and $w_{\\text{BERTScoreF1}}$ are the weights for the BLEU, ROUGE-Lsum, and BERTScore F1 scores respectively, and $N_{\\text{Metric}_{\\text{dept}}}$ represents the normalized score for each metric in each department (Engineering or Marketing).\n",
    "\n",
    "We are choosing the following weights based off the importance that each metric brings to evaluating our generated answers. With BERT being best at measuring the contextual similarity between the summaries, it is given the most weight. Then ROUGE, with its measurement of phrase-based similarities, and lastly BLEU providing specific word choice similiarities. Below are the weights:\n",
    "\\begin{align*}\n",
    "    w_{\\text{BLEU}} &= 0.2 \\\\\n",
    "    w_{\\text{ROUGE-Lsum}} &= 0.3 \\\\\n",
    "    w_{\\text{BERTScoreF1}} &= 0.5\n",
    "\\end{align*}\n",
    "\n",
    "We also decided to weight the importance of the responses between the departments.  We understand that this is just a POC, and that should a production system be implemented, it would likely contain much more content in the document store, and with that a significant increase in the technical jargon and complexity within those documents. With the company having 300 engineers and 40 marketers, this system will likely see more usage from engineers.  Also in addition to that, the importance of accuractly containing specific (technical) details is more heavily weighted towards engineering. Marketing requirements will be higher-level and more generalized, and with such will not be as detrimented by missing specifics.  This this we are choosing the following departmental weighting:\n",
    "\n",
    "\\begin{align*}\n",
    "    w_{\\text{eng}} &= 0.6 \\\\\n",
    "    w_{\\text{mk}} &= 0.4\n",
    "\\end{align*}\n",
    "\n",
    "<br>\n",
    "The final composite score, adjusted for departmental impact, is calculated as:\n",
    "\n",
    "<br>\n",
    "\n",
    "\\begin{equation*}\n",
    "\\text{Final Composite Score} = w_{\\text{eng}} \\cdot \\text{Composite Score}_{\\text{eng}} + w_{\\text{mk}} \\cdot \\text{Composite Score}_{\\text{mk}}\n",
    "\\end{equation*}\n",
    "\n",
    "<br>\n",
    "\n",
    "This equation provides a framework to evaluate the performance of the RAG system, taking into account the precision, recall, semantic similarity, and the strategic importance of each department to the company. Below is that calculuation for the train dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "p2k4VcxKTwOS",
    "outputId": "725196e4-5499-4c89-c595-8cf01de4b4e0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"train_baseline\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"Sample\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.373295374522416,\n        \"min\": 0.0,\n        \"max\": 13.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          6.6,\n          7.5,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eng_bleu\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.521279347350936,\n        \"min\": 0.0,\n        \"max\": 10.0,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          10.0,\n          0.0361543177208585,\n          0.05153243662070849\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mk_bleu\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.5232834858211333,\n        \"min\": 0.0,\n        \"max\": 10.0,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          10.0,\n          0.03134890816354261,\n          0.05237915232603129\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eng_rouge\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.4712669266827407,\n        \"min\": 0.04635130798206871,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.20671763352413436,\n          0.20147390324258713,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mk_rouge\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.4883808034221766,\n        \"min\": 0.032679738562091505,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.14183856287164534,\n          0.13651105651105652,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eng_f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.2867691754250696,\n        \"min\": 0.018634154395948392,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.8620860517024994,\n          0.8603556752204895,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mk_f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.289445104965232,\n        \"min\": 0.026099429388732037,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.8501676738262176,\n          0.845429927110672,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"composite_eng\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.3871671192315786,\n        \"min\": 0.025551513431532443,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.5002891794526618,\n          0.49647846168858045,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"composite_mk\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.3930527179723917,\n        \"min\": 0.04382590347534562,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.47390518740731097,\n          0.46607894963058644,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"composite_total\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.3897471477417906,\n        \"min\": 0.027880522678073637,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.48973558263452144,\n          0.4811967272910731,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-462ebe1b-7442-47b3-8f81-fd8a3ad05aa6\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample</th>\n",
       "      <th>eng_bleu</th>\n",
       "      <th>mk_bleu</th>\n",
       "      <th>eng_rouge</th>\n",
       "      <th>mk_rouge</th>\n",
       "      <th>eng_f1</th>\n",
       "      <th>mk_f1</th>\n",
       "      <th>composite_eng</th>\n",
       "      <th>composite_mk</th>\n",
       "      <th>composite_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.600000</td>\n",
       "      <td>0.036154</td>\n",
       "      <td>0.031349</td>\n",
       "      <td>0.206718</td>\n",
       "      <td>0.141839</td>\n",
       "      <td>0.862086</td>\n",
       "      <td>0.850168</td>\n",
       "      <td>0.500289</td>\n",
       "      <td>0.473905</td>\n",
       "      <td>0.489736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.788876</td>\n",
       "      <td>0.040034</td>\n",
       "      <td>0.037755</td>\n",
       "      <td>0.046351</td>\n",
       "      <td>0.082965</td>\n",
       "      <td>0.018634</td>\n",
       "      <td>0.026099</td>\n",
       "      <td>0.025552</td>\n",
       "      <td>0.043826</td>\n",
       "      <td>0.027881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.129213</td>\n",
       "      <td>0.032680</td>\n",
       "      <td>0.837753</td>\n",
       "      <td>0.805979</td>\n",
       "      <td>0.457640</td>\n",
       "      <td>0.412793</td>\n",
       "      <td>0.439702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.192153</td>\n",
       "      <td>0.077106</td>\n",
       "      <td>0.848590</td>\n",
       "      <td>0.838420</td>\n",
       "      <td>0.488342</td>\n",
       "      <td>0.442528</td>\n",
       "      <td>0.477322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.500000</td>\n",
       "      <td>0.034476</td>\n",
       "      <td>0.020574</td>\n",
       "      <td>0.201474</td>\n",
       "      <td>0.136511</td>\n",
       "      <td>0.860356</td>\n",
       "      <td>0.845430</td>\n",
       "      <td>0.496478</td>\n",
       "      <td>0.466079</td>\n",
       "      <td>0.481197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>10.500000</td>\n",
       "      <td>0.051532</td>\n",
       "      <td>0.052379</td>\n",
       "      <td>0.212566</td>\n",
       "      <td>0.176126</td>\n",
       "      <td>0.870040</td>\n",
       "      <td>0.871407</td>\n",
       "      <td>0.502899</td>\n",
       "      <td>0.497392</td>\n",
       "      <td>0.510889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>0.124233</td>\n",
       "      <td>0.103572</td>\n",
       "      <td>0.298969</td>\n",
       "      <td>0.305556</td>\n",
       "      <td>0.891206</td>\n",
       "      <td>0.886961</td>\n",
       "      <td>0.548372</td>\n",
       "      <td>0.555862</td>\n",
       "      <td>0.538268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-462ebe1b-7442-47b3-8f81-fd8a3ad05aa6')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-462ebe1b-7442-47b3-8f81-fd8a3ad05aa6 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-462ebe1b-7442-47b3-8f81-fd8a3ad05aa6');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-78bad548-3a05-4adb-8ae0-532f32501a97\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-78bad548-3a05-4adb-8ae0-532f32501a97')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-78bad548-3a05-4adb-8ae0-532f32501a97 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "          Sample   eng_bleu    mk_bleu  eng_rouge   mk_rouge     eng_f1  \\\n",
       "count  10.000000  10.000000  10.000000  10.000000  10.000000  10.000000   \n",
       "mean    6.600000   0.036154   0.031349   0.206718   0.141839   0.862086   \n",
       "std     4.788876   0.040034   0.037755   0.046351   0.082965   0.018634   \n",
       "min     0.000000   0.000000   0.000000   0.129213   0.032680   0.837753   \n",
       "25%     2.250000   0.000000   0.000000   0.192153   0.077106   0.848590   \n",
       "50%     7.500000   0.034476   0.020574   0.201474   0.136511   0.860356   \n",
       "75%    10.500000   0.051532   0.052379   0.212566   0.176126   0.870040   \n",
       "max    13.000000   0.124233   0.103572   0.298969   0.305556   0.891206   \n",
       "\n",
       "           mk_f1  composite_eng  composite_mk  composite_total  \n",
       "count  10.000000      10.000000     10.000000        10.000000  \n",
       "mean    0.850168       0.500289      0.473905         0.489736  \n",
       "std     0.026099       0.025552      0.043826         0.027881  \n",
       "min     0.805979       0.457640      0.412793         0.439702  \n",
       "25%     0.838420       0.488342      0.442528         0.477322  \n",
       "50%     0.845430       0.496478      0.466079         0.481197  \n",
       "75%     0.871407       0.502899      0.497392         0.510889  \n",
       "max     0.886961       0.548372      0.555862         0.538268  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_baseline = composite_evaluation(train_baseline)\n",
    "train_baseline.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KGHD8uJSKPZa"
   },
   "source": [
    "**Our baseline model returns a 'Composite Score' of 0.489736.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cPTEbaMP-7G6"
   },
   "source": [
    "### 3.E) Reviewing Baseline Model Document Context Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edfXRp0uSsEY"
   },
   "source": [
    "Now we perform some additional EDA on the train set. Exploring some of the interim steps within the RAG system. Below we look at what we retrieving what we are getting as context from our docstore and feeding to the LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oZET5MJ4Sn8w",
    "outputId": "f1a6d661-4936-448b-ed1e-9a461771e7ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before LLM: messages=[HumanMessage(content='[INST]Please answer the question below only based on the context information provided.\\n\\nHere is a context:\\namong the largest language models today and we apply them on a wide range of language tasks,\\n\\nlimitations, and societal impact of large language models. arXiv preprint arXiv:2102.02503.\\n\\narli, and Denny Zhou. Large language models can be easily distracted by irrelevant context.\\n\\nimportant for language models that are deployed and used in hundreds of applications. \\n\\nHere is a question: \\nWhat purpose do large language models serve in the field of natural language processing?.[/INST]')]\n",
      "Before LLM: messages=[HumanMessage(content='[INST]Please answer the question below only based on the context information provided.\\n\\nHere is a context:\\namong the largest language models today and we apply them on a wide range of language tasks,\\n\\nlimitations, and societal impact of large language models. arXiv preprint arXiv:2102.02503.\\n\\narli, and Denny Zhou. Large language models can be easily distracted by irrelevant context.\\n\\nimportant for language models that are deployed and used in hundreds of applications. \\n\\nHere is a question: \\nWhat purpose do large language models serve in the field of natural language processing?.[/INST]')]\n",
      "0 -> "
     ]
    }
   ],
   "source": [
    "rag_chain = build_RAG_prompt_chain(rag_template, llm_model, retriever, format_docs)\n",
    "metrics = ['rouge', 'bleu', 'bertscore']\n",
    "train_baseline = evaluate(metrics, validation_questions_answers, rag_chain, iterations=1, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u81afpgwWG4U"
   },
   "source": [
    "Below is the actual prompt to the llm reformated for clarity:\n",
    "\n",
    "<blockquote>\n",
    "\n",
    "'[INST]Please answer the question below only based on the context information provided.\n",
    "\n",
    "Here is a context:\n",
    "\n",
    "among the largest language models today and we apply them on a wide range of language tasks,\n",
    "\n",
    "limitations, and societal impact of large language models. arXiv preprint arXiv:2102.02503.\n",
    "\n",
    "arli, and Denny Zhou. Large language models can be easily distracted by irrelevant context.\n",
    "\n",
    "important for language models that are deployed and used in hundreds of applications.\n",
    "\n",
    "Here is a question:\n",
    "\n",
    "What purpose do large language models serve in the field of natural language processing?.[/INST]'\n",
    "\n",
    "</blockquote>\n",
    "\n",
    "While the content is relevant to large language models, none of them are directly aiding the question being asked and instead provide some distracting content, like a blurb on how LLM can get easily distracted (see what I did there. . .).  Based on the generated summaries we were recieving, it appears the LLM might be filling in a lot of the context gaps on its own to arrive at adequate answers.  We will be focusing some significant training energy on providing the llm more content to work with to answer questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WxG6syIO96qz"
   },
   "source": [
    "## 4) Fine Tuning Model and Parameter Choices\n",
    "\n",
    "Here we are now exploring some of the fine tuning available to improve our model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wRnCzu5AAQI7"
   },
   "source": [
    "#### 4.A) **Exploring embedding options**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yebv_e04EQJZ"
   },
   "source": [
    "We were provided three model options for embeddings: 'multi-qa-mpnet-base-dot-v1', 'all-MiniLM-L6-v2', 'avsolatorio/GIST-Embedding-v0', with the first one being the default.  After some research it appears that 'all-MiniLM-L6-v2' is a smaller model that is built for efficiency, but at the general sacrifice of performance.  However, we will still test it's performance to confirm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 978,
     "referenced_widgets": [
      "68aa905c7ba24fcebe224084baaead28",
      "a5ebf730d7424f19989a6da524a25fbb",
      "f29e5a964cb34c9b8c137f66eb19bbb1",
      "c9865b9309be4d6b97faa80e91445254",
      "0a0199431c1f4f07b1741468c9f1d114",
      "04f435a02ed0473cbb2c20286f7d69c0",
      "6740dbdd02fd4861af1b9c9410a29ecc",
      "b83e0e5894694527915b0877105d68d5",
      "ef2fcee2a6354921bced437f08abae5f",
      "5e9a6c88452c402f9afbf550d7670add",
      "26c970f739694c7fb39d7265bcc1cd42",
      "deb04b39086d4b5fab3663bb251ac7c3",
      "8f3a0ac9cd5e44508434e1aaa13c5ccd",
      "9f5feea658c044ca8f725685d44c53e7",
      "a86c0daf8f554702a794e68160114cb5",
      "ad6bfab680804b329345964c6f8bd8ba",
      "3b723473f56744339ac7fe2c84b2132d",
      "fe48ac8a18bd4042ab10a4ce2a19d75d",
      "eaaa36fed39f4127a983fd6a0567babe",
      "7cfefd424b304890a5894e83368e7bcb",
      "72c48a1476ab475785943437d1542911",
      "508290f007d84574b9f44bcff490bff4",
      "eebde3ca3de740758b01bf5d929279d1",
      "fc165979f4c94c288f6529a1c092b58a",
      "ae0cd7cd24af4b1baec8dff607c082d1",
      "7c6c6f53d0c84d3ebf5ab6a3622becab",
      "f71582a8837b4758bbf167e96edd02d6",
      "6688ecfa60ac43ba8ed96530e18d231f",
      "c6720dd433aa48f3a3871dd8e45bd70b",
      "f41dc475cde447efb6af54340a0f0fd7",
      "a83a1c079bf347a29481d080558c2ae6",
      "313af608a8e44338b4baee58091a5edd",
      "0e2cd61b60974115ac8beae93ebd4287",
      "98664e3b3ae74ccea5b4d2be8d6fac80",
      "a5ea699b5e274823beaa1cc80be7274c",
      "934879e22b324c6e8d2a5fa50bafc891",
      "499c89fd510f42e88ea5e63f484ec803",
      "e010fafde1f24938beb913566bcdcaae",
      "b6e2b14696f347e4b4082f4ab3039b0e",
      "4483e4e4700a47399722045190e7d397",
      "f2f75194a2ce4c78aaf09a2c6750ab93",
      "7e3ae2ca80e54433b5bad299692f3788",
      "49653cc5c85d4ded885e47ba760c0dc3",
      "04f229f8b5d847e9bfed5ffd6b78278d",
      "3c1e8199c27d4024ba5e24e30d35e43c",
      "26419e544ac54982b4d6785fdcc3e806",
      "596bd7c1d833426dab41ef79c4ca286d",
      "159bf6a876f247bda3d8b6097739c1cb",
      "084ec135b91a4b8285220d285809bbdc",
      "7f7a8e3453734b388f0e7380ace7e339",
      "5b0ba54016b3462fb321037162eec2b6",
      "5a66cca774ad48a69b2364989926576b",
      "20b6c8bf2505415ea451b2c2fdf28cbd",
      "2276c89996004e8e99c1df0fecd69d89",
      "2b667c4db1134a5e983696072aa8212e",
      "16258087eda641e9ba243837d5974c8c",
      "f67abb31961845d6ba6422aabd790ca3",
      "22b23fcc2a774731b410521e5cb8de7e",
      "c1f3804d62584cc182cd13f90c299f0f",
      "97f5cba66e764e6e8a5dab9caf08c3bc",
      "f85faf7c29dc4c26bf2c4fb0663eb1ff",
      "3481cf26144d495bb16512fb64f52abe",
      "2d89f99001d94c96a76a970b8dd6c4b5",
      "6ab40f4d10314413abb7a1435900c596",
      "24944ca627c746188d109df655eaa4de",
      "9b6461ed30224cf3a18ad36dae0c7023",
      "eb8a4272c93d419abe5ae95b02e2f0b4",
      "435e6838c65b4957953fc0ee88a9b882",
      "644b24f94f85412b8feef0f625ef8fa5",
      "0af94016632446c1a86e75f4f51a7f0f",
      "e37f776845ca483eb9bf94e71abdc825",
      "0dfa37c24f424776b3d15f8f0de1af0c",
      "27f80615837649a68e938d7beecd3bf6",
      "a6a38030c9bb41958543a91f72b4d6cc",
      "7a36340e31fd49beb920bad19aef01a6",
      "504401a87e23485784ccf9d3957a37da",
      "10021f5d22a747688c8f8f58a2bd406a",
      "62781bbe27384f2197cbc935d5e8423d",
      "40827ab62918499d987d6d2824b9ddd0",
      "3940939e0d31433c8da8d355e1bee8c5",
      "f5f62b1b55fc4b8a9100015de19f22cc",
      "8e2ed5154fd4415a8bd5f4437ddc2e55",
      "8a5bece63f1545c4ae5ac1d080596af4",
      "12e9e208ec6e41beb10df311907ff5e3",
      "23b8aaecb2384c85a8d12a4cc39d513a",
      "b013050f923b45c3ab77eb7069cadf47",
      "7592f515d48346b380c03d1ebe979350",
      "247f3b37a349499fb88185e01f7ce359",
      "a1c5bc04620c440e9e12b2a5cd082e4d",
      "b414adada5384d4880830339139d38e4",
      "0742615e751f4e7b9278237ba28c4a80",
      "3de167a1d20b4367a6bff8ea661edd7f",
      "e29e81fb1be4460d8d277eb074a26a3c",
      "29f741fffd404c8c87df8fa7a62d8d36",
      "4e6627df1f3e455d85e07c9a44f8ef52",
      "3bbe80f266ea43eeb6946c24242f6db6",
      "e611a30d9fac4b10ae4e39c49c233512",
      "b26e7ad1b33a442c942bb1701c31ebd9",
      "b28a3699532340498a50915bb860111a",
      "ab5338e1673044a7a535241202bfca1f",
      "f71d0b1a2e53451696fe1abae9cd4bb6",
      "5cd5d029725847119ee7ffbdaa6969ce",
      "71625362a863438e9dc3dcd5f71b952a",
      "e092acfe72b84945bafcfa53cdfe69b1",
      "e6ee5c03dc0a4275a1e0f1f96ffb1904",
      "2f63831c1dee463f8e5f56fa516d48a8",
      "36a4812c37204c98b257cc959a2b7278",
      "f2d526cf04284a79862b3b454b0dc216",
      "03b1a1106cc8493b971c534c9413a010",
      "741e8028c3b64415b01c0a658ea5d600",
      "2f0c47366b15435fa6a47666a76a405e",
      "3b20c1f15f104fef9d35b0f765670edc",
      "f1ec39a6c3d742d1b36735d3303ef7ca",
      "bf17bd1fa8a943ed93c564f691edf5f3",
      "3e8927492a8e4af7aba8339da1d9a454",
      "9cb79e0763a54f1abce148fbdce93c59",
      "ade46f9756bc415995865e9684681c7c",
      "46ebcdd0ed9d48c08952e6d137881455",
      "0e108f1f1fe242748e5e99342436765a",
      "19a8531b90114368bd0c6298f136d760",
      "49b9f8f3d38b4094bee6916d5318a445"
     ]
    },
    "id": "atbIMgYfAWiU",
    "outputId": "32efe07a-6e65-4fd5-b999-4779992df2d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Index  0\n",
      "Embedding Model: avsolatorio/GIST-Embedding-v0\n",
      "0 -> 1 -> 2 -> 3 -> 7 -> 8 -> 9 -> 11 -> 12 -> 13 -> Sample       6.600000\n",
      "eng_bleu     0.033869\n",
      "mk_bleu      0.021581\n",
      "eng_rouge    0.194728\n",
      "mk_rouge     0.121817\n",
      "eng_f1       0.855194\n",
      "mk_f1        0.839183\n",
      "dtype: float64\n",
      "\n",
      "Index  1\n",
      "Embedding Model: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68aa905c7ba24fcebe224084baaead28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deb04b39086d4b5fab3663bb251ac7c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eebde3ca3de740758b01bf5d929279d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98664e3b3ae74ccea5b4d2be8d6fac80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c1e8199c27d4024ba5e24e30d35e43c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16258087eda641e9ba243837d5974c8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb8a4272c93d419abe5ae95b02e2f0b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62781bbe27384f2197cbc935d5e8423d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1c5bc04620c440e9e12b2a5cd082e4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab5338e1673044a7a535241202bfca1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f0c47366b15435fa6a47666a76a405e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -> 1 -> 2 -> 3 -> 7 -> 8 -> 9 -> 11 -> 12 -> 13 -> Sample       6.600000\n",
      "eng_bleu     0.051392\n",
      "mk_bleu      0.026366\n",
      "eng_rouge    0.225280\n",
      "mk_rouge     0.132156\n",
      "eng_f1       0.865265\n",
      "mk_f1        0.847400\n",
      "dtype: float64\n",
      "\n",
      "Index  2\n",
      "Embedding Model: multi-qa-mpnet-base-dot-v1\n",
      "0 -> 1 -> 2 -> 3 -> 7 -> 8 -> 9 -> 11 -> 12 -> 13 -> Sample       6.600000\n",
      "eng_bleu     0.042218\n",
      "mk_bleu      0.024269\n",
      "eng_rouge    0.204649\n",
      "mk_rouge     0.127378\n",
      "eng_f1       0.862459\n",
      "mk_f1        0.846005\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# defining model parameters\n",
    "chunk_size = 128\n",
    "chunk_overlap = 0\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "llm = \"cohere\"\n",
    "rag_template = \"\"\"[INST]Please answer the question below only based on the context information provided.\\n\\nHere is a context:\\n{context} \\n\\nHere is a question: \\n{question}.[/INST]\"\"\"\n",
    "\n",
    "# embedding models to iterate through\n",
    "embedding_models = ['avsolatorio/GIST-Embedding-v0',  'all-MiniLM-L6-v2', 'multi-qa-mpnet-base-dot-v1']\n",
    "\n",
    "results = []\n",
    "index = 0\n",
    "# for each embedding model, completing the RAG chain and evaluating.\n",
    "for embedding_model in embedding_models:\n",
    "  print(\"\")\n",
    "  print(\"Index \", index)\n",
    "  index += 1\n",
    "  print(f\"Embedding Model: {embedding_model}\")\n",
    "\n",
    "  #building model for current embedding\n",
    "  base_embeddings, text_splitter, qdrant_vectorstore, retriever = build_embedding_splitter_vectorstore(embedding_model, splitter)\n",
    "  qdrant_vectorstore = vectorize_documents(text_splitter, qdrant_vectorstore)\n",
    "  llm_model = load_llm(llm)\n",
    "  rag_chain = build_RAG_prompt_chain(rag_template, llm_model, retriever, format_docs)\n",
    "\n",
    "  # computing metrics\n",
    "  metrics = ['rouge', 'bleu', 'bertscore']\n",
    "  embed_result = evaluate(metrics, validation_questions_answers, rag_chain, iterations=10, verbose=True, sleep=False)\n",
    "  print(embed_result.mean())\n",
    "  results.append(embed_result)\n",
    "\n",
    "  # deleting previous variables\n",
    "  del qdrant_vectorstore\n",
    "  del text_splitter\n",
    "  del llm_model\n",
    "  del rag_chain\n",
    "  del base_embeddings\n",
    "  del retriever\n",
    "  gc.collect() # forcing garbage collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HmneGv9myEXP",
    "outputId": "6299960c-4e02-45c7-f210-597dc901dc2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: , 0\n",
      "0.479854879028463\n",
      "Index: , 1\n",
      "0.4937449646786769\n",
      "Index: , 2\n",
      "0.48706869462383756\n"
     ]
    }
   ],
   "source": [
    "# calculating the composite score for each embedding\n",
    "for i, result in enumerate(results):\n",
    "  print(\"Index: ,\", i)\n",
    "  result = composite_evaluation(result)\n",
    "  print(result['composite_total'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DFm8Gn4QYjvU"
   },
   "source": [
    "**Context Check:** To our surprise the 'Mini' model actually performed the best.  Now we want to take a look at the context retrieval to see if we see any noticeable difference in using this embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444,
     "referenced_widgets": [
      "6926235886e64a29a4bd945c083ed89f",
      "e66dae1012b54a4a98f223c0560b695f",
      "833b764e721b4304aaaa97a3f5d0a399",
      "664db7525bfb457a9fbffdd31908c52b",
      "9d82de220e4f44ff8eff707bb4ab0691",
      "044c1758656841c086ab105e5c2977d0",
      "8e23598a3334441a82d0f7c4b3eff0f1",
      "0ef5557a48864cf390c9f63ab394636f",
      "f8a9268a04034142b3a3ace39c75753f",
      "592a25432dba4625bfdb3ba0ee7054c1",
      "09803fd745254cb9901e94411bb71540",
      "011889a215454ebdace22dc8f02f457f",
      "8194ebcecbf244688b70a771de7cf75b",
      "19aa00338d3f4e78878dcffad74fafcf",
      "45814158a33b46efb1c5b3e5dd55f73e",
      "07d75d8cf7ad459bb93e3a14c4b873ef",
      "75120fbec53943e8a30e589a4b26c2ba",
      "876a8d99290a4e37b1c780f91fcfeda2",
      "7ca5b258f67e46a185ca8fa7e647ca50",
      "1579c889649a49b4989273662897fb8d",
      "f82d9ee7d5c041cb800769838a501c24",
      "0dc891cc2eec4d27b73774782bca2d6b",
      "120547a0deb74a69bce43b08f7e453e0",
      "1a50ddf4eeff436f8dd87dccfbe497cf",
      "161081ea350b41e1ab4228eb51cfcc4f",
      "653af0f977154c2b8a5336893efac036",
      "0091c32769f14b83b0ca4e5ba87f1d13",
      "eaa6409e563c436386c0257444bc2a5c",
      "ba5fc8c18c5b4f47a2a7eaeae5278591",
      "e269f2ae0ddd4265bfe01ae0b9c1c9ff",
      "a021d0a6f40b4b53b8c81279ad657a0f",
      "25450062f6894e3487bf642c568f618b",
      "4206b8c4e11445e180f91dee84f23a1a",
      "98b7173dcbec416299d39c5da6971ccf",
      "314278a71ef946988bb8f5fc97fadfa4",
      "3f0f2022b56a4853bf62f37d393ab5f0",
      "4954ee9997f149dfa0c63dbb5fa36160",
      "b9cfdcf1172c467a9430fab02aebee93",
      "10de2dae49d94481946a8d20e800cff4",
      "3c49589aa8f04a91884ca44d20aba1c5",
      "7426ddf30ae94afd92856cc87c31ea0c",
      "e503346ab4a347a39ba17ccb401bbebe",
      "8c7d23e2415241c984bc295fab2fab7d",
      "33288a3ef7e04f4c9fa43cfa376f942d",
      "aea1b1fb4cf64fefbdc388ec74b22287",
      "f4f2719436674ce88d287701ebf9516c",
      "6cb66d7257e94658a61eb5b0b92242f9",
      "d88cc523ee5c43598ba2046e539cc8d3",
      "5a410e134013469ba387648575d5b944",
      "9fc83b6769b0489a83ac6a5853cfe7a4",
      "8c4dcb776cdf4426968093252bc5d631",
      "addf6ae6f9cd4227aa0fb77bfc43edad",
      "4fe39f51db8c4e65b41e0441f3e0a9b1",
      "64bd611054c54bc493a936dd87a316f0",
      "ef6dd12c633e4366a34a07a53de950d1",
      "ae4ab0cbd7dc45c9ad96163ab3d1dc91",
      "ed4fe0f8fd6145c7b0b8e1e6e7cd6206",
      "02e4827845d7458b850f63b63be1bbdd",
      "a0add11803794b2eb1061baa6b15e77b",
      "85d5a23a50c548158a56b0fb2cd70ae1",
      "b4e962948e2e4c0b903529ccd579b9d5",
      "ff08870e519f43c8865b284ed065dd97",
      "0b2072323ce840f88690bc72dcebd357",
      "2fa8f710890a4ea69d0b3806aa4346fc",
      "54e4009c06844525874494f303ba2b86",
      "ee175110fea7417f855dadef69e1a64e",
      "a61fb28b1c154e22895ce8b67a29f119",
      "1b23976143f643c08871d5c17f364036",
      "f2257b285f4d4f4b83b602d7cc6eaaf8",
      "69bf13c4cece4f3eab1219b4788e058a",
      "861d51b05a4f440b83239aeb60ca3f65",
      "53975da8ecb5463d873d4674629b180d",
      "67b79b611a32439fb6c9e16436b8dd19",
      "c7c52a5f98654f9eafe6150ee4e894a7",
      "9145d0e2f8f34806b847a5d22066a8dd",
      "993b525d8f304fb89faacabb5d3b4b53",
      "357923f84865432d8f4a5c06b5311f09",
      "b9af1242884d4a05a90261513d42b330",
      "6631c98379f64b20927e621c5397b27c",
      "0956b86c701f436e844f0c37aee45662",
      "75e2eb26982f42ceb9de039e04b82a48",
      "0aefb5ec218c4048b660662b54cec77f",
      "f2c85271acf8462ebec8ce09fd057b2f",
      "bc3ffd2d1e8b485f9c2111848f810a0a",
      "db8645b9f2d34d568699766a7b396281",
      "c2268477307043779d6c2070342774ad",
      "5ee303ba267e4294a8d5b72a16a886c3",
      "a264344c05d043feb64d4cabb7d03233",
      "0fcdecd7f17d457ba197d7dade252823",
      "f6c77a3adb4e4093802838a15a583a9d",
      "3b7ca4878ae5411ca8b6271919f06223",
      "e76abefba7a144e3973ab74007867eee",
      "564074528f84422b8015f8ef202c5bde",
      "37dbcc92e5a54e4f80b3b170296d8bb8",
      "bfcb4345efe543aba62acbb07e6a3f7d",
      "ae51e898667e48a9ad2da8ab65dac2e1",
      "08a06ba0111f4c90820fa7da2aeade27",
      "2f4505cdde3c4301bc814c1cc4534b12",
      "7696d9f0266e4c10aba176e7e431681d",
      "1439a037803e4d9b90539a96226d6e7c",
      "2c691a4e2b114d70880e7c81974d9223",
      "cdd760a06b814bd187b40ea5ffea49f3",
      "8a72a0ca0171424bab6fdd9e4a223bb7",
      "573b6c54ff6c4ce6b9669b420c670c59",
      "633303b3635f48a58de1c4a4d0a8dd47",
      "38b93318e17d42c1bac8eacba74b81d2",
      "1127b8dfd3cf46ba8219ba7227d2cd81",
      "c5dfa6b08ba24299abac2f27bc68bdfd",
      "a15aa03eaf6745ca832eca411158e398",
      "b0ae4b8fcc544579be40f15248bc326e",
      "06ecaf2ff42241c6b232db150914078e",
      "544ce0bb6bdc405e998e0c2f19342a81",
      "b7a0d02ad2f3466c90e54ee2de63d91b",
      "20ebeeba9e42401c8db667bd6757c1d0",
      "97db510554f04266835dedb6da450e08",
      "6e85ce89f1a34beaa02dbd09087f516a",
      "7a7c753cdfb3430bada12cb00b810819",
      "34c0130976f440a691a4b3a8362fc5b2",
      "31ebf07a34694748ac87d3ca339259a7",
      "159f751bdeb741c3bbdf96f8bf241463",
      "71b202d1efef470a8a75892b30618c69"
     ]
    },
    "id": "7qKCWeBVY8Du",
    "outputId": "58b5deec-b334-46fc-fa22-6e527fc48d66"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6926235886e64a29a4bd945c083ed89f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "011889a215454ebdace22dc8f02f457f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "120547a0deb74a69bce43b08f7e453e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98b7173dcbec416299d39c5da6971ccf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aea1b1fb4cf64fefbdc388ec74b22287",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae4ab0cbd7dc45c9ad96163ab3d1dc91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a61fb28b1c154e22895ce8b67a29f119",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9af1242884d4a05a90261513d42b330",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fcdecd7f17d457ba197d7dade252823",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1439a037803e4d9b90539a96226d6e7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06ecaf2ff42241c6b232db150914078e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before LLM: messages=[HumanMessage(content='[INST]Please answer the question below only based on the context information provided.\\n\\nHere is a context:\\nlarge language models efficient. Through our work, our aim is to help the community create more\\n\\n[10] Y. Chen, R. Wang, H. Jiang, S. Shi, and R.-L. Xu. Exploring the use of large language models\\n\\nThis implies that large models are more generaliz-\\nable to compute texts in various domains and task\\n\\namong the largest language models today and we apply them on a wide range of language tasks, \\n\\nHere is a question: \\nWhat purpose do large language models serve in the field of natural language processing?.[/INST]')]\n",
      "Before LLM: messages=[HumanMessage(content='[INST]Please answer the question below only based on the context information provided.\\n\\nHere is a context:\\nlarge language models efficient. Through our work, our aim is to help the community create more\\n\\n[10] Y. Chen, R. Wang, H. Jiang, S. Shi, and R.-L. Xu. Exploring the use of large language models\\n\\nThis implies that large models are more generaliz-\\nable to compute texts in various domains and task\\n\\namong the largest language models today and we apply them on a wide range of language tasks, \\n\\nHere is a question: \\nWhat purpose do large language models serve in the field of natural language processing?.[/INST]')]\n",
      "0 -> "
     ]
    }
   ],
   "source": [
    "base_embeddings, text_splitter, qdrant_vectorstore, retriever = build_embedding_splitter_vectorstore('all-MiniLM-L6-v2', splitter)\n",
    "qdrant_vectorstore = vectorize_documents(text_splitter, qdrant_vectorstore)\n",
    "llm_model = load_llm(llm)\n",
    "rag_chain = build_RAG_prompt_chain(rag_template, llm_model, retriever, format_docs)\n",
    "metrics = ['rouge', 'bleu', 'bertscore']\n",
    "train_baseline = evaluate(metrics, validation_questions_answers, rag_chain, iterations=1, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p4ytJjaUZhMC"
   },
   "source": [
    "Below is the prompt with context, reformatted.\n",
    "\n",
    "<blockquote>\n",
    "\n",
    "'[INST]Please answer the question below only based on the context information provided.\n",
    "\n",
    "Here is a context:\n",
    "\n",
    "large language models efficient. Through our work, our aim is to help the community create more\n",
    "\n",
    "[10] Y. Chen, R. Wang, H. Jiang, S. Shi, and R.-L. Xu. Exploring the use of large language models\n",
    "\n",
    "This implies that large models are more generalizable to compute texts in various domains and task\n",
    "\n",
    "among the largest language models today and we apply them on a wide range of language tasks,\n",
    "\n",
    "Here is a question:\n",
    "\n",
    "What purpose do large language models serve in the field of natural language processing?.[/INST]\n",
    "\n",
    "</blockquote>\n",
    "\n",
    "While there is some change in the context that is provided it is hard to discern enough of a difference beyond random chance.  In order to stay true to our defined performance metric, we will be continuing with the 'Mini' embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w8xGbgVxFNRw"
   },
   "source": [
    "**Embedding Decision**: Based on these results, with all else remaining as baseline, the 'MINI' embedding model managed to produce the best results. We will be proceeding with the **'all-MiniLM-L6-v2'** model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hg__Qo7-g_Sr"
   },
   "source": [
    "#### 4.B) **Adjusting Chunking**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EggXwYvKy7O1"
   },
   "source": [
    "Now we are going to perform a grid search of various chunk sizes and overlaps to find what combination performs the best.  Based off our previous review of the context retreive, we suspect there is room for improvement here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZOyRGk2iFxg-",
    "outputId": "9e0ee391-f065-4e7c-bc9d-06d17280168d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Index  0\n",
      "Chunk Size: 32, Chunk Overlap: 0\n",
      "0 -> 1 -> 2 -> 3 -> 7 -> 8 -> 9 -> 11 -> 12 -> 13 -> Sample       6.600000\n",
      "eng_bleu     0.028118\n",
      "mk_bleu      0.012418\n",
      "eng_rouge    0.194782\n",
      "mk_rouge     0.109207\n",
      "eng_f1       0.855823\n",
      "mk_f1        0.840538\n",
      "dtype: float64\n",
      "\n",
      "Index  1\n",
      "Chunk Size: 32, Chunk Overlap: 4\n",
      "0 -> 1 -> 2 -> 3 -> 7 -> 8 -> 9 -> 11 -> 12 -> 13 -> Sample       6.600000\n",
      "eng_bleu     0.030376\n",
      "mk_bleu      0.007748\n",
      "eng_rouge    0.197281\n",
      "mk_rouge     0.095496\n",
      "eng_f1       0.851986\n",
      "mk_f1        0.835213\n",
      "dtype: float64\n",
      "\n",
      "Index  2\n",
      "Chunk Size: 32, Chunk Overlap: 8\n",
      "0 -> 1 -> 2 -> 3 -> 7 -> 8 -> 9 -> 11 -> 12 -> 13 -> Sample       6.600000\n",
      "eng_bleu     0.024835\n",
      "mk_bleu      0.021533\n",
      "eng_rouge    0.158797\n",
      "mk_rouge     0.118179\n",
      "eng_f1       0.850785\n",
      "mk_f1        0.845171\n",
      "dtype: float64\n",
      "\n",
      "Index  3\n",
      "Chunk Size: 32, Chunk Overlap: 16\n",
      "0 -> 1 -> 2 -> 3 -> 7 -> 8 -> 9 -> 11 -> 12 -> 13 -> Sample       6.600000\n",
      "eng_bleu     0.035043\n",
      "mk_bleu      0.011213\n",
      "eng_rouge    0.177818\n",
      "mk_rouge     0.099950\n",
      "eng_f1       0.853516\n",
      "mk_f1        0.836572\n",
      "dtype: float64\n",
      "\n",
      "Index  4\n",
      "Chunk Size: 64, Chunk Overlap: 0\n",
      "0 -> 1 -> 2 -> 3 -> 7 -> 8 -> 9 -> 11 -> 12 -> 13 -> Sample       6.600000\n",
      "eng_bleu     0.030427\n",
      "mk_bleu      0.022435\n",
      "eng_rouge    0.182085\n",
      "mk_rouge     0.109349\n",
      "eng_f1       0.852851\n",
      "mk_f1        0.842104\n",
      "dtype: float64\n",
      "\n",
      "Index  5\n",
      "Chunk Size: 64, Chunk Overlap: 4\n",
      "0 -> 1 -> 2 -> 3 -> 7 -> 8 -> 9 -> 11 -> 12 -> 13 -> Sample       6.600000\n",
      "eng_bleu     0.037605\n",
      "mk_bleu      0.015730\n",
      "eng_rouge    0.191591\n",
      "mk_rouge     0.096065\n",
      "eng_f1       0.851189\n",
      "mk_f1        0.835997\n",
      "dtype: float64\n",
      "\n",
      "Index  6\n",
      "Chunk Size: 64, Chunk Overlap: 8\n",
      "0 -> 1 -> 2 -> 3 -> 7 -> 8 -> 9 -> 11 -> 12 -> 13 -> Sample       6.600000\n",
      "eng_bleu     0.034170\n",
      "mk_bleu      0.014780\n",
      "eng_rouge    0.185217\n",
      "mk_rouge     0.097628\n",
      "eng_f1       0.852428\n",
      "mk_f1        0.837867\n",
      "dtype: float64\n",
      "\n",
      "Index  7\n",
      "Chunk Size: 64, Chunk Overlap: 16\n",
      "0 -> 1 -> 2 -> 3 -> 7 -> 8 -> 9 -> 11 -> 12 -> 13 -> Sample       6.600000\n",
      "eng_bleu     0.038589\n",
      "mk_bleu      0.018213\n",
      "eng_rouge    0.187788\n",
      "mk_rouge     0.112163\n",
      "eng_f1       0.855116\n",
      "mk_f1        0.839569\n",
      "dtype: float64\n",
      "\n",
      "Index  8\n",
      "Chunk Size: 128, Chunk Overlap: 0\n",
      "0 -> 1 -> 2 -> 3 -> 7 -> 8 -> 9 -> 11 -> 12 -> 13 -> Sample       6.600000\n",
      "eng_bleu     0.048007\n",
      "mk_bleu      0.028122\n",
      "eng_rouge    0.217813\n",
      "mk_rouge     0.134304\n",
      "eng_f1       0.863077\n",
      "mk_f1        0.849362\n",
      "dtype: float64\n",
      "\n",
      "Index  9\n",
      "Chunk Size: 128, Chunk Overlap: 4\n",
      "0 -> 1 -> 2 -> 3 -> 7 -> 8 -> 9 -> 11 -> 12 -> 13 -> Sample       6.600000\n",
      "eng_bleu     0.050216\n",
      "mk_bleu      0.032407\n",
      "eng_rouge    0.210195\n",
      "mk_rouge     0.141518\n",
      "eng_f1       0.860524\n",
      "mk_f1        0.847557\n",
      "dtype: float64\n",
      "\n",
      "Index  10\n",
      "Chunk Size: 128, Chunk Overlap: 8\n",
      "0 -> 1 -> 2 -> 3 -> 7 -> 8 -> 9 -> 11 -> 12 -> 13 -> Sample       6.600000\n",
      "eng_bleu     0.048331\n",
      "mk_bleu      0.019229\n",
      "eng_rouge    0.215263\n",
      "mk_rouge     0.118995\n",
      "eng_f1       0.859951\n",
      "mk_f1        0.843189\n",
      "dtype: float64\n",
      "\n",
      "Index  11\n",
      "Chunk Size: 128, Chunk Overlap: 16\n",
      "0 -> 1 -> 2 -> 3 -> 7 -> 8 -> 9 -> 11 -> 12 -> 13 -> Sample       6.600000\n",
      "eng_bleu     0.060322\n",
      "mk_bleu      0.023821\n",
      "eng_rouge    0.231263\n",
      "mk_rouge     0.123084\n",
      "eng_f1       0.861851\n",
      "mk_f1        0.843353\n",
      "dtype: float64\n",
      "\n",
      "Index  12\n",
      "Chunk Size: 256, Chunk Overlap: 0\n",
      "0 -> 1 -> 2 -> 3 -> 7 -> 8 -> 9 -> 11 -> 12 -> 13 -> Sample       6.600000\n",
      "eng_bleu     0.061769\n",
      "mk_bleu      0.040350\n",
      "eng_rouge    0.223019\n",
      "mk_rouge     0.148779\n",
      "eng_f1       0.863123\n",
      "mk_f1        0.850274\n",
      "dtype: float64\n",
      "\n",
      "Index  13\n",
      "Chunk Size: 256, Chunk Overlap: 4\n",
      "0 -> 1 -> 2 -> 3 -> 7 -> 8 -> 9 -> 11 -> 12 -> 13 -> Sample       6.600000\n",
      "eng_bleu     0.056784\n",
      "mk_bleu      0.039616\n",
      "eng_rouge    0.223038\n",
      "mk_rouge     0.149041\n",
      "eng_f1       0.861923\n",
      "mk_f1        0.850271\n",
      "dtype: float64\n",
      "\n",
      "Index  14\n",
      "Chunk Size: 256, Chunk Overlap: 8\n",
      "0 -> 1 -> 2 -> 3 -> 7 -> 8 -> 9 -> 11 -> 12 -> 13 -> Sample       6.600000\n",
      "eng_bleu     0.053987\n",
      "mk_bleu      0.034211\n",
      "eng_rouge    0.227184\n",
      "mk_rouge     0.143273\n",
      "eng_f1       0.867680\n",
      "mk_f1        0.849742\n",
      "dtype: float64\n",
      "\n",
      "Index  15\n",
      "Chunk Size: 256, Chunk Overlap: 16\n",
      "0 -> 1 -> 2 -> 3 -> 7 -> 8 -> 9 -> 11 -> 12 -> 13 -> Sample       6.600000\n",
      "eng_bleu     0.068378\n",
      "mk_bleu      0.036746\n",
      "eng_rouge    0.227989\n",
      "mk_rouge     0.144628\n",
      "eng_f1       0.867241\n",
      "mk_f1        0.852637\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# assigning model parameters\n",
    "embedding_model = 'all-MiniLM-L6-v2'\n",
    "llm = \"cohere\"\n",
    "rag_template = \"\"\"[INST]Please answer the question below only based on the context information provided.\\n\\nHere is a context:\\n{context} \\n\\nHere is a question: \\n{question}.[/INST]\"\"\"\n",
    "llm_model = load_llm(llm)\n",
    "\n",
    "# chunk sizes and overlaps to iterate through\n",
    "chunk_sizes = [32, 64, 128, 256]\n",
    "chunk_overlaps = [0, 4, 8, 16]\n",
    "\n",
    "results = []\n",
    "index = 0\n",
    "# for each chunk size & overlap combo, completing the RAG system and evaluating.\n",
    "for chunk_size in chunk_sizes:\n",
    "    for chunk_overlap in chunk_overlaps:\n",
    "        print(\"\")\n",
    "        print(\"Index \", index)\n",
    "        index += 1\n",
    "        print(f\"Chunk Size: {chunk_size}, Chunk Overlap: {chunk_overlap}\")\n",
    "\n",
    "        # building model for current chunking\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "        base_embeddings, text_splitter, qdrant_vectorstore, retriever = build_embedding_splitter_vectorstore(embedding_model, splitter)\n",
    "        qdrant_vectorstore = vectorize_documents(text_splitter, qdrant_vectorstore)\n",
    "        rag_chain = build_RAG_prompt_chain(rag_template, llm_model, retriever, format_docs)\n",
    "\n",
    "        # computing metrics\n",
    "        metrics = ['rouge', 'bleu', 'bertscore']\n",
    "        chunk_result = evaluate(metrics, validation_questions_answers, rag_chain, iterations=10, verbose=True, sleep=False)\n",
    "        print(chunk_result.mean())\n",
    "        results.append(chunk_result)\n",
    "\n",
    "        # deleting previous variables\n",
    "        del qdrant_vectorstore\n",
    "        del text_splitter\n",
    "        gc.collect() # forcing garbage collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hVY1ujhDUrjl",
    "outputId": "3a244a18-2ad4-41bb-e887-17a8c3d3cbe4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: , 0\n",
      "0.47738752984398225\n",
      "Index: , 1\n",
      "0.47387331441219416\n",
      "Index: , 2\n",
      "0.471737410212241\n",
      "Index: , 3\n",
      "0.47247251703615556\n",
      "Index: , 4\n",
      "0.475619310549057\n",
      "Index: , 5\n",
      "0.47434139872304604\n",
      "Index: , 6\n",
      "0.47363885699726627\n",
      "Index: , 7\n",
      "0.4777977152501647\n",
      "Index: , 8\n",
      "0.4921290047686167\n",
      "Index: , 9\n",
      "0.49110410919102854\n",
      "Index: , 10\n",
      "0.4869879168879005\n",
      "Index: , 11\n",
      "0.4927676679165994\n",
      "Index: , 12\n",
      "0.49762912126782427\n",
      "Index: , 13\n",
      "0.49664616910159837\n",
      "Index: , 14\n",
      "0.4975537489503412\n",
      "Index: , 15\n",
      "0.5002383253742265\n"
     ]
    }
   ],
   "source": [
    "# calculating the composite score for each iteration\n",
    "for i, result in enumerate(results):\n",
    "  print(\"Index: ,\", i)\n",
    "  result = composite_evaluation(result)\n",
    "  print(result['composite_total'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATl--kMcLeNj"
   },
   "source": [
    "Based on the above results, it appears that the max chunk size (256) and chunk overlap (16) performed the best.  Because of this we are going to expand the max values to see if it continues to improve the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FjtOhZxgXTRf",
    "outputId": "46d55c59-b749-4459-8288-bec5f7ac1d66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Index  0\n",
      "Chunk Size: 512, Chunk Overlap: 8\n",
      "0 -> 1 -> 2 -> 3 -> 7 -> 8 -> 9 -> 11 -> 12 -> 13 -> Sample       6.600000\n",
      "eng_bleu     0.055507\n",
      "mk_bleu      0.028529\n",
      "eng_rouge    0.220229\n",
      "mk_rouge     0.145474\n",
      "eng_f1       0.867614\n",
      "mk_f1        0.854710\n",
      "dtype: float64\n",
      "\n",
      "Index  1\n",
      "Chunk Size: 512, Chunk Overlap: 16\n",
      "0 -> 1 -> 2 -> 3 -> 7 -> 8 -> 9 -> 11 -> 12 -> 13 -> Sample       6.600000\n",
      "eng_bleu     0.043765\n",
      "mk_bleu      0.025808\n",
      "eng_rouge    0.224259\n",
      "mk_rouge     0.126912\n",
      "eng_f1       0.863763\n",
      "mk_f1        0.850277\n",
      "dtype: float64\n",
      "\n",
      "Index  2\n",
      "Chunk Size: 512, Chunk Overlap: 32\n",
      "0 -> 1 -> 2 -> 3 -> 7 -> 8 -> 9 -> 11 -> 12 -> 13 -> Sample       6.600000\n",
      "eng_bleu     0.053143\n",
      "mk_bleu      0.031342\n",
      "eng_rouge    0.210684\n",
      "mk_rouge     0.141335\n",
      "eng_f1       0.862925\n",
      "mk_f1        0.848733\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "embedding_model = 'all-MiniLM-L6-v2'\n",
    "llm = \"cohere\"\n",
    "rag_template = \"\"\"[INST]Please answer the question below only based on the context information provided.\\n\\nHere is a context:\\n{context} \\n\\nHere is a question: \\n{question}.[/INST]\"\"\"\n",
    "llm_model = load_llm(llm)\n",
    "chunk_sizes = [512]\n",
    "chunk_overlaps = [8, 16, 32]\n",
    "\n",
    "results = []\n",
    "index = 0\n",
    "for chunk_size in chunk_sizes:\n",
    "    for chunk_overlap in chunk_overlaps:\n",
    "        print(\"\")\n",
    "        print(\"Index \", index)\n",
    "        index += 1\n",
    "        print(f\"Chunk Size: {chunk_size}, Chunk Overlap: {chunk_overlap}\")\n",
    "\n",
    "        # building model for current chunking\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "        base_embeddings, text_splitter, qdrant_vectorstore, retriever = build_embedding_splitter_vectorstore(embedding_model, splitter)\n",
    "        qdrant_vectorstore = vectorize_documents(text_splitter, qdrant_vectorstore)\n",
    "        rag_chain = build_RAG_prompt_chain(rag_template, llm_model, retriever, format_docs)\n",
    "\n",
    "        # computing metrics\n",
    "        metrics = ['rouge', 'bleu', 'bertscore']\n",
    "        chunk_result = evaluate(metrics, validation_questions_answers, rag_chain, iterations=10, verbose=True, sleep=False)\n",
    "        print(chunk_result.mean())\n",
    "        results.append(chunk_result)\n",
    "\n",
    "        # deleting previous variables\n",
    "        del qdrant_vectorstore\n",
    "        del text_splitter\n",
    "        gc.collect() # forcing garbage collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X4hsk03LX0TO",
    "outputId": "82d4dda8-9562-43fb-c7a5-ee1523abfef8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: , 0\n",
      "0.49726734330825567\n",
      "Index: , 1\n",
      "0.4920968688966886\n",
      "Index: , 2\n",
      "0.49239176281594227\n"
     ]
    }
   ],
   "source": [
    "for i, result in enumerate(results):\n",
    "  print(\"Index: ,\", i)\n",
    "  result = composite_evaluation(result)\n",
    "  print(result['composite_total'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmycmG6AbDX2"
   },
   "source": [
    "##### **Context Check:**\n",
    "\n",
    "\n",
    "Now we review difference this has made in our context retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ZbXOgfjbJl7",
    "outputId": "d220460f-0f72-487c-deb9-c78165741904"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before LLM: messages=[HumanMessage(content='[INST]Please answer the question below only based on the context information provided.\\n\\nHere is a context:\\nalgorithm. Machine Learning, 97(3):327–351, July 2014. doi: 10.1007/s10994-014-5458-8.\\nURL https://doi.org/10.1007/s10994-014-5458-8.\\n[10] Y. Chen, R. Wang, H. Jiang, S. Shi, and R.-L. Xu. Exploring the use of large language models\\n\\narli, and Denny Zhou. Large language models can be easily distracted by irrelevant context.\\nIn Proceedings of the 40th International Conference on Machine Learning, 2023. URL https:\\n//proceedings.mlr.press/v202/shi23a.html.\\n\\namong the largest language models today and we apply them on a wide range of language tasks,\\nincluding classiﬁcation, summarization, question-answering, creative writing, dialogue, and others.\\n\\n[7] S. Biderman, H. Schoelkopf, Q. Anthony, H. Bradley, K. O’Brien, E. Hallahan, M. A. Khan,\\nS. Purohit, U. S. Prashanth, E. Raff, et al. Pythia: A suite for analyzing large language models \\n\\nHere is a question: \\nWhat purpose do large language models serve in the field of natural language processing?.[/INST]')]\n",
      "Before LLM: messages=[HumanMessage(content='[INST]Please answer the question below only based on the context information provided.\\n\\nHere is a context:\\nalgorithm. Machine Learning, 97(3):327–351, July 2014. doi: 10.1007/s10994-014-5458-8.\\nURL https://doi.org/10.1007/s10994-014-5458-8.\\n[10] Y. Chen, R. Wang, H. Jiang, S. Shi, and R.-L. Xu. Exploring the use of large language models\\n\\narli, and Denny Zhou. Large language models can be easily distracted by irrelevant context.\\nIn Proceedings of the 40th International Conference on Machine Learning, 2023. URL https:\\n//proceedings.mlr.press/v202/shi23a.html.\\n\\namong the largest language models today and we apply them on a wide range of language tasks,\\nincluding classiﬁcation, summarization, question-answering, creative writing, dialogue, and others.\\n\\n[7] S. Biderman, H. Schoelkopf, Q. Anthony, H. Bradley, K. O’Brien, E. Hallahan, M. A. Khan,\\nS. Purohit, U. S. Prashanth, E. Raff, et al. Pythia: A suite for analyzing large language models \\n\\nHere is a question: \\nWhat purpose do large language models serve in the field of natural language processing?.[/INST]')]\n",
      "0 -> "
     ]
    }
   ],
   "source": [
    "chunk_size = 256\n",
    "chunk_overlaps = 16\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "base_embeddings, text_splitter, qdrant_vectorstore, retriever = build_embedding_splitter_vectorstore('all-MiniLM-L6-v2', splitter)\n",
    "qdrant_vectorstore = vectorize_documents(text_splitter, qdrant_vectorstore)\n",
    "llm_model = load_llm(llm)\n",
    "rag_chain = build_RAG_prompt_chain(rag_template, llm_model, retriever, format_docs)\n",
    "metrics = ['rouge', 'bleu', 'bertscore']\n",
    "train_baseline = evaluate(metrics, validation_questions_answers, rag_chain, iterations=1, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GakPHMcqb6Cz"
   },
   "source": [
    "Below is the reformatted prompt with embedded context.\n",
    "\n",
    "<blockquote>\n",
    "\n",
    "'[INST]Please answer the question below only based on the context information provided.\n",
    "\n",
    "Here is a context:\n",
    "\n",
    "algorithm. Machine Learning, 97(3):327–351, July 2014. doi: 10.1007/s10994-014-5458-8. URL https://doi.org/10.1007/s10994-014-5458-8.\\n[10] Y. Chen, R. Wang, H. Jiang, S. Shi, and R.-L. Xu. Exploring the use of large language models\n",
    "\n",
    "arli, and Denny Zhou. Large language models can be easily distracted by irrelevant context. In Proceedings of the 40th International Conference on Machine Learning, 2023. URL https://proceedings.mlr.press/v202/shi23a.html.\n",
    "\n",
    "among the largest language models today and we apply them on a wide range of language tasks, including classiﬁcation, summarization, question-answering, creative writing, dialogue, and others.\n",
    "\n",
    "[7] S. Biderman, H. Schoelkopf, Q. Anthony, H. Bradley, K. O’Brien, E. Hallahan, M. A. Khan, S. Purohit, U. S. Prashanth, E. Raff, et al. Pythia: A suite for analyzing large language models\n",
    "\n",
    "Here is a question:\n",
    "\n",
    "What purpose do large language models serve in the field of natural language processing?.[/INST]'\n",
    "\n",
    "</blockquote>\n",
    "\n",
    "It can be seen from this that with the new chunking we were able to capture the additional, helpful context of the types of language tasks that LLMs are used for.  This is a good.  However, it also looks like the chunking now also incorporates additional noise in the context like, what looks like are reference snippets that happen to align with the question.  Overall, since this is showing some additional value and is aligned with our performance metric, we will proceed with the best scoring scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EmlNFFpnEaig"
   },
   "source": [
    "**Chunking Decision:** From our grid search, we found that, for our current model, on this train set, the chunking of **chunk size = 256** and **chunk overlap = 16** returned the best results, so we will proceed with those values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u7JPLwRIK7tu"
   },
   "source": [
    "#### 4.C) **Adjusting the Retriever**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "obGoZ9Jxt32x"
   },
   "source": [
    "After our review of chunking we are noticing an improvement in some of the content coming in, but also some additional noise.  Next we want to explore some of the retriever parameters to see if there is an opportunity to improve the chunks that we retrieve.  We will be exploring the search type, and number of chunks to return.\n",
    "\n",
    "*Note: In a prior run of this grid search we also explored various 'score_thresholds' to accompany the 'similarity_score_threshold' search. Due to the expotential increase in compute that this creates, and the fact that it did not perform as well, it was dropped and not rerun. So the below output is just utilizing the default '0.5' value.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "referenced_widgets": [
      "a69d1966927b4d0db0040b3f1ff21d24",
      "0e74588e049842a2a207d8aaad3b4c9b",
      "ab16f60256ae43f384d319af11e48d63",
      "9026567365b94e94b5f799cba90265ae",
      "20dfb400adc040bf85aa7b9d179ef46e",
      "546d274303574956a1fa8f7b409e4347",
      "7a5604969c9a481ab6e41079660042b0",
      "a060449aff4d48cfb4c87293ac2813b0",
      "3c67e7effde948249b7f50f945de96fe",
      "7894fd052aef4c0892a44854d60a3634",
      "1f5b84c9567e4d67b8dec40284c4a326",
      "a48ecf53832b404282c0ec146a665976",
      "1e225b5bdee34454ab860722c41cc286",
      "20a10315b3134e868029987222642232",
      "7e505515d41d459eb8d2341c0bf9ef34",
      "736d57d7e889422194dab51dab7ab065",
      "d86c4fc49c0744f6bc4c45778b561733",
      "fe6118f3635a4a1dac2ced42046a7318",
      "cc39cd9f128f44c09efe00b47feccf80",
      "c4bfae8972374bb581d5d5abcc8a832d",
      "6c894160f44b4db09bec1e82dce4f325",
      "34a59abe9953424aa9d8dd12e87a8a64",
      "12ed138c2b6047788198c5983801a320",
      "dcf0d88429104c798d3048da28035521",
      "5fa0665b0f294c0dad16ab252c132809",
      "3dc8124776044277a18cf2fcd5782403",
      "e52558d760244c5ebb731b4af7878e45",
      "4d2f67dc24794cd3ad8101a98c794c4b",
      "1d0c669af7e8433fa170fe61a91ae6bc",
      "6529e68e94174966ad9ceaf3024b2825",
      "b22856d9b37d43b1950a48f256afb037",
      "1e7bc085b40e40a8aa334d8cbb8c9304",
      "4b64a670d8b54e7dabbf366be04be2a1",
      "3408be09aac6440dbbfea71f05802850",
      "8b42f1e530764f16bd1d969e44c63f23",
      "04bf8b42a38c45268a65f8cb8cf223b0",
      "45eb11d8aec44039bde98695099c3a4f",
      "e97ab7e898984a4f85102646a59b5c11",
      "00400719a0904489aed145e287eedbee",
      "cc7916202f89433196c32424d9c7ced7",
      "7a8139029aaf4eb49f249ac7c9d182cd",
      "31346dbadbcd4eacae0c39017c72a2c5",
      "851e5131828742eca012c8bcd6a8e256",
      "a5156a1f7ffb45b698488a70748779f2",
      "d05ad18758a04f588084a8d0670abc06",
      "e2169ded590346b0856460ada5ad2d41",
      "705eeadc71be452b9779623d08f0c0c6",
      "516a5a39e2ca4799ac1866d67f3ab8de",
      "4bd9def6f5ea4982984dfa177d864517",
      "43a3872361cc497f8163cfb661265f73",
      "4dbc82dae5c948aab3ec38d28b67d53e",
      "548f91976638484eac3db53e1390107e",
      "6ca465c5bcd746efb3d8d6db4defaa0d",
      "5cdf8ce4cede446db92df6a68e68ce52",
      "5cb94b304b8d4e7cba498cda5b80417e",
      "a78dbd71ec4f4146acb6561bdadcb9b1",
      "9508ac23781042dbba6ece95f3eb3f3a",
      "2ba5b9e374e348aeb8c3fbf36ba3bb88",
      "18115621275a49148f81f9bb73f059d6",
      "eab95a3f9b5e49e0a05e36211cca9693",
      "7a0e619549a749c8b56fadc7c2b6bb77",
      "4559eaab8b4e454f8611709bac60949d",
      "49eac13e310440378d596881537beb66",
      "20c1cb91da134ec7bf654f0528f08ae9",
      "e2b409dff53441a6909f27c964f8b4bf",
      "d3dcb0c70bb24821b1540a648d86c1da",
      "7b37d6a232644f25b2ff23cc5dd7abc8",
      "2236d0f93d464e50bed2231544fa7ba5",
      "7956b5f990954a7fa1d71aa0c42e8133",
      "b02339f3f5aa40a8858770995468e2a7",
      "f1ba919e78e84da4b69a4d510126ff6b",
      "2b63721e767b4787ae474a7fd7515ebb",
      "8f2b0ccefffe48868e1f15fd232de716",
      "d479615aabfb4aa1b0594625f0d1bf32",
      "a25a24976e8e4b7c857c29de4045e886",
      "f4493d99c0174cd2a0320dca66ccc921",
      "55b8ea09548641d49a1d50109dab779f",
      "43e8f6b20b1a422595b4cbf1103ee82c",
      "6e59233f5b7c4d1682635be518b77d3b",
      "74efa8e44d9e4c86a1f7758c3f62307d",
      "2503801bdc884ab1b924299ea6b32219",
      "c43e50e679ef44c59477e05523b6c1e0",
      "7a0306794ad046b3a1204c4b9bd23818",
      "5904f6edf45e463981cdb2e64cd5eed0",
      "1e15f4e6cbda4f8298540c7a4991de85",
      "9983239b3a3c47da8599b0854201012b",
      "ff77c48eacd74e7cbde24e77faf4112c",
      "56b5cae7eb0d46cabfb122d5a599ce36",
      "1562e799dc8d46c8a49b9c00462d826e",
      "ea3136dd585b4cd9888e191b7c1bd895",
      "7dbcb5ce2143486495089ed89aa6900e",
      "774e8740090f4438be20e43a9c9cf6e3",
      "d298a4f7377344928a65ee4f9b556d39",
      "48ee33c2012c42e8aaea3a558e808d8a",
      "7eb36154990b4732b9897391cc156f96",
      "fb95312965944faf8e57f7a0b700145d",
      "cc403af917244162b84481df711795f6",
      "9dfd5c5b013340b2bc2ecaf0907a3e36",
      "3c20237a88a443bf9ddb9d4436e0c513",
      "db88d4b9f3084775a2a3f54561790630",
      "edde1773123c4a6b8b4bb6d52866a143",
      "48322e9a5bc540b4b66bd47cb608e848",
      "d14529263b2b4d718fc2158b62942a2e",
      "712d125baed4441b93e607312ec83cc8",
      "dc0a0a17eb9b40c182476575d85ff788",
      "48fd6a5a737d4832bc5349cab80abff6",
      "51770412df7848478e714e41afa1ece7",
      "8400e9421545444d9c9aba39b976f0ab",
      "e1dfd20b730b4e648c931ca349124356",
      "af8f7de1b7154b17885f78b5b5574b7a",
      "aa71965ed74b4f609c95ba1a3e4b9163",
      "bf656aa894cf4e6f9ef12c69dbc9c881",
      "79b06fa4ee174dd283897d9bfce1d461",
      "ed01f31687df4240bb4c5ab0f122d827",
      "a5d12e37ee2e42bcabb296aa7642b15d",
      "03ac455559734c06ab934bdba9229262",
      "760dfceff5894ab18c653725dccaf2e0",
      "738d7abc107d4606a0cd84c67d63ab6e",
      "d2dfa246e4b64d13920220457de1a8d9",
      "93d0aeb39303473391d4b3ececefaa99",
      "90f39fbec5774c24bca6094d96ab26ff"
     ]
    },
    "id": "tiaPE8tELCmz",
    "outputId": "b70a1ac5-0d25-4e3d-ed96-0e41229ecbb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Index  0\n",
      "Retriever Search Type: similarity_score_threshold, Retriever K: 2, Retriever Score Threshold: 0.5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a69d1966927b4d0db0040b3f1ff21d24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a48ecf53832b404282c0ec146a665976",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12ed138c2b6047788198c5983801a320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3408be09aac6440dbbfea71f05802850",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d05ad18758a04f588084a8d0670abc06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a78dbd71ec4f4146acb6561bdadcb9b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b37d6a232644f25b2ff23cc5dd7abc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43e8f6b20b1a422595b4cbf1103ee82c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1562e799dc8d46c8a49b9c00462d826e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db88d4b9f3084775a2a3f54561790630",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa71965ed74b4f609c95ba1a3e4b9163",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -> 1 -> 2 -> 3 -> 7 -> 8 -> 9 -> 11 -> 12 -> 13 -> Sample       6.600000\n",
      "eng_bleu     0.044431\n",
      "mk_bleu      0.022195\n",
      "eng_rouge    0.216947\n",
      "mk_rouge     0.130084\n",
      "eng_f1       0.862653\n",
      "mk_f1        0.844525\n",
      "dtype: float64\n",
      "\n",
      "Index  1\n",
      "Retriever Search Type: similarity_score_threshold, Retriever K: 4, Retriever Score Threshold: 0.5\n",
      "0 -> 1 -> 2 -> 3 -> 7 -> 8 -> 9 -> 11 -> 12 -> 13 -> Sample       6.600000\n",
      "eng_bleu     0.063517\n",
      "mk_bleu      0.030256\n",
      "eng_rouge    0.216755\n",
      "mk_rouge     0.130160\n",
      "eng_f1       0.866460\n",
      "mk_f1        0.850197\n",
      "dtype: float64\n",
      "\n",
      "Index  2\n",
      "Retriever Search Type: similarity_score_threshold, Retriever K: 6, Retriever Score Threshold: 0.5\n",
      "0 -> 1 -> 2 -> 3 -> 7 -> 8 -> 9 -> 11 -> 12 -> 13 -> Sample       6.600000\n",
      "eng_bleu     0.066372\n",
      "mk_bleu      0.028718\n",
      "eng_rouge    0.222179\n",
      "mk_rouge     0.141708\n",
      "eng_f1       0.869331\n",
      "mk_f1        0.850582\n",
      "dtype: float64\n",
      "\n",
      "Index  3\n",
      "Retriever Search Type: similarity_score_threshold, Retriever K: 8, Retriever Score Threshold: 0.5\n",
      "0 -> 1 -> 2 -> 3 -> 7 -> 8 -> 9 -> 11 -> 12 -> 13 -> Sample       6.600000\n",
      "eng_bleu     0.059567\n",
      "mk_bleu      0.024431\n",
      "eng_rouge    0.224639\n",
      "mk_rouge     0.145072\n",
      "eng_f1       0.873610\n",
      "mk_f1        0.853858\n",
      "dtype: float64\n",
      "\n",
      "Index  4\n",
      "Retriever Search Type: similarity_score_threshold, Retriever K: 10, Retriever Score Threshold: 0.5\n",
      "0 -> 1 -> 2 -> 3 -> 7 -> 8 -> 9 -> 11 -> 12 -> 13 -> Sample       6.600000\n",
      "eng_bleu     0.069532\n",
      "mk_bleu      0.030232\n",
      "eng_rouge    0.222779\n",
      "mk_rouge     0.158514\n",
      "eng_f1       0.871832\n",
      "mk_f1        0.859604\n",
      "dtype: float64\n",
      "\n",
      "Index  5\n",
      "Retriever Search Type: similarity, Retriever K: 2, Retriever Score Threshold: None\n",
      "0 -> 1 -> 2 -> 3 -> 7 -> 8 -> 9 -> 11 -> 12 -> 13 -> Sample       6.600000\n",
      "eng_bleu     0.040335\n",
      "mk_bleu      0.019809\n",
      "eng_rouge    0.192613\n",
      "mk_rouge     0.128243\n",
      "eng_f1       0.858117\n",
      "mk_f1        0.846847\n",
      "dtype: float64\n",
      "\n",
      "Index  6\n",
      "Retriever Search Type: similarity, Retriever K: 4, Retriever Score Threshold: None\n",
      "0 -> 1 -> 2 -> 3 -> 7 -> 8 -> 9 -> 11 -> 12 -> 13 -> Sample       6.600000\n",
      "eng_bleu     0.065150\n",
      "mk_bleu      0.039904\n",
      "eng_rouge    0.232077\n",
      "mk_rouge     0.158515\n",
      "eng_f1       0.869681\n",
      "mk_f1        0.855236\n",
      "dtype: float64\n",
      "\n",
      "Index  7\n",
      "Retriever Search Type: similarity, Retriever K: 6, Retriever Score Threshold: None\n",
      "0 -> 1 -> 2 -> 3 -> 7 -> 8 -> 9 -> 11 -> 12 -> 13 -> Sample       6.600000\n",
      "eng_bleu     0.059179\n",
      "mk_bleu      0.033712\n",
      "eng_rouge    0.215156\n",
      "mk_rouge     0.150328\n",
      "eng_f1       0.866048\n",
      "mk_f1        0.852469\n",
      "dtype: float64\n",
      "\n",
      "Index  8\n",
      "Retriever Search Type: similarity, Retriever K: 8, Retriever Score Threshold: None\n",
      "0 -> 1 -> 2 -> 3 -> 7 -> 8 -> 9 -> 11 -> 12 -> 13 -> Sample       6.600000\n",
      "eng_bleu     0.056402\n",
      "mk_bleu      0.031415\n",
      "eng_rouge    0.235242\n",
      "mk_rouge     0.153934\n",
      "eng_f1       0.870802\n",
      "mk_f1        0.857928\n",
      "dtype: float64\n",
      "\n",
      "Index  9\n",
      "Retriever Search Type: similarity, Retriever K: 10, Retriever Score Threshold: None\n",
      "0 -> 1 -> 2 -> 3 -> 7 -> 8 -> 9 -> 11 -> 12 -> 13 -> Sample       6.600000\n",
      "eng_bleu     0.073652\n",
      "mk_bleu      0.030941\n",
      "eng_rouge    0.225843\n",
      "mk_rouge     0.149290\n",
      "eng_f1       0.869990\n",
      "mk_f1        0.856595\n",
      "dtype: float64\n",
      "\n",
      "Index  10\n",
      "Retriever Search Type: mmr, Retriever K: 2, Retriever Score Threshold: None\n",
      "0 -> 1 -> 2 -> 3 -> 7 -> 8 -> 9 -> 11 -> 12 -> 13 -> Sample       6.600000\n",
      "eng_bleu     0.050470\n",
      "mk_bleu      0.032744\n",
      "eng_rouge    0.230289\n",
      "mk_rouge     0.142910\n",
      "eng_f1       0.864595\n",
      "mk_f1        0.852660\n",
      "dtype: float64\n",
      "\n",
      "Index  11\n",
      "Retriever Search Type: mmr, Retriever K: 4, Retriever Score Threshold: None\n",
      "0 -> 1 -> 2 -> 3 -> 7 -> 8 -> 9 -> 11 -> 12 -> 13 -> Sample       6.600000\n",
      "eng_bleu     0.071303\n",
      "mk_bleu      0.029910\n",
      "eng_rouge    0.245975\n",
      "mk_rouge     0.146251\n",
      "eng_f1       0.871683\n",
      "mk_f1        0.852687\n",
      "dtype: float64\n",
      "\n",
      "Index  12\n",
      "Retriever Search Type: mmr, Retriever K: 6, Retriever Score Threshold: None\n",
      "0 -> 1 -> 2 -> 3 -> 7 -> 8 -> 9 -> 11 -> 12 -> 13 -> Sample       6.600000\n",
      "eng_bleu     0.058607\n",
      "mk_bleu      0.025322\n",
      "eng_rouge    0.243618\n",
      "mk_rouge     0.153144\n",
      "eng_f1       0.874576\n",
      "mk_f1        0.857767\n",
      "dtype: float64\n",
      "\n",
      "Index  13\n",
      "Retriever Search Type: mmr, Retriever K: 8, Retriever Score Threshold: None\n",
      "0 -> 1 -> 2 -> 3 -> 7 -> 8 -> 9 -> 11 -> 12 -> 13 -> Sample       6.600000\n",
      "eng_bleu     0.060207\n",
      "mk_bleu      0.036633\n",
      "eng_rouge    0.231780\n",
      "mk_rouge     0.145021\n",
      "eng_f1       0.867323\n",
      "mk_f1        0.853808\n",
      "dtype: float64\n",
      "\n",
      "Index  14\n",
      "Retriever Search Type: mmr, Retriever K: 10, Retriever Score Threshold: None\n",
      "0 -> 1 -> 2 -> 3 -> 7 -> 8 -> 9 -> 11 -> 12 -> 13 -> Sample       6.600000\n",
      "eng_bleu     0.063691\n",
      "mk_bleu      0.022107\n",
      "eng_rouge    0.244408\n",
      "mk_rouge     0.154502\n",
      "eng_f1       0.868340\n",
      "mk_f1        0.852620\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "embedding_model = 'all-MiniLM-L6-v2'\n",
    "llm = \"cohere\"\n",
    "rag_template = \"\"\"[INST]Please answer the question below only based on the context information provided.\\n\\nHere is a context:\\n{context} \\n\\nHere is a question: \\n{question}.[/INST]\"\"\"\n",
    "llm_model = load_llm(llm)\n",
    "chunk_size = 256\n",
    "chunk_overlap = 16\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "retr_search_types = [\"similarity_score_threshold\", \"similarity\", \"mmr\"]\n",
    "retr_ks = [2, 4, 6, 8, 10]\n",
    "retr_score_thresholds = [0.5]\n",
    "results = []\n",
    "index = 0\n",
    "for search_type in retr_search_types:\n",
    "    for k in retr_ks:\n",
    "        if search_type != \"similarity_score_threshold\":\n",
    "            thresholds = [None]\n",
    "        else:\n",
    "            thresholds = retr_score_thresholds.copy()\n",
    "        for threshold in thresholds:\n",
    "          threshold = float(threshold) if threshold is not None else None\n",
    "          print(\"\")\n",
    "          print(\"Index \", index)\n",
    "          index += 1\n",
    "          print(f\"Retriever Search Type: {search_type}, Retriever K: {k}, Retriever Score Threshold: {threshold}\")\n",
    "\n",
    "          # building model for current retriever\n",
    "          base_embeddings, text_splitter, qdrant_vectorstore, retriever = build_embedding_splitter_vectorstore(embedding_model, splitter, retr_search_type=search_type, retr_k=k)\n",
    "          qdrant_vectorstore = vectorize_documents(text_splitter, qdrant_vectorstore)\n",
    "          rag_chain = build_RAG_prompt_chain(rag_template, llm_model, retriever, format_docs)\n",
    "\n",
    "          # computing metrics\n",
    "          metrics = ['rouge', 'bleu', 'bertscore']\n",
    "          retriever_result = evaluate(metrics, validation_questions_answers, rag_chain, iterations=10, verbose=True, sleep=False)\n",
    "          print(retriever_result.mean())\n",
    "          results.append(retriever_result)\n",
    "\n",
    "          # deleting previous variables\n",
    "          del qdrant_vectorstore\n",
    "          del text_splitter\n",
    "          del retriever\n",
    "          del rag_chain\n",
    "          gc.collect() # forcing garbage collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IPDdrdNrtubA",
    "outputId": "aac694a3-fa4a-45d9-949f-782038c78147"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: , 0\n",
      "0.48946872115337003\n",
      "Index: , 1\n",
      "0.49465515025590856\n",
      "Index: , 2\n",
      "0.49817502517732154\n",
      "Index: , 3\n",
      "0.49980076225463976\n",
      "Index: , 4\n",
      "0.5033544772597616\n",
      "Index: , 5\n",
      "0.4832889073471821\n",
      "Index: , 6\n",
      "0.5037578324854686\n",
      "Index: , 7\n",
      "0.49687409315131637\n",
      "Index: , 8\n",
      "0.5029232284077463\n",
      "Index: , 9\n",
      "0.5021960673241921\n",
      "Index: , 10\n",
      "0.49718776473797616\n",
      "Index: , 11\n",
      "0.5048170650110574\n",
      "Index: , 12\n",
      "0.5052131639923401\n",
      "Index: , 13\n",
      "0.5002370761126699\n",
      "Index: , 14\n",
      "0.5029710566734872\n"
     ]
    }
   ],
   "source": [
    "for i, result in enumerate(results):\n",
    "  print(\"Index: ,\", i)\n",
    "  result = composite_evaluation(result)\n",
    "  print(result['composite_total'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E6qHZvggfPpA"
   },
   "source": [
    "##### **Context Check**\n",
    "\n",
    " Below we are checking how our best performing retriever metric context looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 111
    },
    "id": "4medBAegfdEt",
    "outputId": "1fc9d5a0-e17d-411d-bdb8-13f897b96691"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before LLM: messages=[HumanMessage(content='[INST]Please answer the question below only based on the context information provided.\\n\\nHere is a context:\\nalgorithm. Machine Learning, 97(3):327–351, July 2014. doi: 10.1007/s10994-014-5458-8.\\nURL https://doi.org/10.1007/s10994-014-5458-8.\\n[10] Y. Chen, R. Wang, H. Jiang, S. Shi, and R.-L. Xu. Exploring the use of large language models\\n\\nmisuse, there are many domains where large language models should be deployed only with great\\ncare, or not at all. Examples include high-stakes domains such as medical diagnoses, classifying\\n\\nAfter the success of many large-scale general language models, many QA models embrace the following approach:\\n\\nthem to do what a given set of humans want them to do. By default, language models optimize\\nthe next word prediction objective, which is only a proxy for what we want these models to do.\\n\\nMany applications in natural language processing rely on adapt-\\ning one large-scale, pre-trained language model to multiple down-\\nstream applications. Such adaptation is usually done via ﬁne-tuning,\\n\\namong the largest language models today and we apply them on a wide range of language tasks,\\nincluding classiﬁcation, summarization, question-answering, creative writing, dialogue, and others. \\n\\nHere is a question: \\nWhat purpose do large language models serve in the field of natural language processing?.[/INST]')]\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Large language models serve as the foundation for many natural language processing (NLP) applications. They are pre-trained on vast amounts of text data and can be adapted to various downstream tasks through fine-tuning. This allows for efficient and effective language model deployment in multiple domains, including classification, summarization, question-answering, creative writing, and dialogue.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_size = 256\n",
    "chunk_overlaps = 16\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "base_embeddings, text_splitter, qdrant_vectorstore, retriever = build_embedding_splitter_vectorstore('all-MiniLM-L6-v2', splitter, retr_search_type=\"mmr\", retr_k=6)\n",
    "qdrant_vectorstore = vectorize_documents(text_splitter, qdrant_vectorstore)\n",
    "llm_model = load_llm(llm)\n",
    "rag_chain = build_RAG_prompt_chain(rag_template, llm_model, retriever, format_docs)\n",
    "rag_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u2870jDnfviM"
   },
   "source": [
    "Below is the reformatted prompt with the context embedded\n",
    "\n",
    "<blockquote>\n",
    "\n",
    "'[INST]Please answer the question below only based on the context information provided.\n",
    "\n",
    "Here is a context:\\nalgorithm. Machine Learning, 97(3):327–351, July 2014. doi: 10.1007/s10994-014-5458-8. URL https://doi.org/10.1007/s10994-014-5458-8. [10] Y. Chen, R. Wang, H. Jiang, S. Shi, and R.-L. Xu. Exploring the use of large language models\n",
    "\n",
    "misuse, there are many domains where large language models should be deployed only with great care, or not at all. Examples include high-stakes domains such as medical diagnoses, classifying\n",
    "\n",
    "After the success of many large-scale general language models, many QA models embrace the following approach:\n",
    "\n",
    "them to do what a given set of humans want them to do. By default, language models optimize the next word prediction objective, which is only a proxy for what we want these models to do.\n",
    "\n",
    "Many applications in natural language processing rely on adapt-ing one large-scale, pre-trained language model to multiple down-stream applications. Such adaptation is usually done via ﬁne-tuning,\n",
    "\n",
    "among the largest language models today and we apply them on a wide range of language tasks, including classiﬁcation, summarization, question-answering, creative writing, dialogue, and others.\n",
    "\n",
    "Here is a question:\n",
    "\n",
    "What purpose do large language models serve in the field of natural language processing?.[/INST]'\n",
    "\n",
    "</blockquote>\n",
    "\n",
    "It does appear that, for this specific instance, changing the search type removed some of the 'references' snippets that had appeared previously.  The increase in number chunks returned also, for this specific instance, provides some additional, benefitial context.  without too much additional noise.\n",
    "\n",
    "**Tangent Curiousity Check** However, if we look at the gold standard answers below there is context that we are not retreiving directly from our documentation, such as 'speech recognition'.  Out of curiousity, and for experimentation, we are going to pass the gold answer through the RAG chain, increase the chunks to retreive to 12, to see what context it picks up.\n",
    "\n",
    "<blockquote>\n",
    "\n",
    "Target Responses Engineering = \"Large language models (LLMs) serve the purpose of enabling general-purpose language generation and other natural language processing tasks such as classification. They achieve this by learning statistical relationships from text documents during computationally intensive self-supervised and semi-supervised training. LLMs can be used for text generation by predicting the next token or word, making them valuable for tasks like **speech recognition, machine translation**, and information retrieval. Additionally, LLMs have superseded previous models like recurrent neural networks, showcasing their efficiency and effectiveness in NLP tasks.\"\n",
    "\n",
    "Marketing = \"Large language models serve the purpose of improving performance in various natural language processing tasks, such as **speech recognition, machine translation,** natural language generation, optical character recognition, handwriting recognition, grammar induction, and information retrieval.\"\n",
    "\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 111
    },
    "id": "eajmM-jswbLG",
    "outputId": "53e1393e-4c18-4f31-a6e3-fc78708f3853"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before LLM: messages=[HumanMessage(content='[INST]Please answer the question below only based on the context information provided.\\n\\nHere is a context:\\namong the largest language models today and we apply them on a wide range of language tasks,\\nincluding classiﬁcation, summarization, question-answering, creative writing, dialogue, and others.\\n\\nguage models trained on a large amount of text – where ﬁne-tuning on task-speciﬁc data after pre-\\ntraining on general domain data provides a signiﬁcant performance gain compared to training on\\n\\nFig. 13. The amount of computation used for training big language models of different sizes is getting big. (Image source: Brown et al., 2020).\\n\\n[2017], have achieved the state-of-the-art performance of various natural language processing (NLP) tasks, including\\ncontext representation learning Devlin et al. [2019], machine translation Vaswani et al. [2017], and language modeling\\n\\naugmentation for language models, recent works also study\\nretrieval for computer vision models (Ashual et al., 2022;\\nBlattmann et al., 2022; Gur et al., 2021; Sarto et al., 2022; Li\\net al., 2022; Ramos et al., 2023; Wang et al., 2022a). More\\n\\nMistral 7B takes a significant step in balancing the goals of getting high performance while keeping\\nlarge language models efficient. Through our work, our aim is to help the community create more\\n\\nQ19-1026.\\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E.\\nGonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model\\n\\nat https://github.com/DaoD/INTERS.\\n1\\nIntroduction\\nLarge language models (LLMs) have shown re-\\nmarkable capabilities across various natural lan-\\nguage processing (NLP) tasks. While these models\\nhave learned vast knowledge from large text cor-\\n\\nBig language models have been pre-trained on a large collection of unsupervised textual corpus. Given enough parameters, these models are able to memorize some factual knowledge within parameter weights. Therefore, we can use these models to do\\n\\naffordable, efficient, and high-performing language models that can be used in a wide range of\\nreal-world applications.\\n2\\nArchitectural details\\nFigure 1: Sliding Window Attention. The number of operations in vanilla attention is quadratic in the sequence\\n\\nspeciﬁc downstream tasks that were learned but not emphasized in the general pre-training model.\\n8\\nCONCLUSION AND FUTURE WORK\\nFine-tuning enormous language models is prohibitively expensive in terms of the hardware required\\n\\nINTERS: Unlocking the Power of Large Language Models in Search\\nwith Instruction Tuning\\nYutao Zhu1, Peitian Zhang1, Chenghao Zhang1,2∗, Yifei Chen1,3∗, Binyu Xie1\\nZhicheng Dou1†, Zheng Liu4, and Ji-Rong Wen1 \\n\\nHere is a question: \\nLarge language models serve the purpose of improving performance in various natural language processing tasks, such as speech recognition, machine translation, natural language generation, optical character recognition, handwriting recognition, grammar induction, and information retrieval..[/INST]')]\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Yes, that is correct. Large language models (LLMs) are designed to enhance performance across a broad range of natural language processing (NLP) tasks. The list of tasks you provided, including speech recognition, machine translation, natural language generation, and others, falls within the scope of applications where LLMs strive to excel.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# selecting a sample question and answers\n",
    "question = validation_questions_answers[0]['question']\n",
    "engineering_answer = validation_questions_answers[0][\"gold_answer_research\"]\n",
    "marketing_answer = validation_questions_answers[0][\"gold_answer_marketing\"]\n",
    "base_embeddings, text_splitter, qdrant_vectorstore, retriever = build_embedding_splitter_vectorstore('all-MiniLM-L6-v2', splitter, retr_search_type=\"mmr\", retr_k=12)\n",
    "qdrant_vectorstore = vectorize_documents(text_splitter, qdrant_vectorstore)\n",
    "llm_model = load_llm(llm)\n",
    "rag_chain = build_RAG_prompt_chain(rag_template, llm_model, retriever, format_docs)\n",
    "rag_chain.invoke(marketing_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xUMUBmMPuCnY"
   },
   "source": [
    "\n",
    "\n",
    "<blockquote>\n",
    "\n",
    "'[INST]Please answer the question below only based on the context information provided.\n",
    "\n",
    "Here is a context:\n",
    "\n",
    "among the largest language models today and we apply them on a wide range of language tasks, including classiﬁcation, summarization, question-answering, creative writing, dialogue, and others.\n",
    "\n",
    "guage models trained on a large amount of text where ﬁne-tuning on task-speciﬁc data after pre-training on general domain data provides a signiﬁcant performance gain compared to training on\n",
    "\n",
    "Fig. 13. The amount of computation used for training big language models of different sizes is getting big. (Image source: Brown et al., 2020).\n",
    "\n",
    "[2017], have achieved the state-of-the-art performance of various natural language processing (NLP) tasks, including\\ncontext representation learning Devlin et al. [2019], machine translation Vaswani et al. [2017], and language modeling\n",
    "\n",
    "augmentation for language models, recent works also study retrieval for computer vision models (Ashual et al., 2022; Blattmann et al., 2022; Gur et al., 2021; Sarto et al., 2022; Li et al., 2022; Ramos et al., 2023; Wang et al., 2022a). More\\\n",
    "\n",
    "Mistral 7B takes a significant step in balancing the goals of getting high performance while keeping large language models efficient. Through our work, our aim is to help the community create more\n",
    "\n",
    "Q19-1026. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model\n",
    "\n",
    "at https://github.com/DaoD/INTERS.\\n1\\nIntroduction\\nLarge language models (LLMs) have shown re-markable capabilities across various natural language processing (NLP) tasks. While these models have learned vast knowledge from large text cor-\n",
    "\n",
    "Big language models have been pre-trained on a large collection of unsupervised textual corpus. Given enough parameters, these models are able to memorize some factual knowledge within parameter weights. Therefore, we can use these models to do\n",
    "\n",
    "affordable, efficient, and high-performing language models that can be used in a wide range of real-world applications. Architectural details Figure 1: Sliding Window Attention. The number of operations in vanilla attention is quadratic in the sequence\n",
    "\n",
    "speciﬁc downstream tasks that were learned but not emphasized in the general pre-training model. 8 CONCLUSION AND FUTURE WORK Fine-tuning enormous language models is prohibitively expensive in terms of the hardware required\n",
    "\n",
    "INTERS: Unlocking the Power of Large Language Models in Search with Instruction Tuning Yutao Zhu1, Peitian Zhang1, Chenghao Zhang1,2, Yifei Chen1,3, Binyu Xie1 Zhicheng Dou1†, Zheng Liu4, and Ji-Rong Wen1\n",
    "\n",
    "Here is a question:\n",
    "\n",
    "Large language models serve the purpose of improving performance in various natural language processing tasks, such as speech recognition, machine translation, natural language generation, optical character recognition, handwriting recognition, grammar induction, and information retrieval..[/INST]'\n",
    "\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NvmneuViwZRL"
   },
   "source": [
    "So this shows that even if we used the gold answer as our prompt for document retreival, we are not returning any context that speaks specifically to 'speech recognition', which is found in both gold standard answers.  With this, we suspect that we may be limited in our ability to achieve the gold standard answers with the documenation that we have available.\n",
    "\n",
    "We will continue to hold true to our guiding metric and proceed with the highest performing parameters for the Retriever."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DVj0s96riuV2"
   },
   "source": [
    "**Retriever Decision:** From our grid search, we found that, for our current model, on this train set, using the default **mmr** search type, but with the return of **k=6** examples produces the best results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bPq6yhTXi7gj"
   },
   "source": [
    "#### 4.D) **Exploring LLM's**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MjVUi-_TtsW1"
   },
   "source": [
    "This concludes our exploration of the setup and retreival of our documentation. Now we move on to the downstream tasks; LLMs and prompts.\n",
    "\n",
    "Below we compare the two models we have available to us.  Before we explore these two models, we want to highlight some of the pros and cons of both.\n",
    "\n",
    "**Cohere:** While Cohere has a free trial version, for this model, and any production usage, a subscription is necessary. The expectation would be that a paid for service will have less downtime, better maintenance and consistent upgrades, increasing the value and justifying the cost.  The other thing to consider is that Cohere is an external resource.  Right now we are utilizing publicly available documents.  Should we, at some point, have sensitive documentation that we want to include in our document store, we should take into consideration the security of this context being shared externally.  An additional note to the advantages of Cohere would be that it should be a safer model in terms of outputting inappropriate content.\n",
    "\n",
    "**Mistral:**  With Mistral being an open source model, there are no subscription costs, but that comes at the potential detriment of down time, reduced maintenance and slower upgrades.  It also, in theory, is more likely to output inappropriate answers.  With that being said, one major advantage of Mistral being open source is that, should we start including sensitive documentation, this is a model that we could set up locally to ensure that no sensitive content is shared externally. You also have more freedom and flexibility to adjust parameters and fine-tune for specific use cases.\n",
    "\n",
    "Now we compare the models outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 400
    },
    "id": "WAi2r3AojCPu",
    "outputId": "763c77d1-743a-4c00-8e98-29c105f6ad80"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['llm_int4_enable_fp32_cpu_offload']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "\n",
    "embedding_model = 'all-MiniLM-L6-v2'\n",
    "rag_template = \"\"\"[INST]Please answer the question below only based on the context information provided.\\n\\nHere is a context:\\n{context} \\n\\nHere is a question: \\n{question}.[/INST]\"\"\"\n",
    "chunk_size = 256\n",
    "chunk_overlap = 16\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "base_embeddings, text_splitter, qdrant_vectorstore, retriever = build_embedding_splitter_vectorstore(embedding_model, splitter, retr_search_type=\"mmr\", retr_k=6)\n",
    "qdrant_vectorstore = vectorize_documents(text_splitter, qdrant_vectorstore)\n",
    "\n",
    "llms = ['cohere', 'mistral']\n",
    "results = []\n",
    "index = 0\n",
    "for llm in llms:\n",
    "    print(\"\")\n",
    "    print(\"Index \", index)\n",
    "    index += 1\n",
    "    print(f\"LLM: {llm}\")\n",
    "\n",
    "    # building model for current llm\n",
    "    llm_model = load_llm(llm)\n",
    "    rag_chain = build_RAG_prompt_chain(rag_template, llm_model, retriever, format_docs)\n",
    "\n",
    "    # computing metrics\n",
    "    metrics = ['rouge', 'bleu', 'bertscore']\n",
    "    llm_result = evaluate(metrics, validation_questions_answers, rag_chain, iterations=10, verbose=True, sleep=False)\n",
    "    results.append(llm_result)\n",
    "\n",
    "    # deleting previous variables\n",
    "    del llm_model\n",
    "    del rag_chain\n",
    "    gc.collect() # forcing garbage collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oFskGMOKlBPS",
    "outputId": "8c146586-201a-4b89-8ff9-91ac24935863"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: , 0\n",
      "Sample             6.600000\n",
      "eng_bleu           0.058681\n",
      "mk_bleu            0.030267\n",
      "eng_rouge          0.240201\n",
      "mk_rouge           0.166196\n",
      "eng_f1             0.875064\n",
      "mk_f1              0.859098\n",
      "composite_eng      0.521329\n",
      "composite_mk       0.485461\n",
      "composite_total    0.506981\n",
      "dtype: float64\n",
      "Index: , 1\n",
      "Sample             6.600000\n",
      "eng_bleu           0.031601\n",
      "mk_bleu            0.018405\n",
      "eng_rouge          0.171653\n",
      "mk_rouge           0.105633\n",
      "eng_f1             0.817403\n",
      "mk_f1              0.809078\n",
      "composite_eng      0.466518\n",
      "composite_mk       0.439910\n",
      "composite_total    0.455875\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "for i, result in enumerate(results):\n",
    "  print(\"Index: ,\", i)\n",
    "  result = composite_evaluation(result)\n",
    "  print(result.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wQdOJW5LnlQS"
   },
   "source": [
    "##### **Output Readability Check**\n",
    "\n",
    "Although the Cohere model is showing significantly better scores, we still want to look at the actually output between the two models for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w_O2VDtWoF8R"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "embedding_model = 'all-MiniLM-L6-v2'\n",
    "rag_template = \"\"\"[INST]Please answer the question below only based on the context information provided.\\n\\nHere is a context:\\n{context} \\n\\nHere is a question: \\n{question}.[/INST]\"\"\"\n",
    "chunk_size = 256\n",
    "chunk_overlap = 16\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "base_embeddings, text_splitter, qdrant_vectorstore, retriever = build_embedding_splitter_vectorstore(embedding_model, splitter, retr_search_type=\"mmr\", retr_k=6)\n",
    "qdrant_vectorstore = vectorize_documents(text_splitter, qdrant_vectorstore)\n",
    "\n",
    "# selecting a sample question and answers\n",
    "question = validation_questions_answers[0]['question']\n",
    "engineering_answer = validation_questions_answers[0][\"gold_answer_research\"]\n",
    "marketing_answer = validation_questions_answers[0][\"gold_answer_marketing\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 148
    },
    "id": "8vnDCTbBoLSZ",
    "outputId": "7a61d38c-4a5e-47d9-d7e4-1f7f4aaf16d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before LLM: messages=[HumanMessage(content='[INST]Please answer the question below only based on the context information provided.\\n\\nHere is a context:\\nalgorithm. Machine Learning, 97(3):327–351, July 2014. doi: 10.1007/s10994-014-5458-8.\\nURL https://doi.org/10.1007/s10994-014-5458-8.\\n[10] Y. Chen, R. Wang, H. Jiang, S. Shi, and R.-L. Xu. Exploring the use of large language models\\n\\nmisuse, there are many domains where large language models should be deployed only with great\\ncare, or not at all. Examples include high-stakes domains such as medical diagnoses, classifying\\n\\nAfter the success of many large-scale general language models, many QA models embrace the following approach:\\n\\nthem to do what a given set of humans want them to do. By default, language models optimize\\nthe next word prediction objective, which is only a proxy for what we want these models to do.\\n\\nMany applications in natural language processing rely on adapt-\\ning one large-scale, pre-trained language model to multiple down-\\nstream applications. Such adaptation is usually done via ﬁne-tuning,\\n\\namong the largest language models today and we apply them on a wide range of language tasks,\\nincluding classiﬁcation, summarization, question-answering, creative writing, dialogue, and others. \\n\\nHere is a question: \\nWhat purpose do large language models serve in the field of natural language processing?.[/INST]')]\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Large language models serve as the foundation for many natural language processing (NLP) applications. They are pre-trained on vast amounts of text data and can be adapted to various downstream tasks through fine-tuning. This allows for efficient and effective language model deployment in multiple domains, including classification, summarization, question-answering, creative writing, and dialogue. While there are concerns about potential misuse, large language models have revolutionized NLP by providing adaptable and contextually aware language understanding and generation capabilities.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_model = load_llm('cohere')\n",
    "rag_chain = build_RAG_prompt_chain(rag_template, llm_model, retriever, format_docs)\n",
    "rag_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 703,
     "referenced_widgets": [
      "1e01b6ba83b84354a783de89601a631f",
      "80ed477043a34061af1d0329def8c336",
      "c38696dc6db64ee28c51ed5718866099",
      "5d331d5b6973462f9883747f8b4c4ab8",
      "8f3ce5ac92a84bb194fe4145ea108400",
      "772b3cc1599340d987c3e9f9d5bd00f0",
      "a57a9ffe8d234ada849669fa408f8bb1",
      "e76dc65a734b4fbcb02d886753ec3753",
      "1204cf7954ff4b638a5d1d25cba79c9e",
      "a66e3f2c98c647daa227659755a11c4e",
      "d55a9624c1984862b1d3a89a9b56574f",
      "af754c24722147dbbe40680118e19bcb",
      "a1fd3c805831464ba8f5b175b293ea60",
      "faefbaa58a5449fb8d308340131d4ec7",
      "00298c86bd5347a7aa8ebe3d2eae51b2",
      "31931eb718934dc793f937ddacb2083b",
      "c3ed1caeb4af4627a60201cce9420f07",
      "2f3b88aa761040c19eafa325ea877f10",
      "f6e7c316f2b746fda925c279be258bfe",
      "145e84288178448dbb4fd3e4b0e45172",
      "e9610898c8f34e0898c0c4f33d76e666",
      "f2383551e8b74982aae2eca73542f635",
      "10e64cd2844740ef9c104fd8adcff09e",
      "c25257a378c04cffa2ce81a6664b87a8",
      "882aef82a3fd42f3a6b310e63e5dfe50",
      "8ad266de548b408face2edc4b1862d7b",
      "292e98e2c2ce46f69e109aa9242431ce",
      "669fe02d06e64c7092110781e5f1e6f4",
      "e46014f27c934682aba97fac7b431abf",
      "9a934b589c8b4b1a8be18814fdbc11f8",
      "cd3adb62488d414b8b7f359786129f17",
      "39fe49065b394b85a79390ec03b0713e",
      "f533e63c0dde4067899e4aeddda25c4c",
      "d6558e58e4db4c04b9dc26cfad065696",
      "6f961b89a2944740925b283f7dce55ea",
      "94cd614cea4a4dd1b60080a69e2c3b83",
      "3ccdbbecfcb54492a15bc0e85f9f0d92",
      "82b57df2e40549b495e247e0602004eb",
      "92a499f69c5046b4be9e5795fccd44ce",
      "4fe1127f030146faa8640301730cbb76",
      "a8847a3075b34dc9b74fb17807b1e214",
      "55d3a2219935428a9b559e8b64ad223a",
      "64bdfdf276a34818bca110ad18c96f02",
      "c67a69bb0e0f4ac18792b71d1e1c5efd",
      "cfaad9a638e843a0b4581d2391cb437f",
      "aa4b86f1504449b4956fd2155335937f",
      "6015887cf6c5459080bde5b0c83442b5",
      "4866cc1279194c27a916d1020d090f20",
      "793bd871a2374c2ebf658af2558c8b43",
      "442b3a51bc49421f9a3598ac47a90711",
      "6a07a9da13da4766a8543afaef5c2cbf",
      "ff86ec4b2122413b96e913eb618abae2",
      "f039047818de46e98962f3d9fb18611b",
      "cd7677fbbacd479f9048558a7da6e694",
      "8bbe4457ad124a1ab7e93c3c21efd695",
      "53478135a2d847beb30b6ae61e2d12fc",
      "c6acb332e87942939a4b82c89e17e5fc",
      "456e2cb3d1e44595b9cd298008811187",
      "e6595872d82046229b0dbe70a9fd199e",
      "970ae7b9ad144e6682bad2f38ec98cba",
      "c63a8af96239401e91d302b01f38b849",
      "ccac7e15cefa4cbe90f88577204b0697",
      "01fa84447757403d87609a97457097a9",
      "5c9bbc07ecb9430ea7e4a98ef70ab462",
      "9d578c44f7e341c28c5125f8bdecf3b0",
      "6bb7bb647ade4cf28ae0dc9bd873dc32",
      "8273312808344fe6bbcee1d2f5ca3943",
      "64af9d28455e41cdb6164684ebf38d07",
      "986baef623e3412d83f12636a77c3f73",
      "71f57f5bf5d3449fb9dd0ecd97f4710d",
      "78b8805aa3a545d3bad8a432337bd554",
      "d84cdde777c742249ddd605bc4587a21",
      "680f4ae2881a47a98d3b0dca816dc3bf",
      "87027f1e6313419ca41f744414d0280b",
      "372c77e8ca214939b29f7b1a61a60c48",
      "9828a559d1b44511b106ed2a0f9fe1dd",
      "5b60e1c5ac11430da1f590750d8ec497",
      "0f831c120bbd49ec9156d2e5f45def29",
      "4aa057719e8f484695edb384033c11af",
      "721b98900e11438b8a13c98d8f442fc2",
      "e0ebbf9c653a4b38a5983ff26b3cb122",
      "a7c1dd526dc64b2899545a0e84e63014",
      "c4add13e56ec48dfb05f4f1a39fa4efb",
      "8f1cb51519294dcd87652085ca49e697",
      "e1ef57d034024c739b00365e46eb3dfb",
      "eb23096a775e46bd8511af85da94bef2",
      "6233e142295b4387ab8438288a94fa08",
      "023aafd15019487f92276d8230f52e7d",
      "f9866af4b7c9478e8a8afa2336d8c8a9",
      "9f658dda4e71401db3ae921d8b978c4c",
      "a2eea0216f3a421c974cef798f7ffc4d",
      "7dd4e6bed42b4fbe919fc565203b63a4",
      "d3985b65b1ae415aa755bf19436dbd6f",
      "8718e08801544720a85064cd4928a93c",
      "110532eccf6847df92563cf649efb47b",
      "cf8eed43356247c0a8cf80abbc80ccf5",
      "3fad7da98caf4aa090d820cbe6481af0",
      "506a2c798745430d8ab8d9eddac7c58b",
      "3d017f12eab847908ba278eae1898616",
      "268267b9a9ac4a268e1aaff23f1da544",
      "7ec4029f051a48ce9e838d6f990b647e",
      "0e0be7f4054a4abe808accd1eb542e36",
      "2d7f4a1c4b7e40e497d8b383b464ec07",
      "fdfcea700a644a8d9deb6cc679eaaf4d",
      "b2d5a7480bde43c28c756445bcd5cdfd",
      "eab1a1b83de14827ae242fee5ff91540",
      "a5c5577478b04951a6c3093b175c8b79",
      "a486fcff9502442e9f68619201b98673",
      "d72c50264df5470d92cc39c23ffbff04",
      "5534bdabb6a749bfb9caec514e9ffa49",
      "9adb71f09145493d9cf75f15b4d66f36",
      "288829dde7444ecea0c7590531881e1a",
      "9fc629fd6f98499086f9ea106eeccaae",
      "a91b290b13b74e7c8b5464bc07be0d69",
      "2d3172fe38c6481fa3451b659007a843",
      "811258b1e9d14978a045a17bfa0fac76",
      "762db47a0b644102a206d50917cacd68",
      "be7ccdd3bd5e4a7ab71df04aaebf18fb",
      "df918b4f89ca496d9f2c5b22c6e096e7",
      "8486056394b1413cb6d0326acbb50667",
      "00ba5672234a4879b0463f784b832b1e"
     ]
    },
    "id": "L_DS6EAjoSVY",
    "outputId": "449b335b-1198-43ee-c5d2-44deb0afab31"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['llm_int4_enable_fp32_cpu_offload']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e01b6ba83b84354a783de89601a631f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af754c24722147dbbe40680118e19bcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10e64cd2844740ef9c104fd8adcff09e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6558e58e4db4c04b9dc26cfad065696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfaad9a638e843a0b4581d2391cb437f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53478135a2d847beb30b6ae61e2d12fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8273312808344fe6bbcee1d2f5ca3943",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f831c120bbd49ec9156d2e5f45def29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.47k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9866af4b7c9478e8a8afa2336d8c8a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "268267b9a9ac4a268e1aaff23f1da544",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9adb71f09145493d9cf75f15b4d66f36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before LLM: messages=[HumanMessage(content='[INST]Please answer the question below only based on the context information provided.\\n\\nHere is a context:\\nalgorithm. Machine Learning, 97(3):327–351, July 2014. doi: 10.1007/s10994-014-5458-8.\\nURL https://doi.org/10.1007/s10994-014-5458-8.\\n[10] Y. Chen, R. Wang, H. Jiang, S. Shi, and R.-L. Xu. Exploring the use of large language models\\n\\nmisuse, there are many domains where large language models should be deployed only with great\\ncare, or not at all. Examples include high-stakes domains such as medical diagnoses, classifying\\n\\nAfter the success of many large-scale general language models, many QA models embrace the following approach:\\n\\nthem to do what a given set of humans want them to do. By default, language models optimize\\nthe next word prediction objective, which is only a proxy for what we want these models to do.\\n\\nMany applications in natural language processing rely on adapt-\\ning one large-scale, pre-trained language model to multiple down-\\nstream applications. Such adaptation is usually done via ﬁne-tuning,\\n\\namong the largest language models today and we apply them on a wide range of language tasks,\\nincluding classiﬁcation, summarization, question-answering, creative writing, dialogue, and others. \\n\\nHere is a question: \\nWhat purpose do large language models serve in the field of natural language processing?.[/INST]')]\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"Human: [INST]Please answer the question below only based on the context information provided.\\n\\nHere is a context:\\nalgorithm. Machine Learning, 97(3):327–351, July 2014. doi: 10.1007/s10994-014-5458-8.\\nURL https://doi.org/10.1007/s10994-014-5458-8.\\n[10] Y. Chen, R. Wang, H. Jiang, S. Shi, and R.-L. Xu. Exploring the use of large language models\\n\\nmisuse, there are many domains where large language models should be deployed only with great\\ncare, or not at all. Examples include high-stakes domains such as medical diagnoses, classifying\\n\\nAfter the success of many large-scale general language models, many QA models embrace the following approach:\\n\\nthem to do what a given set of humans want them to do. By default, language models optimize\\nthe next word prediction objective, which is only a proxy for what we want these models to do.\\n\\nMany applications in natural language processing rely on adapt-\\ning one large-scale, pre-trained language model to multiple down-\\nstream applications. Such adaptation is usually done via ﬁne-tuning,\\n\\namong the largest language models today and we apply them on a wide range of language tasks,\\nincluding classiﬁcation, summarization, question-answering, creative writing, dialogue, and others. \\n\\nHere is a question: \\nWhat purpose do large language models serve in the field of natural language processing?.[/INST] Large language models serve as a tool to solve various problems in natural language processing by providing a means of understanding and generating textual data. They can be used for classification, summarization, question answering, creative writing, dialogue, among other applications. However, it's important to note that they may have limitations and potential misuses, especially in high-stakes domains such as medical diagnosis.\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_model = load_llm('mistral')\n",
    "rag_chain = build_RAG_prompt_chain(rag_template, llm_model, retriever, format_docs)\n",
    "rag_chain.invoke(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N8Z8PZhlqU9w"
   },
   "source": [
    "Below is a breakdown of the prompt, responses and target answers.\n",
    "\n",
    "**LLM Prompt**\n",
    "\n",
    "<blockquote>\n",
    "\n",
    "'[INST]Please answer the question below only based on the context information provided.\n",
    "\n",
    "Here is a context:\n",
    "\n",
    "algorithm. Machine Learning, 97(3):327–351, July 2014. doi: 10.1007/s10994-014-5458-8. URL https://doi.org/10.1007/s10994-014-5458-8.[10] Y. Chen, R. Wang, H. Jiang, S. Shi, and R.-L. Xu. Exploring the use of large language models\n",
    "\n",
    "misuse, there are many domains where large language models should be deployed only with great care, or not at all. Examples include high-stakes domains such as medical diagnoses, classifying\n",
    "\n",
    "After the success of many large-scale general language models, many QA models embrace the following approach:\n",
    "\n",
    "them to do what a given set of humans want them to do. By default, language models optimize the next word prediction objective, which is only a proxy for what we want these models to do.\n",
    "\n",
    "Many applications in natural language processing rely on adapting one large-scale, pre-trained language model to multiple downstream applications. Such adaptation is usually done via ﬁne-tuning,\n",
    "\n",
    "among the largest language models today and we apply them on a wide range of language tasks,\\nincluding classiﬁcation, summarization, question-answering, creative writing, dialogue, and others.\n",
    "\n",
    "Here is a question:\n",
    "\n",
    "What purpose do large language models serve in the field of natural language processing?.[/INST]'\n",
    "\n",
    "</blockquote>\n",
    "\n",
    "**Model Answers**\n",
    "\n",
    "<blockquote>\n",
    "\n",
    "**Cohere Answer** 'Large language models serve as the foundation for many natural language processing (NLP) applications. They are pre-trained on vast amounts of text data and can be adapted to various downstream tasks through fine-tuning. This allows for efficient and effective language model deployment in multiple domains, including classification, summarization, question-answering, creative writing, and dialogue. While there are concerns about potential misuse, large language models have revolutionized NLP by providing adaptable and contextually aware language understanding and generation capabilities.'\n",
    "\n",
    "**Mistral Answer** 'Large language models serve as a tool to solve various problems in natural language processing by providing a means of understanding and generating textual data. They can be used for classification, summarization, question answering, creative writing, dialogue, among other applications. However, it's important to note that they may have limitations and potential misuses, especially in high-stakes domains such as medical diagnosis.'\n",
    "\n",
    "</blockquote>\n",
    "\n",
    "**Target Responses**\n",
    "\n",
    "<blockquote>\n",
    "\n",
    "**Engineering** \"Large language models (LLMs) serve the purpose of enabling general-purpose language generation and other natural language processing tasks such as classification. They achieve this by learning statistical relationships from text documents during computationally intensive self-supervised and semi-supervised training. LLMs can be used for text generation by predicting the next token or word, making them valuable for tasks like speech recognition, machine translation, and information retrieval. Additionally, LLMs have superseded previous models like recurrent neural networks, showcasing their efficiency and effectiveness in NLP tasks.\"\n",
    "\n",
    "**Marketing** \"Large language models serve the purpose of improving performance in various natural language processing tasks, such as speech recognition, machine translation, natural language generation, optical character recognition, handwriting recognition, grammar induction, and information retrieval.\"\n",
    "\n",
    "</blockquote>\n",
    "\n",
    "For this example it does appear that the Cohere model is providing a more comprehensive answer that Mistral.  We, once again, will be proceeding with what our evaluation metric as identified as the superior (default) LLM.\n",
    "\n",
    "**LLM Decision:** We will be proceeding with the 'Cohere' LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FGgw9AHe0g2h"
   },
   "source": [
    "#### 4.E) **Modifying Prompt**\n",
    "\n",
    "With our LLM chosen, we now look at our last (but not least) chosen step in tuning our RAG chain model; the prompt. We first look to improve the standardized response, and then defining specific responses for the separate departements.\n",
    "\n",
    "*Note: The RAG chains functions were defined to allow for the delineation between 'engineering' and 'marketing' departments.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h11tBRq9ZH3L"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "embedding_model = 'all-MiniLM-L6-v2'\n",
    "chunk_size = 256\n",
    "chunk_overlap = 16\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "llm = \"cohere\"\n",
    "\n",
    "base_embeddings, text_splitter, qdrant_vectorstore, retriever = build_embedding_splitter_vectorstore(embedding_model, splitter, retr_search_type=\"mmr\", retr_k=6)\n",
    "qdrant_vectorstore = vectorize_documents(text_splitter, qdrant_vectorstore)\n",
    "llm_model = load_llm(llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ckroOX6qc9cD"
   },
   "source": [
    "##### **Improving Agnostic Language in Prompt**\n",
    "\n",
    "**First Pass:**\n",
    "Below is a simple adjustment (from the baseline prompt) in formatting and word selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tWb_XdlMdJ5G"
   },
   "outputs": [],
   "source": [
    "rag_template = \"\"\"[INST]\n",
    "              Please provide an precise and concise answer to the question below based on the context information provided.\\n\\n\n",
    "              Below is a context:\\n{context}\\n\n",
    "              Below is a question:\\n{question}\\n[/INST]\n",
    "              \"\"\"\n",
    "\n",
    "rag_chain = build_RAG_prompt_chain(rag_template, llm_model, retriever, format_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TYO3YKRHdZBy",
    "outputId": "3d945530-335c-4271-f204-8003f17396fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before LLM: messages=[HumanMessage(content='[INST]\\n              Please provide an precise and concise answer to the question below based on the context information provided.\\n\\n\\n              Below is a context:\\nalgorithm. Machine Learning, 97(3):327–351, July 2014. doi: 10.1007/s10994-014-5458-8.\\nURL https://doi.org/10.1007/s10994-014-5458-8.\\n[10] Y. Chen, R. Wang, H. Jiang, S. Shi, and R.-L. Xu. Exploring the use of large language models\\n\\nmisuse, there are many domains where large language models should be deployed only with great\\ncare, or not at all. Examples include high-stakes domains such as medical diagnoses, classifying\\n\\nAfter the success of many large-scale general language models, many QA models embrace the following approach:\\n\\nthem to do what a given set of humans want them to do. By default, language models optimize\\nthe next word prediction objective, which is only a proxy for what we want these models to do.\\n\\nMany applications in natural language processing rely on adapt-\\ning one large-scale, pre-trained language model to multiple down-\\nstream applications. Such adaptation is usually done via ﬁne-tuning,\\n\\namong the largest language models today and we apply them on a wide range of language tasks,\\nincluding classiﬁcation, summarization, question-answering, creative writing, dialogue, and others.\\n\\n              Below is a question:\\nWhat purpose do large language models serve in the field of natural language processing?\\n[/INST]\\n              ')]\n",
      "Before LLM: messages=[HumanMessage(content='[INST]\\n              Please provide an precise and concise answer to the question below based on the context information provided.\\n\\n\\n              Below is a context:\\nalgorithm. Machine Learning, 97(3):327–351, July 2014. doi: 10.1007/s10994-014-5458-8.\\nURL https://doi.org/10.1007/s10994-014-5458-8.\\n[10] Y. Chen, R. Wang, H. Jiang, S. Shi, and R.-L. Xu. Exploring the use of large language models\\n\\nmisuse, there are many domains where large language models should be deployed only with great\\ncare, or not at all. Examples include high-stakes domains such as medical diagnoses, classifying\\n\\nAfter the success of many large-scale general language models, many QA models embrace the following approach:\\n\\nthem to do what a given set of humans want them to do. By default, language models optimize\\nthe next word prediction objective, which is only a proxy for what we want these models to do.\\n\\nMany applications in natural language processing rely on adapt-\\ning one large-scale, pre-trained language model to multiple down-\\nstream applications. Such adaptation is usually done via ﬁne-tuning,\\n\\namong the largest language models today and we apply them on a wide range of language tasks,\\nincluding classiﬁcation, summarization, question-answering, creative writing, dialogue, and others.\\n\\n              Below is a question:\\nWhat purpose do large language models serve in the field of natural language processing?\\n[/INST]\\n              ')]\n",
      "------------------------------------------------------------\n",
      "Question: What purpose do large language models serve in the field of natural language processing?\n",
      " \n",
      "Engineering Prediction: Large language models serve as the foundation for adapting and fine-tuning various downstream applications in natural language processing, enabling a wide range of tasks such as classification, summarization, question-answering, creative content generation, and dialogue systems.\n",
      "Engineering Answer: Large language models (LLMs) serve the purpose of enabling general-purpose language generation and other natural language processing tasks such as classification. They achieve this by learning statistical relationships from text documents during computationally intensive self-supervised and semi-supervised training. LLMs can be used for text generation by predicting the next token or word, making them valuable for tasks like speech recognition, machine translation, and information retrieval. Additionally, LLMs have superseded previous models like recurrent neural networks, showcasing their efficiency and effectiveness in NLP tasks.\n",
      "\n",
      "Marketing Prediction: Large language models serve as the foundation for adapting and fine-tuning various downstream applications in natural language processing, enabling a wide range of tasks such as classification, summarization, question-answering, creative content generation, and dialogue systems.\n",
      "Marketing Answer: Large language models serve the purpose of improving performance in various natural language processing tasks, such as speech recognition, machine translation, natural language generation, optical character recognition, handwriting recognition, grammar induction, and information retrieval.\n",
      "Before LLM: messages=[HumanMessage(content='[INST]\\n              Please provide an precise and concise answer to the question below based on the context information provided.\\n\\n\\n              Below is a context:\\n[43] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal,\\nK. Slama, A. Ray, et al. Training language models to follow instructions with human feedback.\\n\\nFig. 13. The amount of computation used for training big language models of different sizes is getting big. (Image source: Brown et al., 2020).\\n\\nconsider it in our experiments.\\nT5\\nThe T5 model (Raffel et al., 2020) proposes\\nanother method of pre-training Transformers for\\ntransfer learning, via converting several language\\ntasks into “text-to-text” tasks. T5 is pre-trained\\n\\nSch¨\\narli, and Denny Zhou. Large language models can be easily distracted by irrelevant context.\\nIn Proceedings of the 40th International Conference on Machine Learning, 2023. URL https:\\n//proceedings.mlr.press/v202/shi23a.html.\\n\\nspeciﬁc downstream tasks that were learned but not emphasized in the general pre-training model.\\n8\\nCONCLUSION AND FUTURE WORK\\nFine-tuning enormous language models is prohibitively expensive in terms of the hardware required\\n\\nusing sequences longer than the pre-training settings for language modeling. At 1000 steps, we can\\nsee the models have improved steadily and achieve a significantly better perplexity.\\nModel\\nEvaluation Context Window Size\\nSize\\nContext Window\\nMethod\\n2048\\n\\n              Below is a question:\\nHow does a large language model learn from text during training?\\n[/INST]\\n              ')]\n",
      "Before LLM: messages=[HumanMessage(content='[INST]\\n              Please provide an precise and concise answer to the question below based on the context information provided.\\n\\n\\n              Below is a context:\\n[43] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal,\\nK. Slama, A. Ray, et al. Training language models to follow instructions with human feedback.\\n\\nFig. 13. The amount of computation used for training big language models of different sizes is getting big. (Image source: Brown et al., 2020).\\n\\nconsider it in our experiments.\\nT5\\nThe T5 model (Raffel et al., 2020) proposes\\nanother method of pre-training Transformers for\\ntransfer learning, via converting several language\\ntasks into “text-to-text” tasks. T5 is pre-trained\\n\\nSch¨\\narli, and Denny Zhou. Large language models can be easily distracted by irrelevant context.\\nIn Proceedings of the 40th International Conference on Machine Learning, 2023. URL https:\\n//proceedings.mlr.press/v202/shi23a.html.\\n\\nspeciﬁc downstream tasks that were learned but not emphasized in the general pre-training model.\\n8\\nCONCLUSION AND FUTURE WORK\\nFine-tuning enormous language models is prohibitively expensive in terms of the hardware required\\n\\nusing sequences longer than the pre-training settings for language modeling. At 1000 steps, we can\\nsee the models have improved steadily and achieve a significantly better perplexity.\\nModel\\nEvaluation Context Window Size\\nSize\\nContext Window\\nMethod\\n2048\\n\\n              Below is a question:\\nHow does a large language model learn from text during training?\\n[/INST]\\n              ')]\n",
      "------------------------------------------------------------\n",
      "Question: How does a large language model learn from text during training?\n",
      " \n",
      "Engineering Prediction: A large language model learns from text during training through a process called \"pre-training.\" In this phase, the model is exposed to a vast amount of text data and learns to predict the next word in a sequence, known as language modeling. This process helps the model capture the underlying patterns and semantics of human language.\n",
      "Engineering Answer: A large language model learns from text during training by first going through an unsupervised generative 'pretraining' stage where it sets initial parameters using a language modeling objective. Then, it goes through a supervised discriminative 'fine-tuning' stage where it refines its parameters based on annotated examples or task demonstrations. This dual-stage approach allows the model to learn statistical relationships from text documents in a computationally intensive process, enabling it to achieve general-purpose language generation and natural language processing tasks.\n",
      "\n",
      "Marketing Prediction: A large language model learns from text during training through a process called \"pre-training.\" In this phase, the model is exposed to a vast amount of text data and learns to predict the next word in a sequence, known as language modeling. This process helps the model understand the patterns, syntax, and semantics of human language. After pre-training, the model can be fine-tuned on specific downstream tasks, such as text classification or question answering, by adjusting its parameters based on labeled examples. This allows the model to learn task-specific nuances and perform better on those particular tasks.\n",
      "Marketing Answer: A large language model learns from text during training by first pretraining on a diverse dataset to acquire general language knowledge, and then fine-tuning on specific tasks or demonstrations to adapt its parameters for more targeted performance.\n",
      "Before LLM: messages=[HumanMessage(content='[INST]\\n              Please provide an precise and concise answer to the question below based on the context information provided.\\n\\n\\n              Below is a context:\\nSch¨\\narli, and Denny Zhou. Large language models can be easily distracted by irrelevant context.\\nIn Proceedings of the 40th International Conference on Machine Learning, 2023. URL https:\\n//proceedings.mlr.press/v202/shi23a.html.\\n\\nWe mostly focus on QA models that contain neural networks, specially Transformer-based language models.\\nI admit that I missed a lot of papers with architectures designed specifically for QA tasks between 2017-2019\\uf8ffüòî\\n\\nour current language model systems, we seek general and scalable methods that work for future AI\\nsystems (Leike et al., 2018). The systems we work with here are still fairly limited, but they are\\n\\nAfter the success of many large-scale general language models, many QA models embrace the following approach:\\n\\nere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Ar-\\nmand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation\\nlanguage models, 2023.\\n\\nShishir G Patil, Tianjun Zhang, Xin Wang, and\\nJoseph E Gonzalez. 2023. Gorilla: Large language\\nmodel connected with massive apis. arXiv preprint\\narXiv:2305.15334.\\nJayr Pereira, Robson Fidalgo, Roberto Lotufo, and Ro-\\n\\n              Below is a question:\\nWhat are some key architectures behind the development of large language models?\\n[/INST]\\n              ')]\n",
      "Before LLM: messages=[HumanMessage(content='[INST]\\n              Please provide an precise and concise answer to the question below based on the context information provided.\\n\\n\\n              Below is a context:\\nSch¨\\narli, and Denny Zhou. Large language models can be easily distracted by irrelevant context.\\nIn Proceedings of the 40th International Conference on Machine Learning, 2023. URL https:\\n//proceedings.mlr.press/v202/shi23a.html.\\n\\nWe mostly focus on QA models that contain neural networks, specially Transformer-based language models.\\nI admit that I missed a lot of papers with architectures designed specifically for QA tasks between 2017-2019\\uf8ffüòî\\n\\nour current language model systems, we seek general and scalable methods that work for future AI\\nsystems (Leike et al., 2018). The systems we work with here are still fairly limited, but they are\\n\\nAfter the success of many large-scale general language models, many QA models embrace the following approach:\\n\\nere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Ar-\\nmand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation\\nlanguage models, 2023.\\n\\nShishir G Patil, Tianjun Zhang, Xin Wang, and\\nJoseph E Gonzalez. 2023. Gorilla: Large language\\nmodel connected with massive apis. arXiv preprint\\narXiv:2305.15334.\\nJayr Pereira, Robson Fidalgo, Roberto Lotufo, and Ro-\\n\\n              Below is a question:\\nWhat are some key architectures behind the development of large language models?\\n[/INST]\\n              ')]\n",
      "------------------------------------------------------------\n",
      "Question: What are some key architectures behind the development of large language models?\n",
      " \n",
      "Engineering Prediction: Some key architectures behind the development of large language models include Transformer-based language models and neural networks. Additionally, the papers \"Llama: Open and Efficient Foundation Language Models\" and \"Gorilla: Large Language Model Connected with Massive APIs\" may contain insights into the architectures used in their respective models.\n",
      "Engineering Answer: Key architectures behind the development of large language models include the use of self-attention mechanisms, such as those seen in Transformer decoders. These architectures have been applied to tasks like autoregressive language modeling and have led to the dominance of Transformer-based language models in NLP. Models like BERT and GPT-2 have further advanced this paradigm, showcasing the power of large Transformer language models in achieving state-of-the-art results across various NLP tasks. Additionally, architectures like neural-retriever-in-the-loop generative-based models have shown improvements in tasks like open-domain QA and knowledge-grounded dialogue, emphasizing the importance of consistent and engaging responses in long-form generation and multi-turn conversations.\n",
      "\n",
      "Marketing Prediction: Some key architectures behind the development of large language models include:\n",
      "- Transformer-based language models, which have been widely embraced by QA models and have become the de facto standard for many NLP tasks.\n",
      "- Neural networks, which are a fundamental component of most QA models and have been integral to the success of large-scale general language models.\n",
      "- Models such as LLaMA (LLM Architecture) and Gorilla, which are designed to be open, efficient, and connected with massive APIs, respectively.\n",
      "Marketing Answer: Key architectures behind the development of large language models include Transformer-based models such as BERT and GPT-2, which utilize self-attention mechanisms for tasks like autoregressive language modeling and knowledge-grounded dialogue. These models have shown significant success in NLP tasks and have led to advancements in general-purpose language generation and natural language processing.\n",
      "Before LLM: messages=[HumanMessage(content='[INST]\\n              Please provide an precise and concise answer to the question below based on the context information provided.\\n\\n\\n              Below is a context:\\nlimitations, and societal impact of large language models. arXiv preprint arXiv:2102.02503.\\nThoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng, H.-T., Jin, A., Bos,\\n\\nmodels.\\nhttps://github.com/tatsu-lab/\\nalpaca_eval, 2023.\\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,\\nMishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,\\net al. Training language models to follow instructions\\n\\n2019), or causal mediation analysis (Vig et al., 2020). There is also work on steering the generation\\nof language models using a second (usually smaller) language model (Dathathri et al., 2019; Krause\\n\\nAfter the success of many large-scale general language models, many QA models embrace the following approach:\\n\\n[47] Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu,\\nand Alexander Miller. Language models as knowledge bases? In Proceedings of the 2019\\n\\nFraser Kelton\\nLuke Miller\\nMaddie Simens\\nAmanda Askell†\\nPeter Welinder\\nPaul Christiano∗†\\nJan Leike∗\\nRyan Lowe∗\\nOpenAI\\nAbstract\\nMaking language models bigger does not inherently make them better at following\\n\\n              Below is a question:\\nCan you name some specific large language models and the companies or organizations that have developed them?\\n[/INST]\\n              ')]\n",
      "Before LLM: messages=[HumanMessage(content='[INST]\\n              Please provide an precise and concise answer to the question below based on the context information provided.\\n\\n\\n              Below is a context:\\nlimitations, and societal impact of large language models. arXiv preprint arXiv:2102.02503.\\nThoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng, H.-T., Jin, A., Bos,\\n\\nmodels.\\nhttps://github.com/tatsu-lab/\\nalpaca_eval, 2023.\\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,\\nMishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,\\net al. Training language models to follow instructions\\n\\n2019), or causal mediation analysis (Vig et al., 2020). There is also work on steering the generation\\nof language models using a second (usually smaller) language model (Dathathri et al., 2019; Krause\\n\\nAfter the success of many large-scale general language models, many QA models embrace the following approach:\\n\\n[47] Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu,\\nand Alexander Miller. Language models as knowledge bases? In Proceedings of the 2019\\n\\nFraser Kelton\\nLuke Miller\\nMaddie Simens\\nAmanda Askell†\\nPeter Welinder\\nPaul Christiano∗†\\nJan Leike∗\\nRyan Lowe∗\\nOpenAI\\nAbstract\\nMaking language models bigger does not inherently make them better at following\\n\\n              Below is a question:\\nCan you name some specific large language models and the companies or organizations that have developed them?\\n[/INST]\\n              ')]\n",
      "------------------------------------------------------------\n",
      "Question: Can you name some specific large language models and the companies or organizations that have developed them?\n",
      " \n",
      "Engineering Prediction: Some prominent examples of large language models and their developers include: \n",
      "\n",
      "- GPT-3 (Generative Pre-trained Transformer 3) by OpenAI\n",
      "- BERT (Bidirectional Encoder Representations from Transformers) by Google\n",
      "- T5 (Text-to-Text Transfer Transformer) by Google AI\n",
      "- RoBERTa (Robustly Optimized BERT Approach) by Facebook AI Research\n",
      "- Megatron-LM by NVIDIA and Microsoft\n",
      "- BART (Bidirectional and Auto-Regressive Transformers) by Facebook AI Research\n",
      "\n",
      "These models have pushed the boundaries of natural language processing and have been applied in various tasks such as text generation, machine translation, question answering, and more.\n",
      "Engineering Answer: Some specific large language models include GPT-3 by OpenAI, Chinchilla by DeepMind, and BERT by Google. OpenAI developed GPT-3, DeepMind developed Chinchilla, and Google developed BERT. These models have been significant advancements in the field of natural language processing.\n",
      "\n",
      "Marketing Prediction: Some prominent examples of large language models and their developers include: \n",
      "\n",
      "- GPT-3 (Generative Pre-trained Transformer 3) by OpenAI\n",
      "- BERT (Bidirectional Encoder Representations from Transformers) by Google\n",
      "- T5 (Text-to-Text Transfer Transformer) by Google AI\n",
      "- RoBERTa (Robustly Optimized BERT Approach) by Facebook AI Research\n",
      "- Megatron-LM by NVIDIA and Microsoft\n",
      "- BART (Bidirectional and Auto-Regressive Transformers) by Facebook AI Research\n",
      "- XLNet by Carnegie Mellon University and Microsoft Research\n",
      "\n",
      "These organizations have played a significant role in developing and advancing the field of large language models, each with its unique architecture, training techniques, and capabilities.\n",
      "Marketing Answer: Chinchilla by DeepMind, GPT-3 by OpenAI.\n",
      "Before LLM: messages=[HumanMessage(content='[INST]\\n              Please provide an precise and concise answer to the question below based on the context information provided.\\n\\n\\n              Below is a context:\\nBaptiste Rozière, Naman Goyal, Eric Hambro,\\nFaisal Azhar, et al. 2023. Llama: Open and effi-\\ncient foundation language models. arXiv preprint\\narXiv:2302.13971.\\nAndrew Trotman, Antti Puurula, and Blake Burgess.\\n\\nit becomes challenging to limit harmful applications in these and other domains without proper\\nregulation. On the other hand, if large language model access is restricted to a few organizations\\n\\nthe fast adoption of LLMs.\\n1\\nIntroduction\\nLanguage Models (LMs) capture a vast amount\\nof knowledge about the world, which allows them\\nto answer questions without accessing any exter-\\nnal sources. This idea of LMs as repositories of\\n\\nour current language model systems, we seek general and scalable methods that work for future AI\\nsystems (Leike et al., 2018). The systems we work with here are still fairly limited, but they are\\n\\nbeen extensively documented (Bender et al., 2021; Bommasani et al., 2021; Kenton et al., 2021;\\nWeidinger et al., 2021; Tamkin et al., 2021). Language models can produce biased outputs (Dhamala\\n\\n2019), or causal mediation analysis (Vig et al., 2020). There is also work on steering the generation\\nof language models using a second (usually smaller) language model (Dathathri et al., 2019; Krause\\n\\n              Below is a question:\\nWhat licensing models have been adopted for the distribution of source-available language models?\\n[/INST]\\n              ')]\n",
      "Before LLM: messages=[HumanMessage(content='[INST]\\n              Please provide an precise and concise answer to the question below based on the context information provided.\\n\\n\\n              Below is a context:\\nBaptiste Rozière, Naman Goyal, Eric Hambro,\\nFaisal Azhar, et al. 2023. Llama: Open and effi-\\ncient foundation language models. arXiv preprint\\narXiv:2302.13971.\\nAndrew Trotman, Antti Puurula, and Blake Burgess.\\n\\nit becomes challenging to limit harmful applications in these and other domains without proper\\nregulation. On the other hand, if large language model access is restricted to a few organizations\\n\\nthe fast adoption of LLMs.\\n1\\nIntroduction\\nLanguage Models (LMs) capture a vast amount\\nof knowledge about the world, which allows them\\nto answer questions without accessing any exter-\\nnal sources. This idea of LMs as repositories of\\n\\nour current language model systems, we seek general and scalable methods that work for future AI\\nsystems (Leike et al., 2018). The systems we work with here are still fairly limited, but they are\\n\\nbeen extensively documented (Bender et al., 2021; Bommasani et al., 2021; Kenton et al., 2021;\\nWeidinger et al., 2021; Tamkin et al., 2021). Language models can produce biased outputs (Dhamala\\n\\n2019), or causal mediation analysis (Vig et al., 2020). There is also work on steering the generation\\nof language models using a second (usually smaller) language model (Dathathri et al., 2019; Krause\\n\\n              Below is a question:\\nWhat licensing models have been adopted for the distribution of source-available language models?\\n[/INST]\\n              ')]\n",
      "------------------------------------------------------------\n",
      "Question: What licensing models have been adopted for the distribution of source-available language models?\n",
      " \n",
      "Engineering Prediction: I'm sorry, but the provided context does not include any information about licensing models for the distribution of source-available language models. The context discusses the challenges and potential risks associated with large language models and the need for proper regulation, but it does not mention specific licensing models or distribution methods.\n",
      "Engineering Answer: Based on the provided context, it seems that licensing models for the distribution of source-available language models have not been explicitly discussed in the referenced papers. However, it is crucial to consider potential licensing options such as open-source licenses (e.g., GPL, MIT) or proprietary licenses when distributing language models to ensure legal compliance and control over usage rights. Additionally, considering the implications of different licensing models on accessibility, collaboration, and commercialization is essential for determining the most suitable approach for sharing language models with the community. Further research or consultation with legal experts may be necessary to explore specific licensing strategies for source-available language models.\n",
      "\n",
      "Marketing Prediction: I apologize, but the provided context does not seem to include any information regarding the licensing models adopted for the distribution of source-available language models. Is there any other information you would like me to extract from the context?\n",
      "Marketing Answer: Answer: Some organizations choose open-sourcing, while others restrict access to a few organizations with resources or offer end-to-end deployment via API.\n",
      "Before LLM: messages=[HumanMessage(content='[INST]\\n              Please provide an precise and concise answer to the question below based on the context information provided.\\n\\n\\n              Below is a context:\\nthe purpose of our language tasks, we use a framework similar to Askell et al. (2021), who deﬁne\\nmodels to be aligned if they are helpful, honest, and harmless.\\n\\ntechniques in natural language processing (NLP) where words or phrases from the vocabulary\\nare mapped to vectors of real numbers. Conceptually it involves a mathematical embedding\\n\\nalgorithm. Machine Learning, 97(3):327–351, July 2014. doi: 10.1007/s10994-014-5458-8.\\nURL https://doi.org/10.1007/s10994-014-5458-8.\\n[10] Y. Chen, R. Wang, H. Jiang, S. Shi, and R.-L. Xu. Exploring the use of large language models\\n\\nGeneral-Purpose Architectures for NLP\\nPrior work on general-purpose architectures for NLP\\ntasks has shown great success without the use of retrieval. A single, pre-trained language model\\n\\nmodels.\\nhttps://github.com/tatsu-lab/\\nalpaca_eval, 2023.\\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,\\nMishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,\\net al. Training language models to follow instructions\\n\\nthem to do what a given set of humans want them to do. By default, language models optimize\\nthe next word prediction objective, which is only a proxy for what we want these models to do.\\n\\n              Below is a question:\\nWhat are language models and what is their purpose in natural language processing?\\n[/INST]\\n              ')]\n",
      "Before LLM: messages=[HumanMessage(content='[INST]\\n              Please provide an precise and concise answer to the question below based on the context information provided.\\n\\n\\n              Below is a context:\\nthe purpose of our language tasks, we use a framework similar to Askell et al. (2021), who deﬁne\\nmodels to be aligned if they are helpful, honest, and harmless.\\n\\ntechniques in natural language processing (NLP) where words or phrases from the vocabulary\\nare mapped to vectors of real numbers. Conceptually it involves a mathematical embedding\\n\\nalgorithm. Machine Learning, 97(3):327–351, July 2014. doi: 10.1007/s10994-014-5458-8.\\nURL https://doi.org/10.1007/s10994-014-5458-8.\\n[10] Y. Chen, R. Wang, H. Jiang, S. Shi, and R.-L. Xu. Exploring the use of large language models\\n\\nGeneral-Purpose Architectures for NLP\\nPrior work on general-purpose architectures for NLP\\ntasks has shown great success without the use of retrieval. A single, pre-trained language model\\n\\nmodels.\\nhttps://github.com/tatsu-lab/\\nalpaca_eval, 2023.\\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,\\nMishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,\\net al. Training language models to follow instructions\\n\\nthem to do what a given set of humans want them to do. By default, language models optimize\\nthe next word prediction objective, which is only a proxy for what we want these models to do.\\n\\n              Below is a question:\\nWhat are language models and what is their purpose in natural language processing?\\n[/INST]\\n              ')]\n",
      "------------------------------------------------------------\n",
      "Question: What are language models and what is their purpose in natural language processing?\n",
      " \n",
      "Engineering Prediction: Language models are a set of techniques in natural language processing (NLP) that map words and phrases to vectors of real numbers. They are designed to optimize the next word prediction objective, which is a proxy for the ultimate goal of creating models that are helpful, honest, and harmless.\n",
      "Engineering Answer: Language models are probabilistic models of natural language that help predict or correct text. Their purpose in natural language processing is to assist in various tasks such as speech recognition, machine translation, natural language generation, and information retrieval. By analyzing the performance of human subjects, language models improve the understanding and generation of human-like text.\n",
      "\n",
      "Marketing Prediction: Language models are a set of techniques in natural language processing (NLP) that map words and phrases to vectors of real numbers. They are designed to optimize the next word prediction objective, which is a proxy for the ultimate goal of creating models that are helpful, honest, and harmless.\n",
      "Marketing Answer: Language models are probabilistic models of natural language that are used in tasks such as speech recognition, machine translation, and natural language generation in natural language processing.\n",
      "Before LLM: messages=[HumanMessage(content='[INST]\\n              Please provide an precise and concise answer to the question below based on the context information provided.\\n\\n\\n              Below is a context:\\n[47] Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu,\\nand Alexander Miller. Language models as knowledge bases? In Proceedings of the 2019\\n\\nmade in the 1930’s set in France”) or when constraints can be challenging for language models (e.g.\\nwriting a summary in a speciﬁed number of sentences).\\n\\n2014).\\nHowever, making language models better at following user intentions also makes them easier to\\nmisuse. It may be easier to use these models to generate convincing misinformation, or hateful or\\nabusive content.\\n\\nWe mostly focus on QA models that contain neural networks, specially Transformer-based language models.\\nI admit that I missed a lot of papers with architectures designed specifically for QA tasks between 2017-2019\\uf8ffüòî\\n\\nlimitations, and societal impact of large language models. arXiv preprint arXiv:2102.02503.\\nThoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng, H.-T., Jin, A., Bos,\\n\\nFraser Kelton\\nLuke Miller\\nMaddie Simens\\nAmanda Askell†\\nPeter Welinder\\nPaul Christiano∗†\\nJan Leike∗\\nRyan Lowe∗\\nOpenAI\\nAbstract\\nMaking language models bigger does not inherently make them better at following\\n\\n              Below is a question:\\nHow have language models evolved in terms of architecture, from the 1980s to present times?\\n[/INST]\\n              ')]\n",
      "Before LLM: messages=[HumanMessage(content='[INST]\\n              Please provide an precise and concise answer to the question below based on the context information provided.\\n\\n\\n              Below is a context:\\n[47] Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu,\\nand Alexander Miller. Language models as knowledge bases? In Proceedings of the 2019\\n\\nmade in the 1930’s set in France”) or when constraints can be challenging for language models (e.g.\\nwriting a summary in a speciﬁed number of sentences).\\n\\n2014).\\nHowever, making language models better at following user intentions also makes them easier to\\nmisuse. It may be easier to use these models to generate convincing misinformation, or hateful or\\nabusive content.\\n\\nWe mostly focus on QA models that contain neural networks, specially Transformer-based language models.\\nI admit that I missed a lot of papers with architectures designed specifically for QA tasks between 2017-2019\\uf8ffüòî\\n\\nlimitations, and societal impact of large language models. arXiv preprint arXiv:2102.02503.\\nThoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng, H.-T., Jin, A., Bos,\\n\\nFraser Kelton\\nLuke Miller\\nMaddie Simens\\nAmanda Askell†\\nPeter Welinder\\nPaul Christiano∗†\\nJan Leike∗\\nRyan Lowe∗\\nOpenAI\\nAbstract\\nMaking language models bigger does not inherently make them better at following\\n\\n              Below is a question:\\nHow have language models evolved in terms of architecture, from the 1980s to present times?\\n[/INST]\\n              ')]\n",
      "------------------------------------------------------------\n",
      "Question: How have language models evolved in terms of architecture, from the 1980s to present times?\n",
      " \n",
      "Engineering Prediction: The evolution of language models from the 1980s to the present has been marked by a shift from simpler architectures to more complex ones, with a focus on improving their ability to understand and generate human language. \n",
      "\n",
      "Early language models in the 1980s were based on statistical methods and rule-based systems, which had limited context understanding and generated stiff, unnatural language. \n",
      "\n",
      "The introduction of neural networks and, more specifically, Transformer-based architectures (introduced in the context in 2017) marked a significant shift in the field. These models, including QA models, can process large amounts of data and have a deeper understanding of context, allowing them to generate more human-like responses and handle complex tasks such as question answering and summarization. \n",
      "\n",
      "The focus on making language models better at following user intentions and constraints (e.g., generating content within specific guidelines) further refines their output, but also introduces potential misuse concerns, as mentioned in the context. \n",
      "\n",
      "In summary, language models have evolved from simple statistical methods to complex neural network architectures, improving their understanding of context and generating more human-like responses, but also presenting new challenges and ethical considerations.\n",
      "Engineering Answer: Language models have evolved significantly in terms of architecture from the 1980s to present times. In the 1980s, the first statistical language model was proposed, leading to experiments by IBM that identified areas for improvement by observing human subjects. However, it wasn't until 2017 when the transformer architecture was introduced by Google, revolutionizing the field. This development paved the way for models like BERT in 2018, which marked a shift towards large-scale transformer-based language models. These modern architectures, based on self-attention mechanisms, have dominated the field of natural language processing, achieving state-of-the-art performance in various tasks.\n",
      "\n",
      "Marketing Prediction: The evolution of language models from the 1980s to the present has been marked by a shift from simpler architectures to more complex ones, with a focus on improving their ability to understand and generate human language. In the 1980s, language models were primarily based on statistical methods and rule-based systems. With the rise of machine learning and deep learning, language models started to incorporate neural network architectures in the 2000s. The Transformer architecture, introduced in 2017, revolutionized language modeling by allowing models to capture long-range dependencies in text and generate more contextually relevant responses. Since then, language models have continued to grow in size and complexity, with a focus on improving their ability to understand and generate human language, while also addressing challenges such as misinformation and bias.\n",
      "Marketing Answer: Language models have evolved from early statistical models in the 1980s to modern transformer architectures, such as BERT and GPT-2, which use self-attention mechanisms and have become dominant in natural language processing tasks.\n",
      "Before LLM: messages=[HumanMessage(content='[INST]\\n              Please provide an precise and concise answer to the question below based on the context information provided.\\n\\n\\n              Below is a context:\\nP Pi = 1 and each Pi is proportional to the total probability the model assigns to that completion.\\nMaximum entropy for binary choices is 1. High entropy indicates that the model is unsure of which\\n\\n3.1\\nFormulation\\nTransformer-based language modeling usually leverages the position information of individual tokens through a self-\\nattention mechanism. As can be observed in Equation (2), q⊺\\nmkn typically enables knowledge conveyance between\\n\\ncomplete derivation. Even if we use the MLE estimate rϕ of the ground-truth reward function r∗, it is\\nstill expensive to estimate the partition function Z(x) [17, 15], which makes this representation hard\\n\\ninvestigations of the tradeoffs of simple cross-entropy loss and RLHF training. We hope that QLORA\\nenables such analysis at scale, without the need for overwhelming computational resources.\\n7\\nRelated Work\\nQuantization of Large Language Models\\n\\nDuring full ﬁne-tuning, the model is initialized to pre-trained weights Φ0 and updated to Φ0 + ∆Φ\\nby repeatedly following the gradient to maximize the conditional language modeling objective:\\nmax\\nΦ\\nX\\n(x,y)∈Z\\n|y|\\nX\\nt=1\\nlog (PΦ(yt|x, y<t))\\n(1)\\n\\ncations. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\\n(Tutorial), 2023a. URL https://aclanthology.org/2023.acl-tutorials.6.\\n\\n              Below is a question:\\nCan you explain how maximum entropy language models work and what the partition function signifies?\\n[/INST]\\n              ')]\n",
      "Before LLM: messages=[HumanMessage(content='[INST]\\n              Please provide an precise and concise answer to the question below based on the context information provided.\\n\\n\\n              Below is a context:\\nP Pi = 1 and each Pi is proportional to the total probability the model assigns to that completion.\\nMaximum entropy for binary choices is 1. High entropy indicates that the model is unsure of which\\n\\n3.1\\nFormulation\\nTransformer-based language modeling usually leverages the position information of individual tokens through a self-\\nattention mechanism. As can be observed in Equation (2), q⊺\\nmkn typically enables knowledge conveyance between\\n\\ncomplete derivation. Even if we use the MLE estimate rϕ of the ground-truth reward function r∗, it is\\nstill expensive to estimate the partition function Z(x) [17, 15], which makes this representation hard\\n\\ninvestigations of the tradeoffs of simple cross-entropy loss and RLHF training. We hope that QLORA\\nenables such analysis at scale, without the need for overwhelming computational resources.\\n7\\nRelated Work\\nQuantization of Large Language Models\\n\\nDuring full ﬁne-tuning, the model is initialized to pre-trained weights Φ0 and updated to Φ0 + ∆Φ\\nby repeatedly following the gradient to maximize the conditional language modeling objective:\\nmax\\nΦ\\nX\\n(x,y)∈Z\\n|y|\\nX\\nt=1\\nlog (PΦ(yt|x, y<t))\\n(1)\\n\\ncations. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\\n(Tutorial), 2023a. URL https://aclanthology.org/2023.acl-tutorials.6.\\n\\n              Below is a question:\\nCan you explain how maximum entropy language models work and what the partition function signifies?\\n[/INST]\\n              ')]\n",
      "------------------------------------------------------------\n",
      "Question: Can you explain how maximum entropy language models work and what the partition function signifies?\n",
      " \n",
      "Engineering Prediction: In the context of language modeling, maximum entropy models aim to capture the uncertainty in the data by assigning probabilities to different possible completions of a given sequence. The maximum entropy for binary choices is 1, indicating the highest uncertainty when the model is unsure of the correct choice.\n",
      "\n",
      "The partition function, Z(x), in this context, is a normalization factor that ensures the probabilities assigned by the model sum up to 1. It plays a crucial role in calculating the probability distribution over all possible completions. However, estimating Z(x) can be computationally expensive, especially for large language models.\n",
      "\n",
      "By using QLORA, which is a quantized version of a language model, researchers can investigate the trade-offs of different training methods, such as cross-entropy loss and RLHF, without requiring extensive computational resources. This allows for a more efficient analysis of the language model's behavior and performance.\n",
      "Engineering Answer: Maximum entropy language models use feature functions to encode the relationship between a word and its n-gram history, aiming to maximize reward while satisfying a KL-constrained objective. The partition function, denoted as Z(x), is crucial in normalizing the probabilities of all possible outputs given the input. It represents the sum of the exponential of the reward function over all possible output sequences, making it computationally expensive to estimate but essential for accurate modeling. The partition function ensures that the model's predicted probabilities sum up to 1, providing a foundation for effective language modeling.\n",
      "\n",
      "Marketing Prediction: In the context of language modeling, maximum entropy models aim to capture the uncertainty in the data by assigning probabilities to different possible completions of a given sequence. The maximum entropy for binary choices is 1, indicating the highest uncertainty when the model is unsure of the correct choice.\n",
      "\n",
      "The partition function, Z(x), in this context, is a normalization factor that ensures the probabilities assigned by the model sum up to 1. It plays a crucial role in calculating the probability distribution over all possible completions. However, estimating Z(x) can be computationally expensive, especially for large language models.\n",
      "\n",
      "By using QLORA, which is a quantized version of a language model, researchers can investigate the trade-offs of different training methods, such as cross-entropy loss and RLHF, without requiring extensive computational resources. This allows for a more efficient analysis of the language model's behavior and performance.\n",
      "Marketing Answer: Maximum entropy language models encode the relationship between a word and the n-gram history using feature functions. The partition function in this context represents the total probability of all possible outcomes, making it a crucial factor in determining the optimal solution for the reward maximization objective.\n",
      "Before LLM: messages=[HumanMessage(content='[INST]\\n              Please provide an precise and concise answer to the question below based on the context information provided.\\n\\n\\n              Below is a context:\\nfrom a space with one dimension per word to a continuous vector space with a much lower\\ndimension. Output: Word embeddings are useful for tasks such as sentiment analysis, text\\n\\nInternational Conference on Learning Representations, 2023. URL https://openreview.\\nnet/forum?id=8aHzds2uUyB.\\n[33] M. Ranzato, S. Chopra, M. Auli, and W. Zaremba. Sequence level training with recurrent neural\\nnetworks. CoRR, abs/1511.06732, 2015.\\n\\nencodings.\\n2. Position Interpolation generates strong models that can effectively make use of much ex-\\ntended context window. We show that models extended by Position Interpolation enjoy\\n\\nthem to do what a given set of humans want them to do. By default, language models optimize\\nthe next word prediction objective, which is only a proxy for what we want these models to do.\\n\\nThe external memory can alleviate the restriction of finite attention span.  A standard practice is to save the embedding representation of information into a vector store database that can support fast maximum inner-product search (MIPS). To optimize the\\n\\nwindow is 10\\nUninfor-\\nmative\\n20.0%\\nWhat do they mean by intrinsic\\ngeometry of spaces of learned\\nrepresentations?\\n“the inferred embedding space\\ncreates a globally consistent\\nstructured prediction of the\\nontology, rather than local relation\\npredictions”\\n\\n              Below is a question:\\nWhat is the benefit of using continuous space embeddings in recurrent neural network language models?\\n[/INST]\\n              ')]\n",
      "Before LLM: messages=[HumanMessage(content='[INST]\\n              Please provide an precise and concise answer to the question below based on the context information provided.\\n\\n\\n              Below is a context:\\nfrom a space with one dimension per word to a continuous vector space with a much lower\\ndimension. Output: Word embeddings are useful for tasks such as sentiment analysis, text\\n\\nInternational Conference on Learning Representations, 2023. URL https://openreview.\\nnet/forum?id=8aHzds2uUyB.\\n[33] M. Ranzato, S. Chopra, M. Auli, and W. Zaremba. Sequence level training with recurrent neural\\nnetworks. CoRR, abs/1511.06732, 2015.\\n\\nencodings.\\n2. Position Interpolation generates strong models that can effectively make use of much ex-\\ntended context window. We show that models extended by Position Interpolation enjoy\\n\\nthem to do what a given set of humans want them to do. By default, language models optimize\\nthe next word prediction objective, which is only a proxy for what we want these models to do.\\n\\nThe external memory can alleviate the restriction of finite attention span.  A standard practice is to save the embedding representation of information into a vector store database that can support fast maximum inner-product search (MIPS). To optimize the\\n\\nwindow is 10\\nUninfor-\\nmative\\n20.0%\\nWhat do they mean by intrinsic\\ngeometry of spaces of learned\\nrepresentations?\\n“the inferred embedding space\\ncreates a globally consistent\\nstructured prediction of the\\nontology, rather than local relation\\npredictions”\\n\\n              Below is a question:\\nWhat is the benefit of using continuous space embeddings in recurrent neural network language models?\\n[/INST]\\n              ')]\n",
      "------------------------------------------------------------\n",
      "Question: What is the benefit of using continuous space embeddings in recurrent neural network language models?\n",
      " \n",
      "Engineering Prediction: The benefit of using continuous space embeddings in recurrent neural network language models is to capture the semantic relationships between words and represent them in a lower-dimensional space. This allows for more efficient computation and can improve the performance of downstream tasks such as sentiment analysis.\n",
      "Engineering Answer: Continuous space embeddings in recurrent neural network language models help alleviate the curse of dimensionality by representing words as non-linear combinations of weights in the embedding space. This approach helps address the data sparsity problem caused by the exponential increase in possible word sequences with vocabulary size. By utilizing continuous space embeddings, neural networks can effectively capture semantic relationships and meaning within the language model.\n",
      "\n",
      "Marketing Prediction: The benefit of using continuous space embeddings in recurrent neural network language models is to represent words in a lower-dimensional space, allowing for more efficient computation and the ability to capture semantic relationships between words. This helps in tasks such as sentiment analysis and text generation by providing a more meaningful representation of words and their contexts.\n",
      "Marketing Answer: Continuous space embeddings in recurrent neural network language models help alleviate the curse of dimensionality caused by the exponential increase in possible word sequences, reducing data sparsity issues.\n",
      "Before LLM: messages=[HumanMessage(content='[INST]\\n              Please provide an precise and concise answer to the question below based on the context information provided.\\n\\n\\n              Below is a context:\\nbarrier to training more language models from human preferences.\\nLimitations & Future Work. Our results raise several important questions for future work. How\\n\\n[17] Zhang et al. “Automatic chain of thought prompting in large language models.” arXiv preprint arXiv:2210.03493 (2022).\\n\\n(non-retrieval augmented) large language models\\nindeed suffer from hallucination, whereas our best\\nmodels substantially curtail the issue, reducing\\nhallucinated responses by over 60%. We show\\nthat this effect is even more pronounced on out-\\n\\nGarriga-Alonso, A., et al. Beyond the imitation game:\\nQuantifying and extrapolating the capabilities of language\\nmodels. arXiv preprint arXiv:2206.04615, 2022.\\nStiennon, N., Ouyang, L., Wu, J., Ziegler, D., Lowe, R.,\\n\\nwide range of instructions. There are many open questions to explore to further align language model\\nbehavior with what people actually want them to do.\\nMany methods could be tried to further decrease the models’ propensity to generate toxic, biased,\\n\\namong the largest language models today and we apply them on a wide range of language tasks,\\nincluding classiﬁcation, summarization, question-answering, creative writing, dialogue, and others.\\n\\n              Below is a question:\\nWhat challenges do large language models face in mirroring human cognitive patterns?\\n[/INST]\\n              ')]\n",
      "Before LLM: messages=[HumanMessage(content='[INST]\\n              Please provide an precise and concise answer to the question below based on the context information provided.\\n\\n\\n              Below is a context:\\nbarrier to training more language models from human preferences.\\nLimitations & Future Work. Our results raise several important questions for future work. How\\n\\n[17] Zhang et al. “Automatic chain of thought prompting in large language models.” arXiv preprint arXiv:2210.03493 (2022).\\n\\n(non-retrieval augmented) large language models\\nindeed suffer from hallucination, whereas our best\\nmodels substantially curtail the issue, reducing\\nhallucinated responses by over 60%. We show\\nthat this effect is even more pronounced on out-\\n\\nGarriga-Alonso, A., et al. Beyond the imitation game:\\nQuantifying and extrapolating the capabilities of language\\nmodels. arXiv preprint arXiv:2206.04615, 2022.\\nStiennon, N., Ouyang, L., Wu, J., Ziegler, D., Lowe, R.,\\n\\nwide range of instructions. There are many open questions to explore to further align language model\\nbehavior with what people actually want them to do.\\nMany methods could be tried to further decrease the models’ propensity to generate toxic, biased,\\n\\namong the largest language models today and we apply them on a wide range of language tasks,\\nincluding classiﬁcation, summarization, question-answering, creative writing, dialogue, and others.\\n\\n              Below is a question:\\nWhat challenges do large language models face in mirroring human cognitive patterns?\\n[/INST]\\n              ')]\n",
      "------------------------------------------------------------\n",
      "Question: What challenges do large language models face in mirroring human cognitive patterns?\n",
      " \n",
      "Engineering Prediction: Large language models face challenges in mirroring human cognitive patterns due to their propensity to generate toxic and biased responses, as well as their tendency to hallucinate answers. These issues need to be addressed to better align language model behavior with human expectations.\n",
      "Engineering Answer: Large language models face challenges in mirroring human cognitive patterns because they sometimes learn patterns that humans do not learn, while also failing to learn patterns that humans typically learn. This discrepancy suggests that the models may not be plausible cognitive models, despite matching human performance in some tasks. Further research is needed to address these limitations and improve the alignment of large language models with human cognitive patterns.\n",
      "\n",
      "Marketing Prediction: Large language models face challenges in mirroring human cognitive patterns due to their propensity to generate toxic and biased responses, as well as their tendency to hallucinate answers. These issues need to be addressed to better align language model behavior with human expectations.\n",
      "Marketing Answer: Large language models sometimes learn patterns that humans do not learn and fail to learn patterns that humans typically do learn.\n"
     ]
    }
   ],
   "source": [
    "# using function to capture all metrics\n",
    "metrics = ['rouge', 'bleu', 'bertscore']\n",
    "impr_base_prompt = evaluate(metrics, validation_questions_answers, rag_chain, iterations=10, verbose=False, sleep=False, print_results=True)\n",
    "impr_base_prompt = composite_evaluation(impr_base_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "hCAHaLv4dmZF",
    "outputId": "419ddef1-c029-4983-fddc-fbb4f1883811"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"impr_base_prompt\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"Sample\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.373295374522416,\n        \"min\": 0.0,\n        \"max\": 13.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          6.6,\n          7.5,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eng_bleu\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.507498698017842,\n        \"min\": 0.03641928055502546,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.08530142118328836,\n          0.0766556670353764,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mk_bleu\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.5153478688182687,\n        \"min\": 0.0,\n        \"max\": 10.0,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          10.0,\n          0.05698058577725189,\n          0.11228900094279323\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eng_rouge\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.450940719779549,\n        \"min\": 0.030955421292633335,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.2771495202214796,\n          0.2809868597479217,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mk_rouge\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.4636238142018327,\n        \"min\": 0.0625,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.22630963874632637,\n          0.22556390977443608,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eng_f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.2830034383871314,\n        \"min\": 0.0209368893913612,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.875678938627243,\n          0.8721521496772766,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mk_f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.2848884996364753,\n        \"min\": 0.02489256893296184,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.8692435622215271,\n          0.8738706409931183,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"composite_eng\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.376248898022407,\n        \"min\": 0.020510762570283503,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.5380446096167231,\n          0.5356894275748154,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"composite_mk\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.381248656179004,\n        \"min\": 0.053888081978914454,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.5139107898901119,\n          0.5124733801512447,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"composite_total\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.3790138750081087,\n        \"min\": 0.024182229988266746,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.5283910817260785,\n          0.5310472967709094,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-78df8ea1-7ce2-49f2-b93a-48bff3ee1b2b\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample</th>\n",
       "      <th>eng_bleu</th>\n",
       "      <th>mk_bleu</th>\n",
       "      <th>eng_rouge</th>\n",
       "      <th>mk_rouge</th>\n",
       "      <th>eng_f1</th>\n",
       "      <th>mk_f1</th>\n",
       "      <th>composite_eng</th>\n",
       "      <th>composite_mk</th>\n",
       "      <th>composite_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.600000</td>\n",
       "      <td>0.085301</td>\n",
       "      <td>0.056981</td>\n",
       "      <td>0.277150</td>\n",
       "      <td>0.226310</td>\n",
       "      <td>0.875679</td>\n",
       "      <td>0.869244</td>\n",
       "      <td>0.538045</td>\n",
       "      <td>0.513911</td>\n",
       "      <td>0.528391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.788876</td>\n",
       "      <td>0.037857</td>\n",
       "      <td>0.056170</td>\n",
       "      <td>0.030955</td>\n",
       "      <td>0.110159</td>\n",
       "      <td>0.020937</td>\n",
       "      <td>0.024893</td>\n",
       "      <td>0.020511</td>\n",
       "      <td>0.053888</td>\n",
       "      <td>0.024182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036419</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.231293</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.840178</td>\n",
       "      <td>0.812753</td>\n",
       "      <td>0.507959</td>\n",
       "      <td>0.435503</td>\n",
       "      <td>0.494234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.250000</td>\n",
       "      <td>0.062209</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.254820</td>\n",
       "      <td>0.155309</td>\n",
       "      <td>0.862992</td>\n",
       "      <td>0.862882</td>\n",
       "      <td>0.526855</td>\n",
       "      <td>0.480649</td>\n",
       "      <td>0.508022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.500000</td>\n",
       "      <td>0.076656</td>\n",
       "      <td>0.054083</td>\n",
       "      <td>0.280987</td>\n",
       "      <td>0.225564</td>\n",
       "      <td>0.872152</td>\n",
       "      <td>0.873871</td>\n",
       "      <td>0.535689</td>\n",
       "      <td>0.512473</td>\n",
       "      <td>0.531047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>10.500000</td>\n",
       "      <td>0.118162</td>\n",
       "      <td>0.112289</td>\n",
       "      <td>0.285273</td>\n",
       "      <td>0.279781</td>\n",
       "      <td>0.886593</td>\n",
       "      <td>0.884759</td>\n",
       "      <td>0.554789</td>\n",
       "      <td>0.549099</td>\n",
       "      <td>0.549279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>0.142238</td>\n",
       "      <td>0.125896</td>\n",
       "      <td>0.338462</td>\n",
       "      <td>0.394366</td>\n",
       "      <td>0.912465</td>\n",
       "      <td>0.898166</td>\n",
       "      <td>0.567109</td>\n",
       "      <td>0.591089</td>\n",
       "      <td>0.556010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-78df8ea1-7ce2-49f2-b93a-48bff3ee1b2b')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-78df8ea1-7ce2-49f2-b93a-48bff3ee1b2b button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-78df8ea1-7ce2-49f2-b93a-48bff3ee1b2b');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-32d155e7-5add-4fa0-bd12-9e07055ff573\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-32d155e7-5add-4fa0-bd12-9e07055ff573')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-32d155e7-5add-4fa0-bd12-9e07055ff573 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "          Sample   eng_bleu    mk_bleu  eng_rouge   mk_rouge     eng_f1  \\\n",
       "count  10.000000  10.000000  10.000000  10.000000  10.000000  10.000000   \n",
       "mean    6.600000   0.085301   0.056981   0.277150   0.226310   0.875679   \n",
       "std     4.788876   0.037857   0.056170   0.030955   0.110159   0.020937   \n",
       "min     0.000000   0.036419   0.000000   0.231293   0.062500   0.840178   \n",
       "25%     2.250000   0.062209   0.000000   0.254820   0.155309   0.862992   \n",
       "50%     7.500000   0.076656   0.054083   0.280987   0.225564   0.872152   \n",
       "75%    10.500000   0.118162   0.112289   0.285273   0.279781   0.886593   \n",
       "max    13.000000   0.142238   0.125896   0.338462   0.394366   0.912465   \n",
       "\n",
       "           mk_f1  composite_eng  composite_mk  composite_total  \n",
       "count  10.000000      10.000000     10.000000        10.000000  \n",
       "mean    0.869244       0.538045      0.513911         0.528391  \n",
       "std     0.024893       0.020511      0.053888         0.024182  \n",
       "min     0.812753       0.507959      0.435503         0.494234  \n",
       "25%     0.862882       0.526855      0.480649         0.508022  \n",
       "50%     0.873871       0.535689      0.512473         0.531047  \n",
       "75%     0.884759       0.554789      0.549099         0.549279  \n",
       "max     0.898166       0.567109      0.591089         0.556010  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "impr_base_prompt.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "plUtRTFd3xBh"
   },
   "source": [
    "Our composite score has increase about 1% with these slight modifications.  A quick review of the output answers does highlight a few things that need to be addressed:\n",
    " - Some outputs are in bullet points, but the target answers are all in sentences.\n",
    " - The way that our output handles missing information is different than the target answers.\n",
    " - Some answers are separated into multiple paragraphs, but should be singular.\n",
    "\n",
    "**Second Pass:** Now we will update the prompt to address some of the above issues with the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uJjBWFyeOxHa",
    "outputId": "ab0feea1-8b68-427e-a9e3-f71b9c71fbea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST]\n",
      "              Please provide an precise and concise answer to the question below based on the context information provided.\n",
      "\n",
      "\n",
      "              Below is a context:\n",
      "{context}\n",
      "\n",
      "              Below is a question:\n",
      "{question}\n",
      "\n",
      "              Below are answer instructions:\n",
      "- Keep your answer to one paragraph.\n",
      "- Answer with complete sentences or phrases. Do not use bullet points. \n",
      "[/INST]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rag_template = \"\"\"[INST]\n",
    "              Please provide an precise and concise answer to the question below based on the context information provided.\\n\\n\n",
    "              Below is a context:\\n{context}\\n\n",
    "              Below is a question:\\n{question}\\n\n",
    "              Below are answer instructions:\n",
    "- Keep your answer to one paragraph.\n",
    "- Answer with complete sentences or phrases. Do not use bullet points.\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "rag_chain = build_RAG_prompt_chain(rag_template, llm_model, retriever, format_docs)\n",
    "print(rag_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e7gj5NDUPTn0",
    "outputId": "9b163974-81e5-481f-bf07-44257f79e24b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before LLM: messages=[HumanMessage(content='[INST]\\n              Please provide an precise and concise answer to the question below based on the context information provided.\\n\\n\\n              Below is a context:\\nalgorithm. Machine Learning, 97(3):327–351, July 2014. doi: 10.1007/s10994-014-5458-8.\\nURL https://doi.org/10.1007/s10994-014-5458-8.\\n[10] Y. Chen, R. Wang, H. Jiang, S. Shi, and R.-L. Xu. Exploring the use of large language models\\n\\nmisuse, there are many domains where large language models should be deployed only with great\\ncare, or not at all. Examples include high-stakes domains such as medical diagnoses, classifying\\n\\nAfter the success of many large-scale general language models, many QA models embrace the following approach:\\n\\nthem to do what a given set of humans want them to do. By default, language models optimize\\nthe next word prediction objective, which is only a proxy for what we want these models to do.\\n\\nMany applications in natural language processing rely on adapt-\\ning one large-scale, pre-trained language model to multiple down-\\nstream applications. Such adaptation is usually done via ﬁne-tuning,\\n\\namong the largest language models today and we apply them on a wide range of language tasks,\\nincluding classiﬁcation, summarization, question-answering, creative writing, dialogue, and others.\\n\\n              Below is a question:\\nWhat purpose do large language models serve in the field of natural language processing?\\n\\n              Below are answer instructions:\\n- Keep your answer to one paragraph.\\n- Answer with complete sentences or phrases. Do not use bullet points. \\n[/INST]\\n')]\n",
      "Before LLM: messages=[HumanMessage(content='[INST]\\n              Please provide an precise and concise answer to the question below based on the context information provided.\\n\\n\\n              Below is a context:\\nalgorithm. Machine Learning, 97(3):327–351, July 2014. doi: 10.1007/s10994-014-5458-8.\\nURL https://doi.org/10.1007/s10994-014-5458-8.\\n[10] Y. Chen, R. Wang, H. Jiang, S. Shi, and R.-L. Xu. Exploring the use of large language models\\n\\nmisuse, there are many domains where large language models should be deployed only with great\\ncare, or not at all. Examples include high-stakes domains such as medical diagnoses, classifying\\n\\nAfter the success of many large-scale general language models, many QA models embrace the following approach:\\n\\nthem to do what a given set of humans want them to do. By default, language models optimize\\nthe next word prediction objective, which is only a proxy for what we want these models to do.\\n\\nMany applications in natural language processing rely on adapt-\\ning one large-scale, pre-trained language model to multiple down-\\nstream applications. Such adaptation is usually done via ﬁne-tuning,\\n\\namong the largest language models today and we apply them on a wide range of language tasks,\\nincluding classiﬁcation, summarization, question-answering, creative writing, dialogue, and others.\\n\\n              Below is a question:\\nWhat purpose do large language models serve in the field of natural language processing?\\n\\n              Below are answer instructions:\\n- Keep your answer to one paragraph.\\n- Answer with complete sentences or phrases. Do not use bullet points. \\n[/INST]\\n')]\n",
      "------------------------------------------------------------\n",
      "Question: What purpose do large language models serve in the field of natural language processing?\n",
      " \n",
      "Engineering Prediction: Large language models serve as the foundation for many natural language processing applications. These models are pre-trained on vast amounts of text data and can be adapted to various downstream tasks through fine-tuning. They excel at predicting the next word in a sequence, which forms the basis for more complex language tasks such as classification, summarization, question-answering, and creative writing. While they have achieved remarkable success, there are concerns about their potential misuse, especially in high-stakes domains like medical diagnosis, where their deployment requires careful consideration.\n",
      "Engineering Answer: Large language models (LLMs) serve the purpose of enabling general-purpose language generation and other natural language processing tasks such as classification. They achieve this by learning statistical relationships from text documents during computationally intensive self-supervised and semi-supervised training. LLMs can be used for text generation by predicting the next token or word, making them valuable for tasks like speech recognition, machine translation, and information retrieval. Additionally, LLMs have superseded previous models like recurrent neural networks, showcasing their efficiency and effectiveness in NLP tasks.\n",
      "\n",
      "Marketing Prediction: Large language models serve as the foundation for many natural language processing applications. These models are pre-trained on vast amounts of text data and can be adapted to various downstream tasks through fine-tuning. They excel at predicting the next word in a sequence, which forms the basis for more complex language tasks such as classification, summarization, question-answering, and creative writing. While they have achieved remarkable success, their deployment in sensitive domains, such as medical diagnoses, requires careful consideration due to potential ethical implications.\n",
      "Marketing Answer: Large language models serve the purpose of improving performance in various natural language processing tasks, such as speech recognition, machine translation, natural language generation, optical character recognition, handwriting recognition, grammar induction, and information retrieval.\n",
      "Before LLM: messages=[HumanMessage(content='[INST]\\n              Please provide an precise and concise answer to the question below based on the context information provided.\\n\\n\\n              Below is a context:\\n[43] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal,\\nK. Slama, A. Ray, et al. Training language models to follow instructions with human feedback.\\n\\nFig. 13. The amount of computation used for training big language models of different sizes is getting big. (Image source: Brown et al., 2020).\\n\\nconsider it in our experiments.\\nT5\\nThe T5 model (Raffel et al., 2020) proposes\\nanother method of pre-training Transformers for\\ntransfer learning, via converting several language\\ntasks into “text-to-text” tasks. T5 is pre-trained\\n\\nSch¨\\narli, and Denny Zhou. Large language models can be easily distracted by irrelevant context.\\nIn Proceedings of the 40th International Conference on Machine Learning, 2023. URL https:\\n//proceedings.mlr.press/v202/shi23a.html.\\n\\nspeciﬁc downstream tasks that were learned but not emphasized in the general pre-training model.\\n8\\nCONCLUSION AND FUTURE WORK\\nFine-tuning enormous language models is prohibitively expensive in terms of the hardware required\\n\\nusing sequences longer than the pre-training settings for language modeling. At 1000 steps, we can\\nsee the models have improved steadily and achieve a significantly better perplexity.\\nModel\\nEvaluation Context Window Size\\nSize\\nContext Window\\nMethod\\n2048\\n\\n              Below is a question:\\nHow does a large language model learn from text during training?\\n\\n              Below are answer instructions:\\n- Keep your answer to one paragraph.\\n- Answer with complete sentences or phrases. Do not use bullet points. \\n[/INST]\\n')]\n",
      "Before LLM: messages=[HumanMessage(content='[INST]\\n              Please provide an precise and concise answer to the question below based on the context information provided.\\n\\n\\n              Below is a context:\\n[43] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal,\\nK. Slama, A. Ray, et al. Training language models to follow instructions with human feedback.\\n\\nFig. 13. The amount of computation used for training big language models of different sizes is getting big. (Image source: Brown et al., 2020).\\n\\nconsider it in our experiments.\\nT5\\nThe T5 model (Raffel et al., 2020) proposes\\nanother method of pre-training Transformers for\\ntransfer learning, via converting several language\\ntasks into “text-to-text” tasks. T5 is pre-trained\\n\\nSch¨\\narli, and Denny Zhou. Large language models can be easily distracted by irrelevant context.\\nIn Proceedings of the 40th International Conference on Machine Learning, 2023. URL https:\\n//proceedings.mlr.press/v202/shi23a.html.\\n\\nspeciﬁc downstream tasks that were learned but not emphasized in the general pre-training model.\\n8\\nCONCLUSION AND FUTURE WORK\\nFine-tuning enormous language models is prohibitively expensive in terms of the hardware required\\n\\nusing sequences longer than the pre-training settings for language modeling. At 1000 steps, we can\\nsee the models have improved steadily and achieve a significantly better perplexity.\\nModel\\nEvaluation Context Window Size\\nSize\\nContext Window\\nMethod\\n2048\\n\\n              Below is a question:\\nHow does a large language model learn from text during training?\\n\\n              Below are answer instructions:\\n- Keep your answer to one paragraph.\\n- Answer with complete sentences or phrases. Do not use bullet points. \\n[/INST]\\n')]\n",
      "------------------------------------------------------------\n",
      "Question: How does a large language model learn from text during training?\n",
      " \n",
      "Engineering Prediction: Large language models learn by analyzing vast amounts of text data during the training process. This involves using machine learning techniques to identify patterns, relationships, and context within the text. The model can then generate responses or perform tasks by utilizing this learned knowledge. During training, these models can be fine-tuned for specific tasks, such as text generation or question answering, by adjusting their parameters based on feedback from human evaluators. This allows them to improve their performance and better understand the nuances of human language.\n",
      "Engineering Answer: A large language model learns from text during training by first going through an unsupervised generative 'pretraining' stage where it sets initial parameters using a language modeling objective. Then, it goes through a supervised discriminative 'fine-tuning' stage where it refines its parameters based on annotated examples or task demonstrations. This dual-stage approach allows the model to learn statistical relationships from text documents in a computationally intensive process, enabling it to achieve general-purpose language generation and natural language processing tasks.\n",
      "\n",
      "Marketing Prediction: Large language models learn by analyzing vast amounts of text data through a process called \"pre-training.\" This involves training the model on a general language understanding task, such as predicting the next word in a sequence, which helps it grasp language structure and semantics. One example is the T5 model, which converts various language tasks into \"text-to-text\" tasks for pre-training, enabling effective transfer learning. However, fine-tuning these models for specific tasks can be costly due to hardware requirements. To address this, techniques like increasing the context window size during evaluation are employed, allowing models to consider longer sequences and improve performance, as seen in the provided context.\n",
      "Marketing Answer: A large language model learns from text during training by first pretraining on a diverse dataset to acquire general language knowledge, and then fine-tuning on specific tasks or demonstrations to adapt its parameters for more targeted performance.\n",
      "Before LLM: messages=[HumanMessage(content='[INST]\\n              Please provide an precise and concise answer to the question below based on the context information provided.\\n\\n\\n              Below is a context:\\nSch¨\\narli, and Denny Zhou. Large language models can be easily distracted by irrelevant context.\\nIn Proceedings of the 40th International Conference on Machine Learning, 2023. URL https:\\n//proceedings.mlr.press/v202/shi23a.html.\\n\\nWe mostly focus on QA models that contain neural networks, specially Transformer-based language models.\\nI admit that I missed a lot of papers with architectures designed specifically for QA tasks between 2017-2019\\uf8ffüòî\\n\\nour current language model systems, we seek general and scalable methods that work for future AI\\nsystems (Leike et al., 2018). The systems we work with here are still fairly limited, but they are\\n\\nAfter the success of many large-scale general language models, many QA models embrace the following approach:\\n\\nere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Ar-\\nmand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation\\nlanguage models, 2023.\\n\\nShishir G Patil, Tianjun Zhang, Xin Wang, and\\nJoseph E Gonzalez. 2023. Gorilla: Large language\\nmodel connected with massive apis. arXiv preprint\\narXiv:2305.15334.\\nJayr Pereira, Robson Fidalgo, Roberto Lotufo, and Ro-\\n\\n              Below is a question:\\nWhat are some key architectures behind the development of large language models?\\n\\n              Below are answer instructions:\\n- Keep your answer to one paragraph.\\n- Answer with complete sentences or phrases. Do not use bullet points. \\n[/INST]\\n')]\n",
      "Before LLM: messages=[HumanMessage(content='[INST]\\n              Please provide an precise and concise answer to the question below based on the context information provided.\\n\\n\\n              Below is a context:\\nSch¨\\narli, and Denny Zhou. Large language models can be easily distracted by irrelevant context.\\nIn Proceedings of the 40th International Conference on Machine Learning, 2023. URL https:\\n//proceedings.mlr.press/v202/shi23a.html.\\n\\nWe mostly focus on QA models that contain neural networks, specially Transformer-based language models.\\nI admit that I missed a lot of papers with architectures designed specifically for QA tasks between 2017-2019\\uf8ffüòî\\n\\nour current language model systems, we seek general and scalable methods that work for future AI\\nsystems (Leike et al., 2018). The systems we work with here are still fairly limited, but they are\\n\\nAfter the success of many large-scale general language models, many QA models embrace the following approach:\\n\\nere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Ar-\\nmand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation\\nlanguage models, 2023.\\n\\nShishir G Patil, Tianjun Zhang, Xin Wang, and\\nJoseph E Gonzalez. 2023. Gorilla: Large language\\nmodel connected with massive apis. arXiv preprint\\narXiv:2305.15334.\\nJayr Pereira, Robson Fidalgo, Roberto Lotufo, and Ro-\\n\\n              Below is a question:\\nWhat are some key architectures behind the development of large language models?\\n\\n              Below are answer instructions:\\n- Keep your answer to one paragraph.\\n- Answer with complete sentences or phrases. Do not use bullet points. \\n[/INST]\\n')]\n",
      "------------------------------------------------------------\n",
      "Question: What are some key architectures behind the development of large language models?\n",
      " \n",
      "Engineering Prediction: Some key architectures that have facilitated the development of large language models include Transformer-based language models and neural networks. These models, such as the ones mentioned in the context (LLAMA and GORILLA), have embraced a specific approach that utilizes open and efficient foundation language models and connects them with massive APIs. These architectures aim to create general and scalable methods that can be applied to future AI systems, building upon the success of large-scale general language models in QA tasks.\n",
      "Engineering Answer: Key architectures behind the development of large language models include the use of self-attention mechanisms, such as those seen in Transformer decoders. These architectures have been applied to tasks like autoregressive language modeling and have led to the dominance of Transformer-based language models in NLP. Models like BERT and GPT-2 have further advanced this paradigm, showcasing the power of large Transformer language models in achieving state-of-the-art results across various NLP tasks. Additionally, architectures like neural-retriever-in-the-loop generative-based models have shown improvements in tasks like open-domain QA and knowledge-grounded dialogue, emphasizing the importance of consistent and engaging responses in long-form generation and multi-turn conversations.\n",
      "\n",
      "Marketing Prediction: Transformer-based language models, especially those focused on question-answering (QA) tasks, are key to developing large language models. These models embrace a general approach that utilizes open and efficient foundation language models, such as LLaMA, and connects them with massive APIs, as seen in the Gorilla model. This combination of foundation models and external knowledge sources through APIs allows for the creation of large language models that can effectively process and generate human-like language.\n",
      "Marketing Answer: Key architectures behind the development of large language models include Transformer-based models such as BERT and GPT-2, which utilize self-attention mechanisms for tasks like autoregressive language modeling and knowledge-grounded dialogue. These models have shown significant success in NLP tasks and have led to advancements in general-purpose language generation and natural language processing.\n",
      "Before LLM: messages=[HumanMessage(content='[INST]\\n              Please provide an precise and concise answer to the question below based on the context information provided.\\n\\n\\n              Below is a context:\\nlimitations, and societal impact of large language models. arXiv preprint arXiv:2102.02503.\\nThoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng, H.-T., Jin, A., Bos,\\n\\nmodels.\\nhttps://github.com/tatsu-lab/\\nalpaca_eval, 2023.\\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,\\nMishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,\\net al. Training language models to follow instructions\\n\\n2019), or causal mediation analysis (Vig et al., 2020). There is also work on steering the generation\\nof language models using a second (usually smaller) language model (Dathathri et al., 2019; Krause\\n\\nAfter the success of many large-scale general language models, many QA models embrace the following approach:\\n\\n[47] Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu,\\nand Alexander Miller. Language models as knowledge bases? In Proceedings of the 2019\\n\\nFraser Kelton\\nLuke Miller\\nMaddie Simens\\nAmanda Askell†\\nPeter Welinder\\nPaul Christiano∗†\\nJan Leike∗\\nRyan Lowe∗\\nOpenAI\\nAbstract\\nMaking language models bigger does not inherently make them better at following\\n\\n              Below is a question:\\nCan you name some specific large language models and the companies or organizations that have developed them?\\n\\n              Below are answer instructions:\\n- Keep your answer to one paragraph.\\n- Answer with complete sentences or phrases. Do not use bullet points. \\n[/INST]\\n')]\n",
      "Before LLM: messages=[HumanMessage(content='[INST]\\n              Please provide an precise and concise answer to the question below based on the context information provided.\\n\\n\\n              Below is a context:\\nlimitations, and societal impact of large language models. arXiv preprint arXiv:2102.02503.\\nThoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng, H.-T., Jin, A., Bos,\\n\\nmodels.\\nhttps://github.com/tatsu-lab/\\nalpaca_eval, 2023.\\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,\\nMishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,\\net al. Training language models to follow instructions\\n\\n2019), or causal mediation analysis (Vig et al., 2020). There is also work on steering the generation\\nof language models using a second (usually smaller) language model (Dathathri et al., 2019; Krause\\n\\nAfter the success of many large-scale general language models, many QA models embrace the following approach:\\n\\n[47] Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu,\\nand Alexander Miller. Language models as knowledge bases? In Proceedings of the 2019\\n\\nFraser Kelton\\nLuke Miller\\nMaddie Simens\\nAmanda Askell†\\nPeter Welinder\\nPaul Christiano∗†\\nJan Leike∗\\nRyan Lowe∗\\nOpenAI\\nAbstract\\nMaking language models bigger does not inherently make them better at following\\n\\n              Below is a question:\\nCan you name some specific large language models and the companies or organizations that have developed them?\\n\\n              Below are answer instructions:\\n- Keep your answer to one paragraph.\\n- Answer with complete sentences or phrases. Do not use bullet points. \\n[/INST]\\n')]\n",
      "------------------------------------------------------------\n",
      "Question: Can you name some specific large language models and the companies or organizations that have developed them?\n",
      " \n",
      "Engineering Prediction: Some prominent examples of large language models and their developers include GPT-3, developed by OpenAI, and BERT, which was created by Google. Additionally, Facebook AI has introduced the Blender bot, a conversational agent, and DeepMind has released Gopher, a large-scale transformer model. These organizations have played a significant role in the advancement and application of large language models.\n",
      "Engineering Answer: Some specific large language models include GPT-3 by OpenAI, Chinchilla by DeepMind, and BERT by Google. OpenAI developed GPT-3, DeepMind developed Chinchilla, and Google developed BERT. These models have been significant advancements in the field of natural language processing.\n",
      "\n",
      "Marketing Prediction: Some prominent examples of large language models and their developers include GPT-3, developed by OpenAI, and BERT, which was created by Google. Additionally, Facebook AI has introduced the Blender bot, a conversational agent, and DeepMind has released Gopher, a large-scale transformer model. These organizations have played a significant role in the advancement and application of large language models.\n",
      "Marketing Answer: Chinchilla by DeepMind, GPT-3 by OpenAI.\n",
      "Before LLM: messages=[HumanMessage(content='[INST]\\n              Please provide an precise and concise answer to the question below based on the context information provided.\\n\\n\\n              Below is a context:\\nBaptiste Rozière, Naman Goyal, Eric Hambro,\\nFaisal Azhar, et al. 2023. Llama: Open and effi-\\ncient foundation language models. arXiv preprint\\narXiv:2302.13971.\\nAndrew Trotman, Antti Puurula, and Blake Burgess.\\n\\nit becomes challenging to limit harmful applications in these and other domains without proper\\nregulation. On the other hand, if large language model access is restricted to a few organizations\\n\\nthe fast adoption of LLMs.\\n1\\nIntroduction\\nLanguage Models (LMs) capture a vast amount\\nof knowledge about the world, which allows them\\nto answer questions without accessing any exter-\\nnal sources. This idea of LMs as repositories of\\n\\nour current language model systems, we seek general and scalable methods that work for future AI\\nsystems (Leike et al., 2018). The systems we work with here are still fairly limited, but they are\\n\\nbeen extensively documented (Bender et al., 2021; Bommasani et al., 2021; Kenton et al., 2021;\\nWeidinger et al., 2021; Tamkin et al., 2021). Language models can produce biased outputs (Dhamala\\n\\n2019), or causal mediation analysis (Vig et al., 2020). There is also work on steering the generation\\nof language models using a second (usually smaller) language model (Dathathri et al., 2019; Krause\\n\\n              Below is a question:\\nWhat licensing models have been adopted for the distribution of source-available language models?\\n\\n              Below are answer instructions:\\n- Keep your answer to one paragraph.\\n- Answer with complete sentences or phrases. Do not use bullet points. \\n[/INST]\\n')]\n",
      "Before LLM: messages=[HumanMessage(content='[INST]\\n              Please provide an precise and concise answer to the question below based on the context information provided.\\n\\n\\n              Below is a context:\\nBaptiste Rozière, Naman Goyal, Eric Hambro,\\nFaisal Azhar, et al. 2023. Llama: Open and effi-\\ncient foundation language models. arXiv preprint\\narXiv:2302.13971.\\nAndrew Trotman, Antti Puurula, and Blake Burgess.\\n\\nit becomes challenging to limit harmful applications in these and other domains without proper\\nregulation. On the other hand, if large language model access is restricted to a few organizations\\n\\nthe fast adoption of LLMs.\\n1\\nIntroduction\\nLanguage Models (LMs) capture a vast amount\\nof knowledge about the world, which allows them\\nto answer questions without accessing any exter-\\nnal sources. This idea of LMs as repositories of\\n\\nour current language model systems, we seek general and scalable methods that work for future AI\\nsystems (Leike et al., 2018). The systems we work with here are still fairly limited, but they are\\n\\nbeen extensively documented (Bender et al., 2021; Bommasani et al., 2021; Kenton et al., 2021;\\nWeidinger et al., 2021; Tamkin et al., 2021). Language models can produce biased outputs (Dhamala\\n\\n2019), or causal mediation analysis (Vig et al., 2020). There is also work on steering the generation\\nof language models using a second (usually smaller) language model (Dathathri et al., 2019; Krause\\n\\n              Below is a question:\\nWhat licensing models have been adopted for the distribution of source-available language models?\\n\\n              Below are answer instructions:\\n- Keep your answer to one paragraph.\\n- Answer with complete sentences or phrases. Do not use bullet points. \\n[/INST]\\n')]\n",
      "------------------------------------------------------------\n",
      "Question: What licensing models have been adopted for the distribution of source-available language models?\n",
      " \n",
      "Engineering Prediction: The text provided does not explicitly mention any specific licensing models adopted for the distribution of source-available language models. However, it seems that the discussion revolves around the challenges and potential risks associated with the unrestricted access and adoption of large language models (LLMs). The text highlights the need for proper regulation to prevent harmful applications, suggesting that there is a careful consideration of how these models should be distributed and accessed. It is likely that the licensing models for source-available language models would aim to balance between promoting innovation and ensuring ethical usage, with measures in place to address biases and harmful content.\n",
      "Engineering Answer: Based on the provided context, it seems that licensing models for the distribution of source-available language models have not been explicitly discussed in the referenced papers. However, it is crucial to consider potential licensing options such as open-source licenses (e.g., GPL, MIT) or proprietary licenses when distributing language models to ensure legal compliance and control over usage rights. Additionally, considering the implications of different licensing models on accessibility, collaboration, and commercialization is essential for determining the most suitable approach for sharing language models with the community. Further research or consultation with legal experts may be necessary to explore specific licensing strategies for source-available language models.\n",
      "\n",
      "Marketing Prediction: The text provided does not explicitly mention any specific licensing models adopted for the distribution of source-available language models. However, it seems that the discussion revolves around the challenges and potential risks associated with the unrestricted access and adoption of large language models (LLMs). The text highlights the need for proper regulation to prevent harmful applications, suggesting that there is a careful consideration of how these models should be distributed and accessed. It is likely that the licensing models for source-available language models would aim to balance between promoting innovation and ensuring ethical usage, with measures in place to address biases and harmful content.\n",
      "Marketing Answer: Answer: Some organizations choose open-sourcing, while others restrict access to a few organizations with resources or offer end-to-end deployment via API.\n",
      "Before LLM: messages=[HumanMessage(content='[INST]\\n              Please provide an precise and concise answer to the question below based on the context information provided.\\n\\n\\n              Below is a context:\\nthe purpose of our language tasks, we use a framework similar to Askell et al. (2021), who deﬁne\\nmodels to be aligned if they are helpful, honest, and harmless.\\n\\ntechniques in natural language processing (NLP) where words or phrases from the vocabulary\\nare mapped to vectors of real numbers. Conceptually it involves a mathematical embedding\\n\\nalgorithm. Machine Learning, 97(3):327–351, July 2014. doi: 10.1007/s10994-014-5458-8.\\nURL https://doi.org/10.1007/s10994-014-5458-8.\\n[10] Y. Chen, R. Wang, H. Jiang, S. Shi, and R.-L. Xu. Exploring the use of large language models\\n\\nGeneral-Purpose Architectures for NLP\\nPrior work on general-purpose architectures for NLP\\ntasks has shown great success without the use of retrieval. A single, pre-trained language model\\n\\nmodels.\\nhttps://github.com/tatsu-lab/\\nalpaca_eval, 2023.\\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,\\nMishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,\\net al. Training language models to follow instructions\\n\\nthem to do what a given set of humans want them to do. By default, language models optimize\\nthe next word prediction objective, which is only a proxy for what we want these models to do.\\n\\n              Below is a question:\\nWhat are language models and what is their purpose in natural language processing?\\n\\n              Below are answer instructions:\\n- Keep your answer to one paragraph.\\n- Answer with complete sentences or phrases. Do not use bullet points. \\n[/INST]\\n')]\n",
      "Before LLM: messages=[HumanMessage(content='[INST]\\n              Please provide an precise and concise answer to the question below based on the context information provided.\\n\\n\\n              Below is a context:\\nthe purpose of our language tasks, we use a framework similar to Askell et al. (2021), who deﬁne\\nmodels to be aligned if they are helpful, honest, and harmless.\\n\\ntechniques in natural language processing (NLP) where words or phrases from the vocabulary\\nare mapped to vectors of real numbers. Conceptually it involves a mathematical embedding\\n\\nalgorithm. Machine Learning, 97(3):327–351, July 2014. doi: 10.1007/s10994-014-5458-8.\\nURL https://doi.org/10.1007/s10994-014-5458-8.\\n[10] Y. Chen, R. Wang, H. Jiang, S. Shi, and R.-L. Xu. Exploring the use of large language models\\n\\nGeneral-Purpose Architectures for NLP\\nPrior work on general-purpose architectures for NLP\\ntasks has shown great success without the use of retrieval. A single, pre-trained language model\\n\\nmodels.\\nhttps://github.com/tatsu-lab/\\nalpaca_eval, 2023.\\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.,\\nMishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A.,\\net al. Training language models to follow instructions\\n\\nthem to do what a given set of humans want them to do. By default, language models optimize\\nthe next word prediction objective, which is only a proxy for what we want these models to do.\\n\\n              Below is a question:\\nWhat are language models and what is their purpose in natural language processing?\\n\\n              Below are answer instructions:\\n- Keep your answer to one paragraph.\\n- Answer with complete sentences or phrases. Do not use bullet points. \\n[/INST]\\n')]\n",
      "------------------------------------------------------------\n",
      "Question: What are language models and what is their purpose in natural language processing?\n",
      " \n",
      "Engineering Prediction: Language models are a fundamental component of Natural Language Processing (NLP), where words and phrases are mapped to vectors of real numbers, known as embeddings. These models are designed to be helpful, honest, and harmless, optimizing the next-word prediction objective to enhance their performance in various NLP tasks. The primary purpose of language models in NLP is to enable machines to understand and generate human language, facilitating applications such as language translation, text generation, and sentiment analysis.\n",
      "Engineering Answer: Language models are probabilistic models of natural language that help predict or correct text. Their purpose in natural language processing is to assist in various tasks such as speech recognition, machine translation, natural language generation, and information retrieval. By analyzing the performance of human subjects, language models improve the understanding and generation of human-like text.\n",
      "\n",
      "Marketing Prediction: Language models are a fundamental component of Natural Language Processing (NLP), where words and phrases are mapped to vectors of real numbers, known as embeddings. These models are designed to be helpful, honest, and harmless, optimizing the next-word prediction objective to enhance their performance in various NLP tasks. The primary purpose of language models in NLP is to enable machines to understand and generate human language, facilitating applications such as language translation, text generation, and sentiment analysis.\n",
      "Marketing Answer: Language models are probabilistic models of natural language that are used in tasks such as speech recognition, machine translation, and natural language generation in natural language processing.\n",
      "Before LLM: messages=[HumanMessage(content='[INST]\\n              Please provide an precise and concise answer to the question below based on the context information provided.\\n\\n\\n              Below is a context:\\n[47] Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu,\\nand Alexander Miller. Language models as knowledge bases? In Proceedings of the 2019\\n\\nmade in the 1930’s set in France”) or when constraints can be challenging for language models (e.g.\\nwriting a summary in a speciﬁed number of sentences).\\n\\n2014).\\nHowever, making language models better at following user intentions also makes them easier to\\nmisuse. It may be easier to use these models to generate convincing misinformation, or hateful or\\nabusive content.\\n\\nWe mostly focus on QA models that contain neural networks, specially Transformer-based language models.\\nI admit that I missed a lot of papers with architectures designed specifically for QA tasks between 2017-2019\\uf8ffüòî\\n\\nlimitations, and societal impact of large language models. arXiv preprint arXiv:2102.02503.\\nThoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng, H.-T., Jin, A., Bos,\\n\\nFraser Kelton\\nLuke Miller\\nMaddie Simens\\nAmanda Askell†\\nPeter Welinder\\nPaul Christiano∗†\\nJan Leike∗\\nRyan Lowe∗\\nOpenAI\\nAbstract\\nMaking language models bigger does not inherently make them better at following\\n\\n              Below is a question:\\nHow have language models evolved in terms of architecture, from the 1980s to present times?\\n\\n              Below are answer instructions:\\n- Keep your answer to one paragraph.\\n- Answer with complete sentences or phrases. Do not use bullet points. \\n[/INST]\\n')]\n",
      "Before LLM: messages=[HumanMessage(content='[INST]\\n              Please provide an precise and concise answer to the question below based on the context information provided.\\n\\n\\n              Below is a context:\\n[47] Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu,\\nand Alexander Miller. Language models as knowledge bases? In Proceedings of the 2019\\n\\nmade in the 1930’s set in France”) or when constraints can be challenging for language models (e.g.\\nwriting a summary in a speciﬁed number of sentences).\\n\\n2014).\\nHowever, making language models better at following user intentions also makes them easier to\\nmisuse. It may be easier to use these models to generate convincing misinformation, or hateful or\\nabusive content.\\n\\nWe mostly focus on QA models that contain neural networks, specially Transformer-based language models.\\nI admit that I missed a lot of papers with architectures designed specifically for QA tasks between 2017-2019\\uf8ffüòî\\n\\nlimitations, and societal impact of large language models. arXiv preprint arXiv:2102.02503.\\nThoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng, H.-T., Jin, A., Bos,\\n\\nFraser Kelton\\nLuke Miller\\nMaddie Simens\\nAmanda Askell†\\nPeter Welinder\\nPaul Christiano∗†\\nJan Leike∗\\nRyan Lowe∗\\nOpenAI\\nAbstract\\nMaking language models bigger does not inherently make them better at following\\n\\n              Below is a question:\\nHow have language models evolved in terms of architecture, from the 1980s to present times?\\n\\n              Below are answer instructions:\\n- Keep your answer to one paragraph.\\n- Answer with complete sentences or phrases. Do not use bullet points. \\n[/INST]\\n')]\n",
      "------------------------------------------------------------\n",
      "Question: How have language models evolved in terms of architecture, from the 1980s to present times?\n",
      " \n",
      "Engineering Prediction: Language models have evolved significantly since the 1980s, with a shift from traditional rule-based models to more advanced neural network-based models. The introduction of Transformer-based language models, such as those based on the Transformer architecture, has revolutionized the field, allowing models to process longer sequences of text and capture complex linguistic patterns. These models, including those designed specifically for QA tasks, have improved in their ability to understand and generate human language, with a focus on following user intentions and constraints, such as generating text within a specific context or summarizing text in a specified number of sentences. However, as these models become more powerful, there are also increasing concerns about their potential misuse, such as generating misleading or abusive content.\n",
      "Engineering Answer: Language models have evolved significantly in terms of architecture from the 1980s to present times. In the 1980s, the first statistical language model was proposed, leading to experiments by IBM that identified areas for improvement by observing human subjects. However, it wasn't until 2017 when the transformer architecture was introduced by Google, revolutionizing the field. This development paved the way for models like BERT in 2018, which marked a shift towards large-scale transformer-based language models. These modern architectures, based on self-attention mechanisms, have dominated the field of natural language processing, achieving state-of-the-art performance in various tasks.\n",
      "\n",
      "Marketing Prediction: Language models have evolved significantly since the 1980s, with a shift from traditional rule-based models to more advanced neural network architectures. The introduction of Transformer-based models, such as the Transformer architecture and its variants, has revolutionized the field, allowing for improved context understanding and generating more coherent and contextually appropriate responses. These models, with their self-attention mechanisms, have enabled language models to process longer sequences and capture global dependencies, resulting in better performance on various language tasks. Additionally, the increase in model size and the availability of large-scale datasets have further contributed to the advancements in language model capabilities, enabling them to handle complex language tasks and generate more human-like responses.\n",
      "Marketing Answer: Language models have evolved from early statistical models in the 1980s to modern transformer architectures, such as BERT and GPT-2, which use self-attention mechanisms and have become dominant in natural language processing tasks.\n",
      "Before LLM: messages=[HumanMessage(content='[INST]\\n              Please provide an precise and concise answer to the question below based on the context information provided.\\n\\n\\n              Below is a context:\\nP Pi = 1 and each Pi is proportional to the total probability the model assigns to that completion.\\nMaximum entropy for binary choices is 1. High entropy indicates that the model is unsure of which\\n\\n3.1\\nFormulation\\nTransformer-based language modeling usually leverages the position information of individual tokens through a self-\\nattention mechanism. As can be observed in Equation (2), q⊺\\nmkn typically enables knowledge conveyance between\\n\\ncomplete derivation. Even if we use the MLE estimate rϕ of the ground-truth reward function r∗, it is\\nstill expensive to estimate the partition function Z(x) [17, 15], which makes this representation hard\\n\\ninvestigations of the tradeoffs of simple cross-entropy loss and RLHF training. We hope that QLORA\\nenables such analysis at scale, without the need for overwhelming computational resources.\\n7\\nRelated Work\\nQuantization of Large Language Models\\n\\nDuring full ﬁne-tuning, the model is initialized to pre-trained weights Φ0 and updated to Φ0 + ∆Φ\\nby repeatedly following the gradient to maximize the conditional language modeling objective:\\nmax\\nΦ\\nX\\n(x,y)∈Z\\n|y|\\nX\\nt=1\\nlog (PΦ(yt|x, y<t))\\n(1)\\n\\ncations. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\\n(Tutorial), 2023a. URL https://aclanthology.org/2023.acl-tutorials.6.\\n\\n              Below is a question:\\nCan you explain how maximum entropy language models work and what the partition function signifies?\\n\\n              Below are answer instructions:\\n- Keep your answer to one paragraph.\\n- Answer with complete sentences or phrases. Do not use bullet points. \\n[/INST]\\n')]\n",
      "Before LLM: messages=[HumanMessage(content='[INST]\\n              Please provide an precise and concise answer to the question below based on the context information provided.\\n\\n\\n              Below is a context:\\nP Pi = 1 and each Pi is proportional to the total probability the model assigns to that completion.\\nMaximum entropy for binary choices is 1. High entropy indicates that the model is unsure of which\\n\\n3.1\\nFormulation\\nTransformer-based language modeling usually leverages the position information of individual tokens through a self-\\nattention mechanism. As can be observed in Equation (2), q⊺\\nmkn typically enables knowledge conveyance between\\n\\ncomplete derivation. Even if we use the MLE estimate rϕ of the ground-truth reward function r∗, it is\\nstill expensive to estimate the partition function Z(x) [17, 15], which makes this representation hard\\n\\ninvestigations of the tradeoffs of simple cross-entropy loss and RLHF training. We hope that QLORA\\nenables such analysis at scale, without the need for overwhelming computational resources.\\n7\\nRelated Work\\nQuantization of Large Language Models\\n\\nDuring full ﬁne-tuning, the model is initialized to pre-trained weights Φ0 and updated to Φ0 + ∆Φ\\nby repeatedly following the gradient to maximize the conditional language modeling objective:\\nmax\\nΦ\\nX\\n(x,y)∈Z\\n|y|\\nX\\nt=1\\nlog (PΦ(yt|x, y<t))\\n(1)\\n\\ncations. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\\n(Tutorial), 2023a. URL https://aclanthology.org/2023.acl-tutorials.6.\\n\\n              Below is a question:\\nCan you explain how maximum entropy language models work and what the partition function signifies?\\n\\n              Below are answer instructions:\\n- Keep your answer to one paragraph.\\n- Answer with complete sentences or phrases. Do not use bullet points. \\n[/INST]\\n')]\n",
      "------------------------------------------------------------\n",
      "Question: Can you explain how maximum entropy language models work and what the partition function signifies?\n",
      " \n",
      "Engineering Prediction: Maximum entropy language models aim to capture as much uncertainty as possible, and the maximum entropy for binary choices is 1, indicating the model's uncertainty. The partition function, Z(x), in this context, represents the normalization factor in the model's probability distribution and is challenging to estimate, even with maximum likelihood estimation. It plays a crucial role in ensuring the model's probabilities sum to 1 and provides a measure of the model's complexity and uncertainty.\n",
      "Engineering Answer: Maximum entropy language models use feature functions to encode the relationship between a word and its n-gram history, aiming to maximize reward while satisfying a KL-constrained objective. The partition function, denoted as Z(x), is crucial in normalizing the probabilities of all possible outputs given the input. It represents the sum of the exponential of the reward function over all possible output sequences, making it computationally expensive to estimate but essential for accurate modeling. The partition function ensures that the model's predicted probabilities sum up to 1, providing a foundation for effective language modeling.\n",
      "\n",
      "Marketing Prediction: Maximum entropy language models aim to capture as much uncertainty as possible, and the maximum entropy for binary choices is 1, indicating the model's uncertainty. The partition function, Z(x), in this context, represents the normalization factor in the model's probability distribution and is challenging to estimate, even with maximum likelihood estimation. It plays a crucial role in ensuring the model's probabilities sum to 1 and is a key component in understanding the model's behavior and tradeoffs between cross-entropy loss and RLHF training.\n",
      "Marketing Answer: Maximum entropy language models encode the relationship between a word and the n-gram history using feature functions. The partition function in this context represents the total probability of all possible outcomes, making it a crucial factor in determining the optimal solution for the reward maximization objective.\n",
      "Before LLM: messages=[HumanMessage(content='[INST]\\n              Please provide an precise and concise answer to the question below based on the context information provided.\\n\\n\\n              Below is a context:\\nfrom a space with one dimension per word to a continuous vector space with a much lower\\ndimension. Output: Word embeddings are useful for tasks such as sentiment analysis, text\\n\\nInternational Conference on Learning Representations, 2023. URL https://openreview.\\nnet/forum?id=8aHzds2uUyB.\\n[33] M. Ranzato, S. Chopra, M. Auli, and W. Zaremba. Sequence level training with recurrent neural\\nnetworks. CoRR, abs/1511.06732, 2015.\\n\\nencodings.\\n2. Position Interpolation generates strong models that can effectively make use of much ex-\\ntended context window. We show that models extended by Position Interpolation enjoy\\n\\nthem to do what a given set of humans want them to do. By default, language models optimize\\nthe next word prediction objective, which is only a proxy for what we want these models to do.\\n\\nThe external memory can alleviate the restriction of finite attention span.  A standard practice is to save the embedding representation of information into a vector store database that can support fast maximum inner-product search (MIPS). To optimize the\\n\\nwindow is 10\\nUninfor-\\nmative\\n20.0%\\nWhat do they mean by intrinsic\\ngeometry of spaces of learned\\nrepresentations?\\n“the inferred embedding space\\ncreates a globally consistent\\nstructured prediction of the\\nontology, rather than local relation\\npredictions”\\n\\n              Below is a question:\\nWhat is the benefit of using continuous space embeddings in recurrent neural network language models?\\n\\n              Below are answer instructions:\\n- Keep your answer to one paragraph.\\n- Answer with complete sentences or phrases. Do not use bullet points. \\n[/INST]\\n')]\n",
      "Before LLM: messages=[HumanMessage(content='[INST]\\n              Please provide an precise and concise answer to the question below based on the context information provided.\\n\\n\\n              Below is a context:\\nfrom a space with one dimension per word to a continuous vector space with a much lower\\ndimension. Output: Word embeddings are useful for tasks such as sentiment analysis, text\\n\\nInternational Conference on Learning Representations, 2023. URL https://openreview.\\nnet/forum?id=8aHzds2uUyB.\\n[33] M. Ranzato, S. Chopra, M. Auli, and W. Zaremba. Sequence level training with recurrent neural\\nnetworks. CoRR, abs/1511.06732, 2015.\\n\\nencodings.\\n2. Position Interpolation generates strong models that can effectively make use of much ex-\\ntended context window. We show that models extended by Position Interpolation enjoy\\n\\nthem to do what a given set of humans want them to do. By default, language models optimize\\nthe next word prediction objective, which is only a proxy for what we want these models to do.\\n\\nThe external memory can alleviate the restriction of finite attention span.  A standard practice is to save the embedding representation of information into a vector store database that can support fast maximum inner-product search (MIPS). To optimize the\\n\\nwindow is 10\\nUninfor-\\nmative\\n20.0%\\nWhat do they mean by intrinsic\\ngeometry of spaces of learned\\nrepresentations?\\n“the inferred embedding space\\ncreates a globally consistent\\nstructured prediction of the\\nontology, rather than local relation\\npredictions”\\n\\n              Below is a question:\\nWhat is the benefit of using continuous space embeddings in recurrent neural network language models?\\n\\n              Below are answer instructions:\\n- Keep your answer to one paragraph.\\n- Answer with complete sentences or phrases. Do not use bullet points. \\n[/INST]\\n')]\n",
      "------------------------------------------------------------\n",
      "Question: What is the benefit of using continuous space embeddings in recurrent neural network language models?\n",
      " \n",
      "Engineering Prediction: Using continuous space embeddings in recurrent neural network language models offers the advantage of transforming words from a high-dimensional space to a lower-dimensional continuous vector space. This transformation, known as word embeddings, captures semantic relationships between words and improves the model's ability to understand and generate language. By representing words in a continuous space, the model can perform tasks such as sentiment analysis and text generation more effectively, as it can capture the context and relationships between words more accurately.\n",
      "Engineering Answer: Continuous space embeddings in recurrent neural network language models help alleviate the curse of dimensionality by representing words as non-linear combinations of weights in the embedding space. This approach helps address the data sparsity problem caused by the exponential increase in possible word sequences with vocabulary size. By utilizing continuous space embeddings, neural networks can effectively capture semantic relationships and meaning within the language model.\n",
      "\n",
      "Marketing Prediction: Using continuous space embeddings in recurrent neural network language models offers the advantage of transforming words from a high-dimensional space to a lower-dimensional continuous vector space. This transformation enhances the model's ability to capture semantic relationships between words, enabling more effective sentiment analysis and text classification tasks. Additionally, it facilitates the utilization of extended context windows, improving the model's understanding of word contexts and enhancing its performance in language modeling tasks.\n",
      "Marketing Answer: Continuous space embeddings in recurrent neural network language models help alleviate the curse of dimensionality caused by the exponential increase in possible word sequences, reducing data sparsity issues.\n",
      "Before LLM: messages=[HumanMessage(content='[INST]\\n              Please provide an precise and concise answer to the question below based on the context information provided.\\n\\n\\n              Below is a context:\\nbarrier to training more language models from human preferences.\\nLimitations & Future Work. Our results raise several important questions for future work. How\\n\\n[17] Zhang et al. “Automatic chain of thought prompting in large language models.” arXiv preprint arXiv:2210.03493 (2022).\\n\\n(non-retrieval augmented) large language models\\nindeed suffer from hallucination, whereas our best\\nmodels substantially curtail the issue, reducing\\nhallucinated responses by over 60%. We show\\nthat this effect is even more pronounced on out-\\n\\nGarriga-Alonso, A., et al. Beyond the imitation game:\\nQuantifying and extrapolating the capabilities of language\\nmodels. arXiv preprint arXiv:2206.04615, 2022.\\nStiennon, N., Ouyang, L., Wu, J., Ziegler, D., Lowe, R.,\\n\\nwide range of instructions. There are many open questions to explore to further align language model\\nbehavior with what people actually want them to do.\\nMany methods could be tried to further decrease the models’ propensity to generate toxic, biased,\\n\\namong the largest language models today and we apply them on a wide range of language tasks,\\nincluding classiﬁcation, summarization, question-answering, creative writing, dialogue, and others.\\n\\n              Below is a question:\\nWhat challenges do large language models face in mirroring human cognitive patterns?\\n\\n              Below are answer instructions:\\n- Keep your answer to one paragraph.\\n- Answer with complete sentences or phrases. Do not use bullet points. \\n[/INST]\\n')]\n",
      "Before LLM: messages=[HumanMessage(content='[INST]\\n              Please provide an precise and concise answer to the question below based on the context information provided.\\n\\n\\n              Below is a context:\\nbarrier to training more language models from human preferences.\\nLimitations & Future Work. Our results raise several important questions for future work. How\\n\\n[17] Zhang et al. “Automatic chain of thought prompting in large language models.” arXiv preprint arXiv:2210.03493 (2022).\\n\\n(non-retrieval augmented) large language models\\nindeed suffer from hallucination, whereas our best\\nmodels substantially curtail the issue, reducing\\nhallucinated responses by over 60%. We show\\nthat this effect is even more pronounced on out-\\n\\nGarriga-Alonso, A., et al. Beyond the imitation game:\\nQuantifying and extrapolating the capabilities of language\\nmodels. arXiv preprint arXiv:2206.04615, 2022.\\nStiennon, N., Ouyang, L., Wu, J., Ziegler, D., Lowe, R.,\\n\\nwide range of instructions. There are many open questions to explore to further align language model\\nbehavior with what people actually want them to do.\\nMany methods could be tried to further decrease the models’ propensity to generate toxic, biased,\\n\\namong the largest language models today and we apply them on a wide range of language tasks,\\nincluding classiﬁcation, summarization, question-answering, creative writing, dialogue, and others.\\n\\n              Below is a question:\\nWhat challenges do large language models face in mirroring human cognitive patterns?\\n\\n              Below are answer instructions:\\n- Keep your answer to one paragraph.\\n- Answer with complete sentences or phrases. Do not use bullet points. \\n[/INST]\\n')]\n",
      "------------------------------------------------------------\n",
      "Question: What challenges do large language models face in mirroring human cognitive patterns?\n",
      " \n",
      "Engineering Prediction: Large language models face challenges in mirroring human cognitive patterns due to their propensity to generate toxic and biased responses, as well as their tendency to hallucinate answers. These issues highlight the need for further research and improvements to better align language model behavior with human expectations and preferences. Current efforts are focused on exploring methods to decrease undesirable responses and enhance the models' understanding of human instructions across a diverse range of language tasks.\n",
      "Engineering Answer: Large language models face challenges in mirroring human cognitive patterns because they sometimes learn patterns that humans do not learn, while also failing to learn patterns that humans typically learn. This discrepancy suggests that the models may not be plausible cognitive models, despite matching human performance in some tasks. Further research is needed to address these limitations and improve the alignment of large language models with human cognitive patterns.\n",
      "\n",
      "Marketing Prediction: Large language models face challenges in mirroring human cognitive patterns due to their propensity to generate toxic and biased responses, as well as their tendency to hallucinate answers. These issues highlight the need for further research and improvements to better align language model behavior with human expectations and preferences. Current efforts are focused on decreasing undesirable responses and improving performance on a wide range of language tasks to make these models more reliable and accurate.\n",
      "Marketing Answer: Large language models sometimes learn patterns that humans do not learn and fail to learn patterns that humans typically do learn.\n"
     ]
    }
   ],
   "source": [
    "# using function to capture all metrics\n",
    "metrics = ['rouge', 'bleu', 'bertscore']\n",
    "impr_base_prompt = evaluate(metrics, validation_questions_answers, rag_chain, iterations=10, verbose=False, sleep=False, print_results=True)\n",
    "impr_base_prompt = composite_evaluation(impr_base_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "8Zw8TLe-WDCt",
    "outputId": "b2507d18-6983-48e3-b7e0-bf6bc9bef0fd"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"impr_base_prompt\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"Sample\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.373295374522416,\n        \"min\": 0.0,\n        \"max\": 13.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          6.6,\n          7.5,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eng_bleu\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.5082437635175068,\n        \"min\": 0.0,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.08228804888133583,\n          0.09335414094751296,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mk_bleu\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.523137606811024,\n        \"min\": 0.0,\n        \"max\": 10.0,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          10.0,\n          0.03357043363726836,\n          0.05681608701866214\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eng_rouge\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.4503125954980036,\n        \"min\": 0.06619247829845366,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.26677369689526376,\n          0.2662728026533997,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mk_rouge\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.4732672339120767,\n        \"min\": 0.07247821359185543,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.1979627756137466,\n          0.2,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eng_f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.280454713437368,\n        \"min\": 0.012792798968946995,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.886654794216156,\n          0.8829266428947449,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mk_f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.2852228769080805,\n        \"min\": 0.012933940922083411,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.8705347597599029,\n          0.8716857433319092,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"composite_eng\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.375359497083477,\n        \"min\": 0.02686678847615891,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.5398171159529243,\n          0.5401704242396028,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"composite_mk\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.386623627840891,\n        \"min\": 0.03288356288359631,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.5013702992915291,\n          0.5071263252168501,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"composite_total\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.380569753609636,\n        \"min\": 0.017455413558537955,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.5244383892883662,\n          0.5218074562651109,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-e9d150af-47e0-4d4c-b8c3-189212af9a8d\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample</th>\n",
       "      <th>eng_bleu</th>\n",
       "      <th>mk_bleu</th>\n",
       "      <th>eng_rouge</th>\n",
       "      <th>mk_rouge</th>\n",
       "      <th>eng_f1</th>\n",
       "      <th>mk_f1</th>\n",
       "      <th>composite_eng</th>\n",
       "      <th>composite_mk</th>\n",
       "      <th>composite_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.600000</td>\n",
       "      <td>0.082288</td>\n",
       "      <td>0.033570</td>\n",
       "      <td>0.266774</td>\n",
       "      <td>0.197963</td>\n",
       "      <td>0.886655</td>\n",
       "      <td>0.870535</td>\n",
       "      <td>0.539817</td>\n",
       "      <td>0.501370</td>\n",
       "      <td>0.524438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.788876</td>\n",
       "      <td>0.063534</td>\n",
       "      <td>0.038740</td>\n",
       "      <td>0.066192</td>\n",
       "      <td>0.072478</td>\n",
       "      <td>0.012793</td>\n",
       "      <td>0.012934</td>\n",
       "      <td>0.026867</td>\n",
       "      <td>0.032884</td>\n",
       "      <td>0.017455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.176211</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.873402</td>\n",
       "      <td>0.841682</td>\n",
       "      <td>0.496033</td>\n",
       "      <td>0.443918</td>\n",
       "      <td>0.495907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.250000</td>\n",
       "      <td>0.016754</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.243405</td>\n",
       "      <td>0.142622</td>\n",
       "      <td>0.875753</td>\n",
       "      <td>0.865623</td>\n",
       "      <td>0.527241</td>\n",
       "      <td>0.474888</td>\n",
       "      <td>0.514752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.500000</td>\n",
       "      <td>0.093354</td>\n",
       "      <td>0.019997</td>\n",
       "      <td>0.266273</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.882927</td>\n",
       "      <td>0.871686</td>\n",
       "      <td>0.540170</td>\n",
       "      <td>0.507126</td>\n",
       "      <td>0.521807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>10.500000</td>\n",
       "      <td>0.136794</td>\n",
       "      <td>0.056816</td>\n",
       "      <td>0.280510</td>\n",
       "      <td>0.250646</td>\n",
       "      <td>0.893990</td>\n",
       "      <td>0.880954</td>\n",
       "      <td>0.560596</td>\n",
       "      <td>0.522799</td>\n",
       "      <td>0.531066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>0.155562</td>\n",
       "      <td>0.099238</td>\n",
       "      <td>0.415842</td>\n",
       "      <td>0.311111</td>\n",
       "      <td>0.908026</td>\n",
       "      <td>0.884005</td>\n",
       "      <td>0.576867</td>\n",
       "      <td>0.545910</td>\n",
       "      <td>0.555555</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e9d150af-47e0-4d4c-b8c3-189212af9a8d')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-e9d150af-47e0-4d4c-b8c3-189212af9a8d button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-e9d150af-47e0-4d4c-b8c3-189212af9a8d');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-9ba27497-39de-4a9b-afc6-59ff30f13af1\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9ba27497-39de-4a9b-afc6-59ff30f13af1')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-9ba27497-39de-4a9b-afc6-59ff30f13af1 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "          Sample   eng_bleu    mk_bleu  eng_rouge   mk_rouge     eng_f1  \\\n",
       "count  10.000000  10.000000  10.000000  10.000000  10.000000  10.000000   \n",
       "mean    6.600000   0.082288   0.033570   0.266774   0.197963   0.886655   \n",
       "std     4.788876   0.063534   0.038740   0.066192   0.072478   0.012793   \n",
       "min     0.000000   0.000000   0.000000   0.176211   0.076923   0.873402   \n",
       "25%     2.250000   0.016754   0.000000   0.243405   0.142622   0.875753   \n",
       "50%     7.500000   0.093354   0.019997   0.266273   0.200000   0.882927   \n",
       "75%    10.500000   0.136794   0.056816   0.280510   0.250646   0.893990   \n",
       "max    13.000000   0.155562   0.099238   0.415842   0.311111   0.908026   \n",
       "\n",
       "           mk_f1  composite_eng  composite_mk  composite_total  \n",
       "count  10.000000      10.000000     10.000000        10.000000  \n",
       "mean    0.870535       0.539817      0.501370         0.524438  \n",
       "std     0.012934       0.026867      0.032884         0.017455  \n",
       "min     0.841682       0.496033      0.443918         0.495907  \n",
       "25%     0.865623       0.527241      0.474888         0.514752  \n",
       "50%     0.871686       0.540170      0.507126         0.521807  \n",
       "75%     0.880954       0.560596      0.522799         0.531066  \n",
       "max     0.884005       0.576867      0.545910         0.555555  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "impr_base_prompt.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JGzF4hdfXUK0"
   },
   "source": [
    "While these additional instructions create a slight decrease in our evaluation metric score, it does show significant improvements in the structuring and formatting of the answers.  It is now time to separate the specific departments responses and start tuning them for the intricacies of the specific departments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f0Z3NUgEaJWL"
   },
   "source": [
    "##### **Basic Inclusion of Department**\n",
    "\n",
    "**First Pass:** We first start with creating a separate prompt for both Marketing and Engineering based off the agnostic prompt that we created above, and adding additional context to the specific user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j7aGOzw_6wbq"
   },
   "outputs": [],
   "source": [
    "eng_rag_template = \"\"\"[INST]\n",
    "              Please provide an precise and concise answer to the engineer's question below based on the context information provided.\\n\\n\n",
    "              Below is a context:\\n{context}\\n\n",
    "              Below is a question:\\n{question}\\n\n",
    "              Below are answer instructions:\n",
    "- Keep your answer to one paragraph.\n",
    "- Answer with complete sentences or phrases. Do not use bullet points.\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "mk_rag_template = \"\"\"[INST]\n",
    "              Please provide an precise and concise answer to the marketer's question below based on the context information provided.\\n\\n\n",
    "              Below is a context:\\n{context}\\n\n",
    "              Below is a question:\\n{question}\\n\n",
    "              Below are answer instructions:\n",
    "- Keep your answer to one paragraph.\n",
    "- Answer with complete sentences or phrases. Do not use bullet points.\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "eng_rag_prompt = ChatPromptTemplate.from_template(eng_rag_template)\n",
    "mk_rag_prompt = ChatPromptTemplate.from_template(mk_rag_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M_xqCAkTzHkk",
    "outputId": "d4844de5-c912-4411-8605-dd5f903bf84b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Question: What purpose do large language models serve in the field of natural language processing?\n",
      " \n",
      "Engineering Prediction: Large language models serve as the foundation for many natural language processing applications. These models are pre-trained on vast amounts of text data and can be adapted to various downstream tasks through fine-tuning. They excel at predicting the next word in a sequence, which forms the basis for more complex language tasks such as classification, summarization, question-answering, and creative writing. The ability to leverage large language models enables more accurate and contextually aware language processing, powering a new generation of language-based applications.\n",
      "Engineering Answer: Large language models (LLMs) serve the purpose of enabling general-purpose language generation and other natural language processing tasks such as classification. They achieve this by learning statistical relationships from text documents during computationally intensive self-supervised and semi-supervised training. LLMs can be used for text generation by predicting the next token or word, making them valuable for tasks like speech recognition, machine translation, and information retrieval. Additionally, LLMs have superseded previous models like recurrent neural networks, showcasing their efficiency and effectiveness in NLP tasks.\n",
      "\n",
      "Marketing Prediction: Large language models serve as the foundation for many natural language processing applications. These models are first trained on vast amounts of text data to learn the underlying patterns and structures of human language. This process allows them to develop a comprehensive understanding of language, which can then be adapted for specific tasks through fine-tuning. The versatility of large language models enables their application across a diverse range of language tasks, such as classification, summarization, question-answering, and even creative endeavors like writing and dialogue generation.\n",
      "Marketing Answer: Large language models serve the purpose of improving performance in various natural language processing tasks, such as speech recognition, machine translation, natural language generation, optical character recognition, handwriting recognition, grammar induction, and information retrieval.\n",
      "------------------------------------------------------------\n",
      "Question: How does a large language model learn from text during training?\n",
      " \n",
      "Engineering Prediction: Large language models are trained using a variety of methods, including pre-training on large text corpora and fine-tuning on specific downstream tasks. During pre-training, these models learn by predicting the next word in a sequence of text, which helps them understand the context and generate coherent responses. Fine-tuning involves further training on specific tasks, such as question-answering or sentiment analysis, to adapt the model to particular applications. This process allows the model to learn from text data and improve its performance on specific language tasks.\n",
      "Engineering Answer: A large language model learns from text during training by first going through an unsupervised generative 'pretraining' stage where it sets initial parameters using a language modeling objective. Then, it goes through a supervised discriminative 'fine-tuning' stage where it refines its parameters based on annotated examples or task demonstrations. This dual-stage approach allows the model to learn statistical relationships from text documents in a computationally intensive process, enabling it to achieve general-purpose language generation and natural language processing tasks.\n",
      "\n",
      "Marketing Prediction: Large language models are trained using a variety of methods, including pre-training on large text corpora and fine-tuning on specific downstream tasks. During pre-training, these models learn by predicting the next word in a sequence of text, which helps them understand the context and generate coherent responses. Fine-tuning involves further training on specific tasks, such as question-answering or sentiment analysis, to adapt the model to a particular domain or task. This process allows the model to learn from text data and improve its performance on specific language-related tasks.\n",
      "Marketing Answer: A large language model learns from text during training by first pretraining on a diverse dataset to acquire general language knowledge, and then fine-tuning on specific tasks or demonstrations to adapt its parameters for more targeted performance.\n",
      "------------------------------------------------------------\n",
      "Question: What are some key architectures behind the development of large language models?\n",
      " \n",
      "Engineering Prediction: Some key architectures that have contributed to the development of large language models include Transformer-based language models and neural networks. Additionally, the paper \"Llama: Open and efficient foundation language models\" by Goyal et al. and \"Gorilla: Large Language Model Connected with Massive APIs\" by Patil et al. are cited, suggesting that these works may also describe relevant architectures or approaches for building large language models. The engineer also expresses interest in QA-specific architectures from 2017 to 2019, indicating a desire to explore more specialized architectures alongside general and scalable methods for future AI systems.\n",
      "Engineering Answer: Key architectures behind the development of large language models include the use of self-attention mechanisms, such as those seen in Transformer decoders. These architectures have been applied to tasks like autoregressive language modeling and have led to the dominance of Transformer-based language models in NLP. Models like BERT and GPT-2 have further advanced this paradigm, showcasing the power of large Transformer language models in achieving state-of-the-art results across various NLP tasks. Additionally, architectures like neural-retriever-in-the-loop generative-based models have shown improvements in tasks like open-domain QA and knowledge-grounded dialogue, emphasizing the importance of consistent and engaging responses in long-form generation and multi-turn conversations.\n",
      "\n",
      "Marketing Prediction: Some key architectures that have contributed to the development of large language models include Transformer-based language models and neural networks. Additionally, the paper \"Llama: Open and efficient foundation language models\" by Goyal et al. and \"Gorilla: Large Language Model Connected with Massive APIs\" by Patil et al. are likely to contain insights into the architectural choices made in developing these large language models. These models embrace a general approach that utilizes open and efficient foundation models and connects them with massive APIs to enhance their capabilities.\n",
      "Marketing Answer: Key architectures behind the development of large language models include Transformer-based models such as BERT and GPT-2, which utilize self-attention mechanisms for tasks like autoregressive language modeling and knowledge-grounded dialogue. These models have shown significant success in NLP tasks and have led to advancements in general-purpose language generation and natural language processing.\n",
      "------------------------------------------------------------\n",
      "Question: Can you name some specific large language models and the companies or organizations that have developed them?\n",
      " \n",
      "Engineering Prediction: Some prominent examples of large language models and their developers include GPT-3, developed by OpenAI, and BERT, developed by Google. Other well-known models are T5, created by Google AI, and RoBERTa, an open-source model built by Facebook AI Research. These models have had a significant impact on the field of natural language processing and have influenced the development of many other language-based AI systems.\n",
      "Engineering Answer: Some specific large language models include GPT-3 by OpenAI, Chinchilla by DeepMind, and BERT by Google. OpenAI developed GPT-3, DeepMind developed Chinchilla, and Google developed BERT. These models have been significant advancements in the field of natural language processing.\n",
      "\n",
      "Marketing Prediction: Some prominent examples of large language models and their developers include GPT-3, developed by OpenAI, and BERT, which was created by Google. Additionally, Facebook AI has introduced the Blender bot, a conversational agent, and DeepMind has released Gopher, a large-scale transformer model. These organizations have played a significant role in the advancement of language models and their applications.\n",
      "Marketing Answer: Chinchilla by DeepMind, GPT-3 by OpenAI.\n",
      "------------------------------------------------------------\n",
      "Question: What licensing models have been adopted for the distribution of source-available language models?\n",
      " \n",
      "Engineering Prediction: The arXiv preprint by Rozière et al. introduces Llama, an open, and efficient foundation language model. While the paper does not explicitly mention licensing models, it emphasizes the importance of open access and regulation to mitigate potential harms associated with large language models. The introduction of restrictions on access to large language models could indicate a shift towards more controlled distribution and licensing models to address these concerns.\n",
      "Engineering Answer: Based on the provided context, it seems that licensing models for the distribution of source-available language models have not been explicitly discussed in the referenced papers. However, it is crucial to consider potential licensing options such as open-source licenses (e.g., GPL, MIT) or proprietary licenses when distributing language models to ensure legal compliance and control over usage rights. Additionally, considering the implications of different licensing models on accessibility, collaboration, and commercialization is essential for determining the most suitable approach for sharing language models with the community. Further research or consultation with legal experts may be necessary to explore specific licensing strategies for source-available language models.\n",
      "\n",
      "Marketing Prediction: The text provided does not explicitly mention any specific licensing models adopted for the distribution of source-available language models. However, it seems that the discussion revolves around the challenges and potential risks associated with the unrestricted access and adoption of large language models (LLMs). The text highlights the need for proper regulation to prevent harmful applications and the desire to find a balance between open access and controlled distribution. It is likely that the licensing models adopted for source-available language models would aim to address these concerns and strike a balance between accessibility and responsible usage.\n",
      "Marketing Answer: Answer: Some organizations choose open-sourcing, while others restrict access to a few organizations with resources or offer end-to-end deployment via API.\n",
      "------------------------------------------------------------\n",
      "Question: What are language models and what is their purpose in natural language processing?\n",
      " \n",
      "Engineering Prediction: Language models are a critical component of natural language processing (NLP), where they are used to map words and phrases to vectors of real numbers, a process known as embedding. These models are designed to optimize the next-word prediction objective, which is a proxy for their ultimate purpose: to be helpful, honest, and harmless, and to follow instructions and perform tasks as humans want them to.\n",
      "Engineering Answer: Language models are probabilistic models of natural language that help predict or correct text. Their purpose in natural language processing is to assist in various tasks such as speech recognition, machine translation, natural language generation, and information retrieval. By analyzing the performance of human subjects, language models improve the understanding and generation of human-like text.\n",
      "\n",
      "Marketing Prediction: Language models are a critical component of natural language processing (NLP), where they are used to map words and phrases to vectors of real numbers, a process known as embedding. These models are designed to be helpful, honest, and harmless, optimizing the next-word prediction objective to ultimately fulfill the intentions of their human users.\n",
      "Marketing Answer: Language models are probabilistic models of natural language that are used in tasks such as speech recognition, machine translation, and natural language generation in natural language processing.\n",
      "------------------------------------------------------------\n",
      "Question: How have language models evolved in terms of architecture, from the 1980s to present times?\n",
      " \n",
      "Engineering Prediction: Language models have evolved significantly since their inception in the 1980s, with a focus on improving their ability to understand and generate human language. Early language models relied on simple statistical methods and rule-based systems, but with the rise of deep learning and transformer-based models in the 2010s, language models have become increasingly sophisticated. These models, trained on vast amounts of text data, have achieved remarkable performance in a range of tasks, including question answering and text generation. However, as the models have become more powerful, there are also increasing concerns about their potential for misuse, such as generating misleading or abusive content.\n",
      "Engineering Answer: Language models have evolved significantly in terms of architecture from the 1980s to present times. In the 1980s, the first statistical language model was proposed, leading to experiments by IBM that identified areas for improvement by observing human subjects. However, it wasn't until 2017 when the transformer architecture was introduced by Google, revolutionizing the field. This development paved the way for models like BERT in 2018, which marked a shift towards large-scale transformer-based language models. These modern architectures, based on self-attention mechanisms, have dominated the field of natural language processing, achieving state-of-the-art performance in various tasks.\n",
      "\n",
      "Marketing Prediction: Language models have evolved significantly since their inception in the 1980s. Early language models relied on simple statistical methods and rule-based approaches, but with the emergence of neural networks and deep learning, the field has advanced rapidly. The introduction of Transformer-based models, such as BERT and GPT, has revolutionized the field, allowing for context-aware language understanding and generation. These models have become increasingly large and complex, with billions of parameters, enabling them to capture intricate linguistic patterns and generate human-like text. However, as mentioned in the context, this progress comes with challenges and risks, such as the potential for misuse and the generation of misleading or harmful content.\n",
      "Marketing Answer: Language models have evolved from early statistical models in the 1980s to modern transformer architectures, such as BERT and GPT-2, which use self-attention mechanisms and have become dominant in natural language processing tasks.\n",
      "------------------------------------------------------------\n",
      "Question: Can you explain how maximum entropy language models work and what the partition function signifies?\n",
      " \n",
      "Engineering Prediction: Maximum entropy language models aim to capture as much uncertainty as possible, indicated by a value of 1 for binary choices. The partition function, Z(x), is a normalization factor that ensures the probability distribution sums to 1, but it is costly to estimate. By using QLORA, we can investigate the trade-offs of different training methods without the need for extensive computational resources.\n",
      "Engineering Answer: Maximum entropy language models use feature functions to encode the relationship between a word and its n-gram history, aiming to maximize reward while satisfying a KL-constrained objective. The partition function, denoted as Z(x), is crucial in normalizing the probabilities of all possible outputs given the input. It represents the sum of the exponential of the reward function over all possible output sequences, making it computationally expensive to estimate but essential for accurate modeling. The partition function ensures that the model's predicted probabilities sum up to 1, providing a foundation for effective language modeling.\n",
      "\n",
      "Marketing Prediction: Maximum entropy language models aim to capture as much uncertainty as possible, and the maximum entropy value of 1 for binary choices indicates the model's uncertainty about the correct choice. The partition function, Z(x), is a normalization factor that ensures the probability distribution sums to 1 and represents the overall probability of the data given the model. Estimating Z(x) is computationally expensive, and the model's performance depends on accurately estimating this function.\n",
      "Marketing Answer: Maximum entropy language models encode the relationship between a word and the n-gram history using feature functions. The partition function in this context represents the total probability of all possible outcomes, making it a crucial factor in determining the optimal solution for the reward maximization objective.\n",
      "------------------------------------------------------------\n",
      "Question: What is the benefit of using continuous space embeddings in recurrent neural network language models?\n",
      " \n",
      "Engineering Prediction: Continuous space embeddings in recurrent neural network language models offer the advantage of transforming words from a high-dimensional space to a lower-dimensional continuous vector space. This transformation captures semantic relationships between words, enabling better context understanding and more accurate predictions. By representing words in a continuous space, the model can make more nuanced distinctions between similar words and capture subtle differences in meaning, improving overall language understanding and generating more coherent responses.\n",
      "Engineering Answer: Continuous space embeddings in recurrent neural network language models help alleviate the curse of dimensionality by representing words as non-linear combinations of weights in the embedding space. This approach helps address the data sparsity problem caused by the exponential increase in possible word sequences with vocabulary size. By utilizing continuous space embeddings, neural networks can effectively capture semantic relationships and meaning within the language model.\n",
      "\n",
      "Marketing Prediction: Continuous space embeddings in recurrent neural network language models offer the advantage of transforming words from discrete units into continuous vectors, allowing for more efficient modeling of semantic relationships between words. This approach enables the capture of contextual information and word meanings, enhancing the model's ability to understand and generate human-like text. By mapping words to a lower-dimensional space, the model can better represent the context and sentiment of words, improving tasks such as sentiment analysis and natural language understanding.\n",
      "Marketing Answer: Continuous space embeddings in recurrent neural network language models help alleviate the curse of dimensionality caused by the exponential increase in possible word sequences, reducing data sparsity issues.\n",
      "------------------------------------------------------------\n",
      "Question: What challenges do large language models face in mirroring human cognitive patterns?\n",
      " \n",
      "Engineering Prediction: Large language models face challenges in mirroring human cognitive patterns due to their propensity to generate toxic and biased responses, as well as their tendency to hallucinate answers. These issues highlight the need for further research and improvements to better align language model behavior with human expectations and preferences. Current efforts are focused on exploring various methods to decrease these issues and improve the alignment of language models with human cognitive patterns.\n",
      "Engineering Answer: Large language models face challenges in mirroring human cognitive patterns because they sometimes learn patterns that humans do not learn, while also failing to learn patterns that humans typically learn. This discrepancy suggests that the models may not be plausible cognitive models, despite matching human performance in some tasks. Further research is needed to address these limitations and improve the alignment of large language models with human cognitive patterns.\n",
      "\n",
      "Marketing Prediction: Large language models face challenges in mirroring human cognitive patterns due to their propensity to generate toxic and biased responses, as well as their tendency to hallucinate answers. These issues highlight the need for further research and improvements to better align language model behavior with human expectations and preferences. Current efforts are focused on decreasing undesirable responses and improving performance on a wide range of language tasks, including classification, summarization, and question-answering, to make these models more reliable and accurate.\n",
      "Marketing Answer: Large language models sometimes learn patterns that humans do not learn and fail to learn patterns that humans typically do learn.\n"
     ]
    }
   ],
   "source": [
    "eng_rag_chain = build_RAG_prompt_chain(eng_rag_template, llm_model, retriever, format_docs)\n",
    "mk_rag_chain = build_RAG_prompt_chain(mk_rag_template, llm_model, retriever, format_docs)\n",
    "\n",
    "metrics = ['rouge', 'bleu', 'bertscore']\n",
    "\n",
    "rag_chains = {\"engineering\": eng_rag_chain, \"marketing\": mk_rag_chain}\n",
    "\n",
    "dpt_prompt = evaluate(metrics, validation_questions_answers, rag_chains, iterations=10, verbose=False, dept_specific=True, sleep=False, print_results=True)\n",
    "dpt_prompt = composite_evaluation(dpt_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "OgcxXmHecLxf",
    "outputId": "67775b3f-e481-43e2-afad-4f3d6d260c99"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"dpt_prompt\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"Sample\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.373295374522416,\n        \"min\": 0.0,\n        \"max\": 13.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          6.6,\n          7.5,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eng_bleu\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.4989493655644788,\n        \"min\": 0.0,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.10275989126031071,\n          0.0858328109762741,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mk_bleu\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.5209300248524014,\n        \"min\": 0.0,\n        \"max\": 10.0,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          10.0,\n          0.040998138772979836,\n          0.07573769582098393\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eng_rouge\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.449872817655737,\n        \"min\": 0.0841167813644206,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.2692314552460561,\n          0.2401054456160624,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mk_rouge\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.4741291550103606,\n        \"min\": 0.06557377049180327,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.2004807601622844,\n          0.23276337562051846,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eng_f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.2813589254060567,\n        \"min\": 0.0193456456296146,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.8809767365455627,\n          0.8745161890983582,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mk_f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.2848049610550194,\n        \"min\": 0.014568275668442836,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.8706983268260956,\n          0.8697362840175629,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"composite_eng\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.373306544635545,\n        \"min\": 0.048172671860576104,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.5418097830986603,\n          0.5267410710828564,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"composite_mk\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.3861948990442725,\n        \"min\": 0.03348814602663228,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.503693019216329,\n          0.5160002138707033,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"composite_total\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.379556709777187,\n        \"min\": 0.02831356501402043,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.5265630775457278,\n          0.5243598356953412,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-66ee7ee7-3387-4eb7-8abe-386e5b79369b\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample</th>\n",
       "      <th>eng_bleu</th>\n",
       "      <th>mk_bleu</th>\n",
       "      <th>eng_rouge</th>\n",
       "      <th>mk_rouge</th>\n",
       "      <th>eng_f1</th>\n",
       "      <th>mk_f1</th>\n",
       "      <th>composite_eng</th>\n",
       "      <th>composite_mk</th>\n",
       "      <th>composite_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.600000</td>\n",
       "      <td>0.102760</td>\n",
       "      <td>0.040998</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.200481</td>\n",
       "      <td>0.880977</td>\n",
       "      <td>0.870698</td>\n",
       "      <td>0.541810</td>\n",
       "      <td>0.503693</td>\n",
       "      <td>0.526563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.788876</td>\n",
       "      <td>0.078244</td>\n",
       "      <td>0.044402</td>\n",
       "      <td>0.084117</td>\n",
       "      <td>0.073555</td>\n",
       "      <td>0.019346</td>\n",
       "      <td>0.014568</td>\n",
       "      <td>0.048173</td>\n",
       "      <td>0.033488</td>\n",
       "      <td>0.028314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.179775</td>\n",
       "      <td>0.065574</td>\n",
       "      <td>0.855122</td>\n",
       "      <td>0.843806</td>\n",
       "      <td>0.486989</td>\n",
       "      <td>0.441575</td>\n",
       "      <td>0.468824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.250000</td>\n",
       "      <td>0.070480</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.212644</td>\n",
       "      <td>0.139139</td>\n",
       "      <td>0.868530</td>\n",
       "      <td>0.863022</td>\n",
       "      <td>0.513371</td>\n",
       "      <td>0.478186</td>\n",
       "      <td>0.520719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.500000</td>\n",
       "      <td>0.085833</td>\n",
       "      <td>0.032646</td>\n",
       "      <td>0.240105</td>\n",
       "      <td>0.232763</td>\n",
       "      <td>0.874516</td>\n",
       "      <td>0.869736</td>\n",
       "      <td>0.526741</td>\n",
       "      <td>0.516000</td>\n",
       "      <td>0.524360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>10.500000</td>\n",
       "      <td>0.150724</td>\n",
       "      <td>0.075738</td>\n",
       "      <td>0.299668</td>\n",
       "      <td>0.243902</td>\n",
       "      <td>0.894291</td>\n",
       "      <td>0.883230</td>\n",
       "      <td>0.564459</td>\n",
       "      <td>0.526708</td>\n",
       "      <td>0.550636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>0.251152</td>\n",
       "      <td>0.098555</td>\n",
       "      <td>0.440367</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.915800</td>\n",
       "      <td>0.890389</td>\n",
       "      <td>0.623024</td>\n",
       "      <td>0.537930</td>\n",
       "      <td>0.559457</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-66ee7ee7-3387-4eb7-8abe-386e5b79369b')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-66ee7ee7-3387-4eb7-8abe-386e5b79369b button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-66ee7ee7-3387-4eb7-8abe-386e5b79369b');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-bc3534ee-f4fe-474e-8ae0-3d30549ce6fe\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bc3534ee-f4fe-474e-8ae0-3d30549ce6fe')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-bc3534ee-f4fe-474e-8ae0-3d30549ce6fe button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "          Sample   eng_bleu    mk_bleu  eng_rouge   mk_rouge     eng_f1  \\\n",
       "count  10.000000  10.000000  10.000000  10.000000  10.000000  10.000000   \n",
       "mean    6.600000   0.102760   0.040998   0.269231   0.200481   0.880977   \n",
       "std     4.788876   0.078244   0.044402   0.084117   0.073555   0.019346   \n",
       "min     0.000000   0.000000   0.000000   0.179775   0.065574   0.855122   \n",
       "25%     2.250000   0.070480   0.000000   0.212644   0.139139   0.868530   \n",
       "50%     7.500000   0.085833   0.032646   0.240105   0.232763   0.874516   \n",
       "75%    10.500000   0.150724   0.075738   0.299668   0.243902   0.894291   \n",
       "max    13.000000   0.251152   0.098555   0.440367   0.277778   0.915800   \n",
       "\n",
       "           mk_f1  composite_eng  composite_mk  composite_total  \n",
       "count  10.000000      10.000000     10.000000        10.000000  \n",
       "mean    0.870698       0.541810      0.503693         0.526563  \n",
       "std     0.014568       0.048173      0.033488         0.028314  \n",
       "min     0.843806       0.486989      0.441575         0.468824  \n",
       "25%     0.863022       0.513371      0.478186         0.520719  \n",
       "50%     0.869736       0.526741      0.516000         0.524360  \n",
       "75%     0.883230       0.564459      0.526708         0.550636  \n",
       "max     0.890389       0.623024      0.537930         0.559457  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpt_prompt.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BAmakq-FaEGW"
   },
   "source": [
    "From a metric standpoint we are seeing improvement in our engineering answers, but a decay in the marketing answers.  Reviewing the marketing answers to the targets, we notice the following:\n",
    " - Every marketing answer is longer than its accompanying target.\n",
    " - There are specific references to papers, which is not present in the target answers.\n",
    "\n",
    "**Second Pass:** We will now modify the prompts to address these issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H3BEuxBdbPtt"
   },
   "outputs": [],
   "source": [
    "eng_rag_template = \"\"\"[INST]\n",
    "              Please provide an precise and concise answer to the engineer's question below based on the context information provided.\\n\\n\n",
    "              Below is a context:\\n{context}\\n\n",
    "              Below is a question:\\n{question}\\n\n",
    "              Below are answer instructions:\n",
    "- Keep your answer to one paragraph.\n",
    "- Answer with complete sentences or phrases. Do not use bullet points.\n",
    "- Do not explicitly reference papers in your answers.\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "mk_rag_template = \"\"\"[INST]\n",
    "              Please provide an precise and concise answer to the marketer's question below based on the context information provided.\\n\\n\n",
    "              Below is a context:\\n{context}\\n\n",
    "              Below is a question:\\n{question}\\n\n",
    "              Below are answer instructions:\n",
    "- Keep your answer to one paragraph.\n",
    "- Your answers need to be brief and to the point. Only focus on the most important aspects.\n",
    "- Answer with complete sentences or phrases. Do not use bullet points.\n",
    "- Do not explicitly reference papers in your answers.\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "eng_rag_prompt = ChatPromptTemplate.from_template(eng_rag_template)\n",
    "mk_rag_prompt = ChatPromptTemplate.from_template(mk_rag_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M7645Cnzbg_0",
    "outputId": "51a4078b-ba69-4e24-bc4e-4c7e54fb394d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Question: What purpose do large language models serve in the field of natural language processing?\n",
      " \n",
      "Engineering Prediction: Large language models serve as the foundation for many natural language processing applications, providing a versatile and adaptable tool for a wide range of tasks. These models are pre-trained on vast amounts of text data, enabling them to understand and generate human-like language. Their primary purpose is to act as a flexible language-understanding system that can be fine-tuned for specific tasks, such as classification, summarization, question-answering, and creative writing. This adaptability allows large language models to be applied to diverse domains and has led to their widespread adoption in the field of natural language processing.\n",
      "Engineering Answer: Large language models (LLMs) serve the purpose of enabling general-purpose language generation and other natural language processing tasks such as classification. They achieve this by learning statistical relationships from text documents during computationally intensive self-supervised and semi-supervised training. LLMs can be used for text generation by predicting the next token or word, making them valuable for tasks like speech recognition, machine translation, and information retrieval. Additionally, LLMs have superseded previous models like recurrent neural networks, showcasing their efficiency and effectiveness in NLP tasks.\n",
      "\n",
      "Marketing Prediction: Large language models serve as the foundation for many natural language processing applications. These models are first trained on vast amounts of text data to learn the underlying patterns and structures of human language. This enables them to generate human-like responses and perform a wide range of tasks, from classification and summarization to question-answering and creative writing. The process of fine-tuning allows these models to be adapted to specific downstream applications, making them versatile and powerful tools in the field of natural language processing.\n",
      "Marketing Answer: Large language models serve the purpose of improving performance in various natural language processing tasks, such as speech recognition, machine translation, natural language generation, optical character recognition, handwriting recognition, grammar induction, and information retrieval.\n",
      "------------------------------------------------------------\n",
      "Question: How does a large language model learn from text during training?\n",
      " \n",
      "Engineering Prediction: Large language models learn by analyzing vast amounts of text data and identifying patterns and relationships between words and phrases. This process, known as \"pre-training,\" involves training the model on a general language understanding task, such as predicting the next word in a sequence or filling in missing words. During pre-training, the model learns to understand the structure and semantics of language, which can then be applied to specific tasks through fine-tuning. Fine-tuning involves further training the model on a specific task, such as text classification or question answering, using labeled examples. This allows the model to adapt the knowledge it has gained during pre-training to perform well on the specific task at hand.\n",
      "Engineering Answer: A large language model learns from text during training by first going through an unsupervised generative 'pretraining' stage where it sets initial parameters using a language modeling objective. Then, it goes through a supervised discriminative 'fine-tuning' stage where it refines its parameters based on annotated examples or task demonstrations. This dual-stage approach allows the model to learn statistical relationships from text documents in a computationally intensive process, enabling it to achieve general-purpose language generation and natural language processing tasks.\n",
      "\n",
      "Marketing Prediction: Large language models are trained using a variety of methods, including pre-training on large text corpora and fine-tuning on specific tasks. During pre-training, models learn by predicting the next word in a sequence, which helps them understand the context and structure of language. Fine-tuning involves further training on specific tasks, such as question-answering or text generation, to adapt the model to a particular domain or task. This process allows the model to learn from text and improve its performance on specific language-related tasks.\n",
      "Marketing Answer: A large language model learns from text during training by first pretraining on a diverse dataset to acquire general language knowledge, and then fine-tuning on specific tasks or demonstrations to adapt its parameters for more targeted performance.\n",
      "------------------------------------------------------------\n",
      "Question: What are some key architectures behind the development of large language models?\n",
      " \n",
      "Engineering Prediction: Some key architectures that have facilitated the development of large language models include the use of neural networks, particularly Transformer-based language models. These models have been successfully applied to a range of QA tasks and have formed the foundation for more advanced language models. Additionally, the embrace of open and efficient foundation language models and the integration of massive APIs have further contributed to the advancement of large language models. These architectures provide the necessary framework and tools to process and generate human-like language, enabling the development of more sophisticated and capable language models.\n",
      "Engineering Answer: Key architectures behind the development of large language models include the use of self-attention mechanisms, such as those seen in Transformer decoders. These architectures have been applied to tasks like autoregressive language modeling and have led to the dominance of Transformer-based language models in NLP. Models like BERT and GPT-2 have further advanced this paradigm, showcasing the power of large Transformer language models in achieving state-of-the-art results across various NLP tasks. Additionally, architectures like neural-retriever-in-the-loop generative-based models have shown improvements in tasks like open-domain QA and knowledge-grounded dialogue, emphasizing the importance of consistent and engaging responses in long-form generation and multi-turn conversations.\n",
      "\n",
      "Marketing Prediction: Key architectures behind the development of large language models include Transformer-based language models and neural networks. These models have been successfully applied to a range of QA tasks, with a focus on general and scalable methods that can be applied to future AI systems. Additionally, these models often embrace an approach that leverages the power of large-scale general language models.\n",
      "Marketing Answer: Key architectures behind the development of large language models include Transformer-based models such as BERT and GPT-2, which utilize self-attention mechanisms for tasks like autoregressive language modeling and knowledge-grounded dialogue. These models have shown significant success in NLP tasks and have led to advancements in general-purpose language generation and natural language processing.\n",
      "------------------------------------------------------------\n",
      "Question: Can you name some specific large language models and the companies or organizations that have developed them?\n",
      " \n",
      "Engineering Prediction: Some prominent examples of large language models and their developers include GPT-3, developed by OpenAI, and BERT, developed by Google. Other well-known models are Tatsu Lab's Alpaca, and Facebook AI's BLENDER. These organizations have played a significant role in the advancement of language models, each with its unique features and capabilities, contributing to the ongoing progress in the field of language modeling.\n",
      "Engineering Answer: Some specific large language models include GPT-3 by OpenAI, Chinchilla by DeepMind, and BERT by Google. OpenAI developed GPT-3, DeepMind developed Chinchilla, and Google developed BERT. These models have been significant advancements in the field of natural language processing.\n",
      "\n",
      "Marketing Prediction: Some prominent large language models include GPT-3, developed by OpenAI, and BERT, developed by Google. Other well-known models are T5 and MegatronLM, created by Google AI and NVIDIA, respectively. These models have pushed the boundaries of natural language processing and have been adopted by numerous organizations for various applications.\n",
      "Marketing Answer: Chinchilla by DeepMind, GPT-3 by OpenAI.\n",
      "------------------------------------------------------------\n",
      "Question: What licensing models have been adopted for the distribution of source-available language models?\n",
      " \n",
      "Engineering Prediction: While there have been efforts to develop open and efficient foundation language models, such as Llama, the distribution of source-available language models is a complex issue due to the potential for harmful applications. As a result, there is a tension between broad access to these models and restricting their use to a select few organizations. Current language model systems are limited, and issues such as bias, toxicity, and hallucination are well-documented. To address these challenges, various techniques have been proposed, including pre-training data filtering, fine-tuning with additional data, and using a second language model to steer generation. However, the question of licensing models for source-available language models remains unresolved, and further research and discussion are needed to determine the best approach for balancing accessibility and responsible usage.\n",
      "Engineering Answer: Based on the provided context, it seems that licensing models for the distribution of source-available language models have not been explicitly discussed in the referenced papers. However, it is crucial to consider potential licensing options such as open-source licenses (e.g., GPL, MIT) or proprietary licenses when distributing language models to ensure legal compliance and control over usage rights. Additionally, considering the implications of different licensing models on accessibility, collaboration, and commercialization is essential for determining the most suitable approach for sharing language models with the community. Further research or consultation with legal experts may be necessary to explore specific licensing strategies for source-available language models.\n",
      "\n",
      "Marketing Prediction: There is a growing concern in the industry about the potential harm caused by Language Models (LMs) and their lack of regulation. As a result, there has been a push for proper licensing and restricted access to large language models to prevent misuse. This text does not explicitly mention specific licensing models, but it emphasizes the need for controlled distribution and access to mitigate potential risks associated with LLMs.\n",
      "Marketing Answer: Answer: Some organizations choose open-sourcing, while others restrict access to a few organizations with resources or offer end-to-end deployment via API.\n",
      "------------------------------------------------------------\n",
      "Question: What are language models and what is their purpose in natural language processing?\n",
      " \n",
      "Engineering Prediction: Language models are a fundamental component of Natural Language Processing (NLP), where they are used to map words and phrases to vectors of real numbers, a process known as embedding. These models are trained to predict the next word in a sequence, and this ability forms the basis for more complex language understanding and generation tasks. The ultimate goal is to create models that are helpful, honest, and harmless, and that can be aligned with human instructions and intentions.\n",
      "Engineering Answer: Language models are probabilistic models of natural language that help predict or correct text. Their purpose in natural language processing is to assist in various tasks such as speech recognition, machine translation, natural language generation, and information retrieval. By analyzing the performance of human subjects, language models improve the understanding and generation of human-like text.\n",
      "\n",
      "Marketing Prediction: Language models are a critical component of natural language processing (NLP), as they enable machines to understand and generate human language. These models use mathematical embeddings to map words and phrases to vectors, allowing them to predict the next word in a sequence, a task known as next-word prediction. This fundamental capability forms the basis for more complex language understanding and generation tasks, such as language translation, text summarization, and question-answering. The ultimate goal is to create models that are helpful, honest, and harmless, aligning with human values and expectations.\n",
      "Marketing Answer: Language models are probabilistic models of natural language that are used in tasks such as speech recognition, machine translation, and natural language generation in natural language processing.\n",
      "------------------------------------------------------------\n",
      "Question: How have language models evolved in terms of architecture, from the 1980s to present times?\n",
      " \n",
      "Engineering Prediction: Language models have evolved significantly since their inception in the 1980s, with a focus on improving their ability to understand and generate human language. Early language models relied on simple statistical methods and rule-based systems, but with the rise of neural networks and deep learning, these models have become increasingly sophisticated. The introduction of Transformer-based models, in particular, has revolutionized the field, allowing for context-aware language understanding and generation on a scale not previously possible. As a result, today's language models can process vast amounts of data, generating human-like responses and performing complex language tasks with ease. However, as these models continue to evolve and improve, there are also growing concerns about their potential misuse and negative societal impact.\n",
      "Engineering Answer: Language models have evolved significantly in terms of architecture from the 1980s to present times. In the 1980s, the first statistical language model was proposed, leading to experiments by IBM that identified areas for improvement by observing human subjects. However, it wasn't until 2017 when the transformer architecture was introduced by Google, revolutionizing the field. This development paved the way for models like BERT in 2018, which marked a shift towards large-scale transformer-based language models. These modern architectures, based on self-attention mechanisms, have dominated the field of natural language processing, achieving state-of-the-art performance in various tasks.\n",
      "\n",
      "Marketing Prediction: Language models have evolved significantly since the 1980s, with a focus on improving their ability to understand and generate human language. Early language models in the 1980s were based on simple statistical methods and rule-based systems, which had limited context understanding. The evolution of neural networks, and more recently, Transformer-based models, has revolutionized the field, allowing language models to process and generate language with far greater context and complexity, and a better understanding of user intentions. This has led to their widespread use in applications such as question answering and text generation. However, as language models become more sophisticated, there are also increasing concerns about their potential misuse, such as generating misleading or abusive content.\n",
      "Marketing Answer: Language models have evolved from early statistical models in the 1980s to modern transformer architectures, such as BERT and GPT-2, which use self-attention mechanisms and have become dominant in natural language processing tasks.\n",
      "------------------------------------------------------------\n",
      "Question: Can you explain how maximum entropy language models work and what the partition function signifies?\n",
      " \n",
      "Engineering Prediction: Maximum entropy language models aim to capture as much uncertainty as possible about the data, and the partition function, Z(x), plays a crucial role in this. It represents the sum of probabilities of all possible completions of a given context, ensuring the model's probabilities sum to 1. This function is challenging to estimate, but it allows the model to express uncertainty when it's unsure of the correct continuation, providing a more flexible and robust approach to language modeling.\n",
      "Engineering Answer: Maximum entropy language models use feature functions to encode the relationship between a word and its n-gram history, aiming to maximize reward while satisfying a KL-constrained objective. The partition function, denoted as Z(x), is crucial in normalizing the probabilities of all possible outputs given the input. It represents the sum of the exponential of the reward function over all possible output sequences, making it computationally expensive to estimate but essential for accurate modeling. The partition function ensures that the model's predicted probabilities sum up to 1, providing a foundation for effective language modeling.\n",
      "\n",
      "Marketing Prediction: Maximum entropy language models aim to capture as much uncertainty as possible, and the partition function is a normalization factor that ensures the probabilities of all possible outcomes sum to 1. In practice, estimating the partition function can be challenging, especially for complex models, as it requires summing over all possible completions of a given context, each weighted by the model's probability.\n",
      "Marketing Answer: Maximum entropy language models encode the relationship between a word and the n-gram history using feature functions. The partition function in this context represents the total probability of all possible outcomes, making it a crucial factor in determining the optimal solution for the reward maximization objective.\n",
      "------------------------------------------------------------\n",
      "Question: What is the benefit of using continuous space embeddings in recurrent neural network language models?\n",
      " \n",
      "Engineering Prediction: Continuous space embeddings in recurrent neural network language models offer a more efficient and effective way to represent words and their relationships. By mapping words to a continuous vector space, these embeddings capture semantic similarities and relationships between words, allowing the model to generalize and make predictions based on the context. This approach improves the model's ability to understand and generate human-like language, enhancing tasks such as sentiment analysis and machine translation.\n",
      "Engineering Answer: Continuous space embeddings in recurrent neural network language models help alleviate the curse of dimensionality by representing words as non-linear combinations of weights in the embedding space. This approach helps address the data sparsity problem caused by the exponential increase in possible word sequences with vocabulary size. By utilizing continuous space embeddings, neural networks can effectively capture semantic relationships and meaning within the language model.\n",
      "\n",
      "Marketing Prediction: Continuous space embeddings in recurrent neural network language models offer the advantage of transforming words into a lower-dimensional continuous vector space. This transformation captures semantic relationships between words, enabling better context understanding and more accurate predictions, especially for sentiment analysis and text classification tasks.\n",
      "Marketing Answer: Continuous space embeddings in recurrent neural network language models help alleviate the curse of dimensionality caused by the exponential increase in possible word sequences, reducing data sparsity issues.\n",
      "------------------------------------------------------------\n",
      "Question: What challenges do large language models face in mirroring human cognitive patterns?\n",
      " \n",
      "Engineering Prediction: Large language models face challenges in mirroring human cognitive patterns due to their propensity to generate toxic and biased responses, as well as their tendency to hallucinate answers. These issues highlight the need to further align language model behavior with human expectations and preferences. While efforts have been made to address these challenges, future work is needed to explore methods that can effectively decrease the models' tendency to generate undesirable responses and improve their ability to accurately reflect human cognitive processes across a diverse range of language tasks.\n",
      "Engineering Answer: Large language models face challenges in mirroring human cognitive patterns because they sometimes learn patterns that humans do not learn, while also failing to learn patterns that humans typically learn. This discrepancy suggests that the models may not be plausible cognitive models, despite matching human performance in some tasks. Further research is needed to address these limitations and improve the alignment of large language models with human cognitive patterns.\n",
      "\n",
      "Marketing Prediction: Large language models face challenges in mirroring human cognitive patterns due to their propensity to generate toxic and biased responses, as well as their tendency to hallucinate answers. These issues highlight the misalignment between the models' behavior and human expectations, emphasizing the need for further research and improvements to align their capabilities with what people truly desire.\n",
      "Marketing Answer: Large language models sometimes learn patterns that humans do not learn and fail to learn patterns that humans typically do learn.\n"
     ]
    }
   ],
   "source": [
    "eng_rag_chain = build_RAG_prompt_chain(eng_rag_template, llm_model, retriever, format_docs)\n",
    "mk_rag_chain = build_RAG_prompt_chain(mk_rag_template, llm_model, retriever, format_docs)\n",
    "\n",
    "metrics = ['rouge', 'bleu', 'bertscore']\n",
    "\n",
    "rag_chains = {\"engineering\": eng_rag_chain, \"marketing\": mk_rag_chain}\n",
    "\n",
    "dpt_prompt = evaluate(metrics, validation_questions_answers, rag_chains, iterations=10, verbose=False, dept_specific=True, sleep=False, print_results=True)\n",
    "dpt_prompt = composite_evaluation(dpt_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "E_2Qi5PCbi_E",
    "outputId": "26d713ef-ca08-4a82-a881-5699fde5da8c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"dpt_prompt\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"Sample\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.373295374522416,\n        \"min\": 0.0,\n        \"max\": 13.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          6.6,\n          7.5,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eng_bleu\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.5035184223935283,\n        \"min\": 0.0,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.09441640041462718,\n          0.09711441365002152,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mk_bleu\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.512089513588859,\n        \"min\": 0.0,\n        \"max\": 10.0,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          10.0,\n          0.06221711661757377,\n          0.08326083340574218\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eng_rouge\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.448372132151368,\n        \"min\": 0.08195361346839418,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.272868308164926,\n          0.24590705355036566,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mk_rouge\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.466381478203252,\n        \"min\": 0.06451612903225806,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.21876759359574546,\n          0.1929455992923485,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eng_f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.2804567327780307,\n        \"min\": 0.015444660969910944,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.8854985892772674,\n          0.8818718791007996,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mk_f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.2833327343500223,\n        \"min\": 0.015154376676796387,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.8760084629058837,\n          0.873887300491333,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"composite_eng\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.373267246354272,\n        \"min\": 0.04096274766620436,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.5434930671710371,\n          0.5392443783574115,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"composite_mk\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.3810552811085115,\n        \"min\": 0.04877683297618946,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.5160779328551802,\n          0.5002790940823867,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"composite_total\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.3766020854529315,\n        \"min\": 0.036277565604114166,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.5325270134446943,\n          0.5211460234575094,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-0ad5143f-ab08-4e6d-b0f1-c1e35a106028\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample</th>\n",
       "      <th>eng_bleu</th>\n",
       "      <th>mk_bleu</th>\n",
       "      <th>eng_rouge</th>\n",
       "      <th>mk_rouge</th>\n",
       "      <th>eng_f1</th>\n",
       "      <th>mk_f1</th>\n",
       "      <th>composite_eng</th>\n",
       "      <th>composite_mk</th>\n",
       "      <th>composite_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.600000</td>\n",
       "      <td>0.094416</td>\n",
       "      <td>0.062217</td>\n",
       "      <td>0.272868</td>\n",
       "      <td>0.218768</td>\n",
       "      <td>0.885499</td>\n",
       "      <td>0.876008</td>\n",
       "      <td>0.543493</td>\n",
       "      <td>0.516078</td>\n",
       "      <td>0.532527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.788876</td>\n",
       "      <td>0.068240</td>\n",
       "      <td>0.075487</td>\n",
       "      <td>0.081954</td>\n",
       "      <td>0.093901</td>\n",
       "      <td>0.015445</td>\n",
       "      <td>0.015154</td>\n",
       "      <td>0.040963</td>\n",
       "      <td>0.048777</td>\n",
       "      <td>0.036278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.189055</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>0.867752</td>\n",
       "      <td>0.852505</td>\n",
       "      <td>0.497641</td>\n",
       "      <td>0.445607</td>\n",
       "      <td>0.488332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.250000</td>\n",
       "      <td>0.043476</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.207168</td>\n",
       "      <td>0.160839</td>\n",
       "      <td>0.873349</td>\n",
       "      <td>0.865376</td>\n",
       "      <td>0.509190</td>\n",
       "      <td>0.483244</td>\n",
       "      <td>0.505423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.500000</td>\n",
       "      <td>0.097114</td>\n",
       "      <td>0.046431</td>\n",
       "      <td>0.245907</td>\n",
       "      <td>0.192946</td>\n",
       "      <td>0.881872</td>\n",
       "      <td>0.873887</td>\n",
       "      <td>0.539244</td>\n",
       "      <td>0.500279</td>\n",
       "      <td>0.521146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>10.500000</td>\n",
       "      <td>0.152868</td>\n",
       "      <td>0.083261</td>\n",
       "      <td>0.346076</td>\n",
       "      <td>0.296894</td>\n",
       "      <td>0.898585</td>\n",
       "      <td>0.886098</td>\n",
       "      <td>0.579225</td>\n",
       "      <td>0.546850</td>\n",
       "      <td>0.563950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>0.187549</td>\n",
       "      <td>0.208805</td>\n",
       "      <td>0.411215</td>\n",
       "      <td>0.369748</td>\n",
       "      <td>0.908937</td>\n",
       "      <td>0.898655</td>\n",
       "      <td>0.597204</td>\n",
       "      <td>0.602013</td>\n",
       "      <td>0.590950</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0ad5143f-ab08-4e6d-b0f1-c1e35a106028')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-0ad5143f-ab08-4e6d-b0f1-c1e35a106028 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-0ad5143f-ab08-4e6d-b0f1-c1e35a106028');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-98d535ef-9b92-4306-bbc5-be71358b1798\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-98d535ef-9b92-4306-bbc5-be71358b1798')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-98d535ef-9b92-4306-bbc5-be71358b1798 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "          Sample   eng_bleu    mk_bleu  eng_rouge   mk_rouge     eng_f1  \\\n",
       "count  10.000000  10.000000  10.000000  10.000000  10.000000  10.000000   \n",
       "mean    6.600000   0.094416   0.062217   0.272868   0.218768   0.885499   \n",
       "std     4.788876   0.068240   0.075487   0.081954   0.093901   0.015445   \n",
       "min     0.000000   0.000000   0.000000   0.189055   0.064516   0.867752   \n",
       "25%     2.250000   0.043476   0.000000   0.207168   0.160839   0.873349   \n",
       "50%     7.500000   0.097114   0.046431   0.245907   0.192946   0.881872   \n",
       "75%    10.500000   0.152868   0.083261   0.346076   0.296894   0.898585   \n",
       "max    13.000000   0.187549   0.208805   0.411215   0.369748   0.908937   \n",
       "\n",
       "           mk_f1  composite_eng  composite_mk  composite_total  \n",
       "count  10.000000      10.000000     10.000000        10.000000  \n",
       "mean    0.876008       0.543493      0.516078         0.532527  \n",
       "std     0.015154       0.040963      0.048777         0.036278  \n",
       "min     0.852505       0.497641      0.445607         0.488332  \n",
       "25%     0.865376       0.509190      0.483244         0.505423  \n",
       "50%     0.873887       0.539244      0.500279         0.521146  \n",
       "75%     0.886098       0.579225      0.546850         0.563950  \n",
       "max     0.898655       0.597204      0.602013         0.590950  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpt_prompt.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l9NaNAnwfRLI"
   },
   "source": [
    "These additional instructions improved the answers and metrics for both engineering and marketing.  However the marketing answers are still too long, and therefore too detailed and verbose.  \n",
    "\n",
    "**Third Pass:** We will further explore prompting to improve the marketing answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YxUmpvCTfph9"
   },
   "outputs": [],
   "source": [
    "eng_rag_template = \"\"\"[INST]\n",
    "              Please provide an precise and concise answer to the engineer's question below based on the context information provided.\\n\\n\n",
    "              Below is a context:\\n{context}\\n\n",
    "              Below is a question:\\n{question}\\n\n",
    "              Below are answer instructions:\n",
    "- Keep your answer to one paragraph.\n",
    "- Answer with complete sentences or phrases. Do not use bullet points.\n",
    "- Do not explicitly reference papers in your answer.\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "mk_rag_template = \"\"\"[INST]\n",
    "              Please provide an precise and concise answer to the marketer's question below based on the context information provided.\\n\\n\n",
    "              Below is a context:\\n{context}\\n\n",
    "              Below is a question:\\n{question}\\n\n",
    "              Below are answer instructions:\n",
    "- Tailor your answer for someone in marketing. Keep your answer high-level.\n",
    "- Keep your answer to one paragraph.\n",
    "- Your answer need to be brief and to the point. Only focus on the most important aspects.\n",
    "- Answer with complete sentences or phrases. Do not use bullet points.\n",
    "- Do not explicitly reference papers in your answer.\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "eng_rag_prompt = ChatPromptTemplate.from_template(eng_rag_template)\n",
    "mk_rag_prompt = ChatPromptTemplate.from_template(mk_rag_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aN5FH0NRf7g9",
    "outputId": "c0ab2247-3e7c-410f-82d7-822d52d75bf7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Question: What purpose do large language models serve in the field of natural language processing?\n",
      " \n",
      "Engineering Prediction: Large language models serve as the foundation for many natural language processing applications. These models are pre-trained on vast amounts of text data and can be adapted to various downstream tasks through fine-tuning. They excel at predicting the next word in a sequence, which forms the basis for more complex language tasks such as classification, summarization, question-answering, and creative writing. The ability to leverage large language models in this way has revolutionized the field of natural language processing, enabling the development of more sophisticated and versatile language-based applications.\n",
      "Engineering Answer: Large language models (LLMs) serve the purpose of enabling general-purpose language generation and other natural language processing tasks such as classification. They achieve this by learning statistical relationships from text documents during computationally intensive self-supervised and semi-supervised training. LLMs can be used for text generation by predicting the next token or word, making them valuable for tasks like speech recognition, machine translation, and information retrieval. Additionally, LLMs have superseded previous models like recurrent neural networks, showcasing their efficiency and effectiveness in NLP tasks.\n",
      "\n",
      "Marketing Prediction: Large language models are an essential tool in natural language processing, enabling machines to understand and generate human-like text. These models are trained on vast amounts of text data and can be adapted to various language tasks, such as classification, summarization, and question-answering. They serve as a foundation for building intelligent language-based applications, aiming to make interactions between humans and machines more natural and intuitive. By leveraging large language models, developers can create sophisticated language processing systems capable of comprehending complex linguistic nuances and generating responsive and contextually appropriate responses.\n",
      "Marketing Answer: Large language models serve the purpose of improving performance in various natural language processing tasks, such as speech recognition, machine translation, natural language generation, optical character recognition, handwriting recognition, grammar induction, and information retrieval.\n",
      "------------------------------------------------------------\n",
      "Question: How does a large language model learn from text during training?\n",
      " \n",
      "Engineering Prediction: Large language models learn by analyzing vast amounts of text data, identifying patterns, and developing a statistical understanding of language. During training, these models ingest massive text corpora and learn to predict the next word in a sequence, enabling them to grasp language structure and semantics. This process, known as pre-training, equips the models with foundational language understanding. Subsequently, fine-tuning involves further training on specific tasks, enhancing the model's performance on those tasks while retaining general language capabilities. The combination of pre-training and fine-tuning enables large language models to learn from text effectively and adapt to diverse linguistic challenges.\n",
      "Engineering Answer: A large language model learns from text during training by first going through an unsupervised generative 'pretraining' stage where it sets initial parameters using a language modeling objective. Then, it goes through a supervised discriminative 'fine-tuning' stage where it refines its parameters based on annotated examples or task demonstrations. This dual-stage approach allows the model to learn statistical relationships from text documents in a computationally intensive process, enabling it to achieve general-purpose language generation and natural language processing tasks.\n",
      "\n",
      "Marketing Prediction: Large language models are trained using vast amounts of text data and advanced machine learning techniques. During training, these models learn to understand and generate human-like language by analyzing patterns, contexts, and relationships within the text. This process involves breaking down text into smaller parts, learning from the structure and meaning, and then using this knowledge to generate responses or perform specific language tasks, such as translation or text completion. The models can be further fine-tuned for specific applications, but this requires significant computational resources.\n",
      "Marketing Answer: A large language model learns from text during training by first pretraining on a diverse dataset to acquire general language knowledge, and then fine-tuning on specific tasks or demonstrations to adapt its parameters for more targeted performance.\n",
      "------------------------------------------------------------\n",
      "Question: What are some key architectures behind the development of large language models?\n",
      " \n",
      "Engineering Prediction: Some key architectures that have facilitated the development of large language models include the use of neural networks, particularly Transformer-based language models. These models have been successfully applied to a range of QA tasks, demonstrating their effectiveness in processing and understanding natural language. Additionally, the embrace of open and efficient foundation language models and the integration of large language models with massive APIs have also contributed to the advancement of large language model architectures. These developments collectively enable the creation of more powerful and versatile language processing systems.\n",
      "Engineering Answer: Key architectures behind the development of large language models include the use of self-attention mechanisms, such as those seen in Transformer decoders. These architectures have been applied to tasks like autoregressive language modeling and have led to the dominance of Transformer-based language models in NLP. Models like BERT and GPT-2 have further advanced this paradigm, showcasing the power of large Transformer language models in achieving state-of-the-art results across various NLP tasks. Additionally, architectures like neural-retriever-in-the-loop generative-based models have shown improvements in tasks like open-domain QA and knowledge-grounded dialogue, emphasizing the importance of consistent and engaging responses in long-form generation and multi-turn conversations.\n",
      "\n",
      "Marketing Prediction: Large language models have seen great success due to their neural network-based architectures, particularly those based on Transformers. This architecture allows these models to process and understand vast amounts of text data, enabling them to generate human-like responses and perform complex language tasks. While there are various architectures designed for specific QA tasks, the focus is on creating general and scalable methods that can be applied to future AI systems, ensuring their longevity and adaptability.\n",
      "Marketing Answer: Key architectures behind the development of large language models include Transformer-based models such as BERT and GPT-2, which utilize self-attention mechanisms for tasks like autoregressive language modeling and knowledge-grounded dialogue. These models have shown significant success in NLP tasks and have led to advancements in general-purpose language generation and natural language processing.\n",
      "------------------------------------------------------------\n",
      "Question: Can you name some specific large language models and the companies or organizations that have developed them?\n",
      " \n",
      "Engineering Prediction: Some prominent examples of large language models and their developers include GPT-3, developed by OpenAI, and BERT, which was created by Google. Other well-known models are T5, created by Google AI, and RoBERTa, an open-source model built by Facebook AI Research and the University of Washington. These models have had a significant impact on the field of natural language processing and have influenced the development of many other language models.\n",
      "Engineering Answer: Some specific large language models include GPT-3 by OpenAI, Chinchilla by DeepMind, and BERT by Google. OpenAI developed GPT-3, DeepMind developed Chinchilla, and Google developed BERT. These models have been significant advancements in the field of natural language processing.\n",
      "\n",
      "Marketing Prediction: Some prominent examples of large language models and their developers include OpenAI's GPT-3, Google's BERT and PaLM, and Facebook AI's BLENDER. These organizations have been at the forefront of language model research and development, continuously pushing the boundaries of what these models can achieve.\n",
      "Marketing Answer: Chinchilla by DeepMind, GPT-3 by OpenAI.\n",
      "------------------------------------------------------------\n",
      "Question: What licensing models have been adopted for the distribution of source-available language models?\n",
      " \n",
      "Engineering Prediction: There has been a recent trend of releasing large language models under open-source licenses, which has sparked concerns about the potential for misuse. To address these concerns, some models are released with specific licenses that allow for usage within certain constraints, such as limiting commercial usage or requiring attribution. These licenses aim to balance the benefits of open access with responsible usage, ensuring that the models are not utilized for harmful applications.\n",
      "Engineering Answer: Based on the provided context, it seems that licensing models for the distribution of source-available language models have not been explicitly discussed in the referenced papers. However, it is crucial to consider potential licensing options such as open-source licenses (e.g., GPL, MIT) or proprietary licenses when distributing language models to ensure legal compliance and control over usage rights. Additionally, considering the implications of different licensing models on accessibility, collaboration, and commercialization is essential for determining the most suitable approach for sharing language models with the community. Further research or consultation with legal experts may be necessary to explore specific licensing strategies for source-available language models.\n",
      "\n",
      "Marketing Prediction: There has been a recent shift towards open and efficient foundation language models, with some developers opting for source-available licensing. This approach aims to balance accessibility and responsible usage, addressing concerns around bias and harmful applications. By making the source code available, developers encourage collaboration and independent audits while retaining some control over the model's distribution. This strategy aims to foster trust and encourage wider adoption, ensuring language models are accessible to a broader range of users and organizations.\n",
      "Marketing Answer: Answer: Some organizations choose open-sourcing, while others restrict access to a few organizations with resources or offer end-to-end deployment via API.\n",
      "------------------------------------------------------------\n",
      "Question: What are language models and what is their purpose in natural language processing?\n",
      " \n",
      "Engineering Prediction: Language models are a fundamental component of Natural Language Processing (NLP), where they are used to map words and phrases to vectors of real numbers, a process known as embedding. These models are trained to predict the next word in a sequence, which is a proxy for their ultimate purpose: to be helpful, honest, and harmless assistants that follow instructions and perform tasks as humans want them to.\n",
      "Engineering Answer: Language models are probabilistic models of natural language that help predict or correct text. Their purpose in natural language processing is to assist in various tasks such as speech recognition, machine translation, natural language generation, and information retrieval. By analyzing the performance of human subjects, language models improve the understanding and generation of human-like text.\n",
      "\n",
      "Marketing Prediction: Language models are a critical component of Natural Language Processing (NLP), which is a field of artificial intelligence that focuses on the interaction between computers and human language. These models use mathematical techniques to map words and phrases to vectors of real numbers, a process known as embedding. The purpose of language models is to help machines understand and generate human language by predicting the next word in a sequence, a task known as next-word prediction. This ability to process and generate language is fundamental to many NLP applications, such as language translation, text generation, and question-answering systems. By improving the accuracy and capabilities of language models, researchers aim to enhance the overall performance and usefulness of NLP technologies in a variety of real-world applications.\n",
      "Marketing Answer: Language models are probabilistic models of natural language that are used in tasks such as speech recognition, machine translation, and natural language generation in natural language processing.\n",
      "------------------------------------------------------------\n",
      "Question: How have language models evolved in terms of architecture, from the 1980s to present times?\n",
      " \n",
      "Engineering Prediction: Language models have evolved significantly since their inception in the 1980s, with a focus on improving their ability to understand and generate human language. Early language models relied on simple statistical methods and rule-based systems, but with the rise of machine learning and deep learning, these models have become increasingly sophisticated. The introduction of neural networks, particularly the Transformer architecture, has revolutionized language modeling, allowing models to capture long-range dependencies in text and generate more coherent and contextually appropriate responses. This has led to the development of large language models with billions of parameters, capable of performing a wide range of tasks, from machine translation to question-answering and text generation. However, as language models become more powerful, there are also increasing concerns about their potential misuse and negative societal impact.\n",
      "Engineering Answer: Language models have evolved significantly in terms of architecture from the 1980s to present times. In the 1980s, the first statistical language model was proposed, leading to experiments by IBM that identified areas for improvement by observing human subjects. However, it wasn't until 2017 when the transformer architecture was introduced by Google, revolutionizing the field. This development paved the way for models like BERT in 2018, which marked a shift towards large-scale transformer-based language models. These modern architectures, based on self-attention mechanisms, have dominated the field of natural language processing, achieving state-of-the-art performance in various tasks.\n",
      "\n",
      "Marketing Prediction: Language models have evolved significantly since the 1980s, with a focus on improving their ability to understand and generate human language. Early language models in the 1980s were based on simple statistical methods and rule-based systems, which had limited context understanding. Over time, language models incorporated neural networks, with the Transformer architecture being a significant milestone in the 2010s, enabling models to process longer sequences of text and capture complex linguistic patterns. The latest language models, such as those discussed in the provided context, aim to improve their ability to follow user intentions and constraints, making them more flexible and powerful tools for a variety of language-based tasks, but also raising concerns about potential misuse.\n",
      "Marketing Answer: Language models have evolved from early statistical models in the 1980s to modern transformer architectures, such as BERT and GPT-2, which use self-attention mechanisms and have become dominant in natural language processing tasks.\n",
      "------------------------------------------------------------\n",
      "Question: Can you explain how maximum entropy language models work and what the partition function signifies?\n",
      " \n",
      "Engineering Prediction: Maximum entropy language models aim to capture as much uncertainty as possible about the data, and the partition function, Z(x), plays a crucial role in this. It represents the sum of probabilities of all possible completions of a given context, ensuring the model's probabilities sum to 1. This function helps the model make probabilistic predictions by normalizing the unnormalized scores it assigns to different outcomes. Maximizing entropy encourages the model to distribute probability mass evenly across all possible outcomes, indicating high uncertainty. The partition function is often intractable to compute exactly, especially for large language models, so various approximation methods are used to estimate it.\n",
      "Engineering Answer: Maximum entropy language models use feature functions to encode the relationship between a word and its n-gram history, aiming to maximize reward while satisfying a KL-constrained objective. The partition function, denoted as Z(x), is crucial in normalizing the probabilities of all possible outputs given the input. It represents the sum of the exponential of the reward function over all possible output sequences, making it computationally expensive to estimate but essential for accurate modeling. The partition function ensures that the model's predicted probabilities sum up to 1, providing a foundation for effective language modeling.\n",
      "\n",
      "Marketing Prediction: Maximum entropy language models aim to capture as much uncertainty as possible, which is reflected in the models' predictions. The partition function, in this context, normalizes the probability distribution over potential predictions, ensuring that the probabilities sum up to 1. By doing so, it provides a measure of the model's confidence in its predictions, with higher entropy indicating higher uncertainty. This approach allows language models to make more informed decisions by considering multiple possibilities and their associated probabilities.\n",
      "Marketing Answer: Maximum entropy language models encode the relationship between a word and the n-gram history using feature functions. The partition function in this context represents the total probability of all possible outcomes, making it a crucial factor in determining the optimal solution for the reward maximization objective.\n",
      "------------------------------------------------------------\n",
      "Question: What is the benefit of using continuous space embeddings in recurrent neural network language models?\n",
      " \n",
      "Engineering Prediction: Continuous space embeddings in recurrent neural network language models offer a more efficient and effective way to represent words and their relationships. By mapping words to a continuous vector space, these embeddings capture semantic similarities and relationships between words, allowing the model to generalize and make predictions based on the context. This approach improves the model's ability to understand and generate human-like language, enhancing tasks such as sentiment analysis and machine translation.\n",
      "Engineering Answer: Continuous space embeddings in recurrent neural network language models help alleviate the curse of dimensionality by representing words as non-linear combinations of weights in the embedding space. This approach helps address the data sparsity problem caused by the exponential increase in possible word sequences with vocabulary size. By utilizing continuous space embeddings, neural networks can effectively capture semantic relationships and meaning within the language model.\n",
      "\n",
      "Marketing Prediction: Continuous space embeddings offer a powerful tool for marketers seeking to unlock the potential of recurrent neural network language models. By mapping words to a continuous vector space, these embeddings capture the semantic relationships between words, enabling more effective sentiment analysis and context understanding. This approach enhances the ability to interpret and generate human-like language, resulting in more accurate and nuanced predictions, and ultimately, more effective marketing strategies.\n",
      "Marketing Answer: Continuous space embeddings in recurrent neural network language models help alleviate the curse of dimensionality caused by the exponential increase in possible word sequences, reducing data sparsity issues.\n",
      "------------------------------------------------------------\n",
      "Question: What challenges do large language models face in mirroring human cognitive patterns?\n",
      " \n",
      "Engineering Prediction: Large language models face challenges in mirroring human cognitive patterns due to their propensity to generate toxic and biased responses, as well as their tendency to hallucinate answers. These issues highlight the need to further align language model behavior with human expectations and preferences. While these models have demonstrated impressive performance across various language tasks, their limitations in understanding and generating human-like responses remain a significant area of focus for future research and improvement.\n",
      "Engineering Answer: Large language models face challenges in mirroring human cognitive patterns because they sometimes learn patterns that humans do not learn, while also failing to learn patterns that humans typically learn. This discrepancy suggests that the models may not be plausible cognitive models, despite matching human performance in some tasks. Further research is needed to address these limitations and improve the alignment of large language models with human cognitive patterns.\n",
      "\n",
      "Marketing Prediction: Large language models face challenges in mirroring human cognitive patterns due to their propensity to generate toxic and biased responses, as well as their tendency to hallucinate answers. These issues can be mitigated but are not yet fully resolved, and further research is needed to align language model behavior with human expectations. This includes exploring methods to decrease undesirable responses and improve performance on a wide range of tasks, from creative writing to question-answering. The goal is to make these models more reliable and trustworthy for human users.\n",
      "Marketing Answer: Large language models sometimes learn patterns that humans do not learn and fail to learn patterns that humans typically do learn.\n"
     ]
    }
   ],
   "source": [
    "eng_rag_chain = build_RAG_prompt_chain(eng_rag_template, llm_model, retriever, format_docs)\n",
    "mk_rag_chain = build_RAG_prompt_chain(mk_rag_template, llm_model, retriever, format_docs)\n",
    "\n",
    "metrics = ['rouge', 'bleu', 'bertscore']\n",
    "\n",
    "rag_chains = {\"engineering\": eng_rag_chain, \"marketing\": mk_rag_chain}\n",
    "\n",
    "dpt_prompt = evaluate(metrics, validation_questions_answers, rag_chains, iterations=10, verbose=False, dept_specific=True, sleep=False, print_results=True)\n",
    "dpt_prompt = composite_evaluation(dpt_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "_MjfpwlPgA70",
    "outputId": "9705b37f-4244-4d2e-d6c5-6ebc596dbc49"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"dpt_prompt\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"Sample\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.373295374522416,\n        \"min\": 0.0,\n        \"max\": 13.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          6.6,\n          7.5,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eng_bleu\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.5051232194117063,\n        \"min\": 0.0,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.09118061573324213,\n          0.11033313381612268,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mk_bleu\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.5261865945070903,\n        \"min\": 0.0,\n        \"max\": 10.0,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          10.0,\n          0.024988877003386683,\n          0.07653242626496376\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eng_rouge\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.4527432339777726,\n        \"min\": 0.08129707007698754,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.2590002249992641,\n          0.2429775280898876,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mk_rouge\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.4803871776301043,\n        \"min\": 0.0589674786263985,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.17590108370582447,\n          0.19268478536957073,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eng_f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.2808785040845145,\n        \"min\": 0.015154683238815822,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.8835038900375366,\n          0.8791722357273102,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mk_f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.285920663073737,\n        \"min\": 0.008975515713692643,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.8676529407501221,\n          0.8702396750450134,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"composite_eng\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.3744783666595306,\n        \"min\": 0.04331574288779294,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.537688135665196,\n          0.540679440210523,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"composite_mk\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.389526791007078,\n        \"min\": 0.026986185480800944,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.4915945708874857,\n          0.49437851147344036,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"composite_total\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.381697830671502,\n        \"min\": 0.02706103328311609,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.5192507097541118,\n          0.5194896635518134,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-a6c7d125-74e6-41fe-9e61-4f22c7f2b35c\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample</th>\n",
       "      <th>eng_bleu</th>\n",
       "      <th>mk_bleu</th>\n",
       "      <th>eng_rouge</th>\n",
       "      <th>mk_rouge</th>\n",
       "      <th>eng_f1</th>\n",
       "      <th>mk_f1</th>\n",
       "      <th>composite_eng</th>\n",
       "      <th>composite_mk</th>\n",
       "      <th>composite_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.600000</td>\n",
       "      <td>0.091181</td>\n",
       "      <td>0.024989</td>\n",
       "      <td>0.259000</td>\n",
       "      <td>0.175901</td>\n",
       "      <td>0.883504</td>\n",
       "      <td>0.867653</td>\n",
       "      <td>0.537688</td>\n",
       "      <td>0.491595</td>\n",
       "      <td>0.519251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.788876</td>\n",
       "      <td>0.073101</td>\n",
       "      <td>0.032995</td>\n",
       "      <td>0.081297</td>\n",
       "      <td>0.058967</td>\n",
       "      <td>0.015155</td>\n",
       "      <td>0.008976</td>\n",
       "      <td>0.043316</td>\n",
       "      <td>0.026986</td>\n",
       "      <td>0.027061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.164835</td>\n",
       "      <td>0.072727</td>\n",
       "      <td>0.865019</td>\n",
       "      <td>0.853006</td>\n",
       "      <td>0.483224</td>\n",
       "      <td>0.448321</td>\n",
       "      <td>0.473266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.250000</td>\n",
       "      <td>0.014269</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.192250</td>\n",
       "      <td>0.140231</td>\n",
       "      <td>0.874698</td>\n",
       "      <td>0.861170</td>\n",
       "      <td>0.497820</td>\n",
       "      <td>0.471076</td>\n",
       "      <td>0.498220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.500000</td>\n",
       "      <td>0.110333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.242978</td>\n",
       "      <td>0.192685</td>\n",
       "      <td>0.879172</td>\n",
       "      <td>0.870240</td>\n",
       "      <td>0.540679</td>\n",
       "      <td>0.494379</td>\n",
       "      <td>0.519490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>10.500000</td>\n",
       "      <td>0.149372</td>\n",
       "      <td>0.052693</td>\n",
       "      <td>0.306723</td>\n",
       "      <td>0.209895</td>\n",
       "      <td>0.896641</td>\n",
       "      <td>0.874439</td>\n",
       "      <td>0.569333</td>\n",
       "      <td>0.509681</td>\n",
       "      <td>0.543573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>0.173959</td>\n",
       "      <td>0.076532</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.253968</td>\n",
       "      <td>0.908103</td>\n",
       "      <td>0.879368</td>\n",
       "      <td>0.609770</td>\n",
       "      <td>0.527218</td>\n",
       "      <td>0.553119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a6c7d125-74e6-41fe-9e61-4f22c7f2b35c')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-a6c7d125-74e6-41fe-9e61-4f22c7f2b35c button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-a6c7d125-74e6-41fe-9e61-4f22c7f2b35c');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-285653a7-f50c-4805-8f64-b77be8ee1eea\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-285653a7-f50c-4805-8f64-b77be8ee1eea')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-285653a7-f50c-4805-8f64-b77be8ee1eea button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "          Sample   eng_bleu    mk_bleu  eng_rouge   mk_rouge     eng_f1  \\\n",
       "count  10.000000  10.000000  10.000000  10.000000  10.000000  10.000000   \n",
       "mean    6.600000   0.091181   0.024989   0.259000   0.175901   0.883504   \n",
       "std     4.788876   0.073101   0.032995   0.081297   0.058967   0.015155   \n",
       "min     0.000000   0.000000   0.000000   0.164835   0.072727   0.865019   \n",
       "25%     2.250000   0.014269   0.000000   0.192250   0.140231   0.874698   \n",
       "50%     7.500000   0.110333   0.000000   0.242978   0.192685   0.879172   \n",
       "75%    10.500000   0.149372   0.052693   0.306723   0.209895   0.896641   \n",
       "max    13.000000   0.173959   0.076532   0.421053   0.253968   0.908103   \n",
       "\n",
       "           mk_f1  composite_eng  composite_mk  composite_total  \n",
       "count  10.000000      10.000000     10.000000        10.000000  \n",
       "mean    0.867653       0.537688      0.491595         0.519251  \n",
       "std     0.008976       0.043316      0.026986         0.027061  \n",
       "min     0.853006       0.483224      0.448321         0.473266  \n",
       "25%     0.861170       0.497820      0.471076         0.498220  \n",
       "50%     0.870240       0.540679      0.494379         0.519490  \n",
       "75%     0.874439       0.569333      0.509681         0.543573  \n",
       "max     0.879368       0.609770      0.527218         0.553119  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpt_prompt.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GW_TUDcji1z2"
   },
   "source": [
    "At this point we seem to get getting diminishing returns on the additional instructions provided to the model. Considering that this additional instructions seems to have provided no additional value, and it deteriorated our evaluation metric, we will revert back to the previous prompt.\n",
    "\n",
    "As a final experimentation of prompt testing, we are going to explore the extreme of providing extensive instructions and direction for the model on what to focus on to see how different the results become."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OTjdHbS768OW"
   },
   "source": [
    "##### **Detailed instructions by department**\n",
    "\n",
    "**First Pass:** Below we explore the extreme case of detailed instructions for the model to adhere to in developing its answer.  Because we are struggling with improving the marketing responses, we are specifically focusing on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "va01OAfUymdT"
   },
   "outputs": [],
   "source": [
    "eng_rag_template = \"\"\"[INST]\n",
    "              Please provide an precise and concise answer to the engineer's question below based on the context information provided.\\n\\n\n",
    "              Below is a context:\\n{context}\\n\n",
    "              Below is a question:\\n{question}\\n\n",
    "              Below are answer instructions:\n",
    "- Formatting: Answers need to be in paragraph format. Do not use bullet points. Do not explicitly reference papers in your answer.\n",
    "- Comprehensive Explanations: Provide a detailed and comprehensive explanation of the topic at hand, reflecting a depth of understanding and research. Focus on covering the subject matter thoroughly.\n",
    "- Technical Detail: Include technical details and terminologies that relate to the question.\n",
    "- Research Focus: Orient answers towards the research aspects of the questions.\n",
    "- Objective Tone: Maintain an objective and informative tone, aiming to educate the reader without persuasive language.\n",
    "- Forward-Looking Insights: Include insights into the future direction of the topic at hand.\n",
    "- Addressing Broad Implications: Go beyond the technical details to address broader implications, including ethical considerations, societal impacts, and practical applications.\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "mk_rag_template = \"\"\"[INST]\n",
    "              Please provide a precise and concise answer to the marketer's question below based on the context provided.\\n\\n\n",
    "              Below is a context:\\n{context}\\n\n",
    "              Below is a question:\\n{question}\\n\n",
    "              Below are answer instructions:\n",
    "- Formatting: Answers need to be in paragraph format. Do not use bullet points. Do not explicitly reference papers in your answer.\n",
    "- Succinctness: Make sure your answer is concise and to the point. Provide essential information without delving into the technical depth. Answer in the least amount of sentences possible. Answer with a single sentence where possible.\n",
    "- Broad Overview: Give a broad overview of the topic, focusing on the practical applications and implications. Aim to communicate the value and utility for a wider audience.\n",
    "- Focus on Applications: Emphasize the applications of the topic at hand. Focus on real-world uses and benefits and highlight how technology can solve problems or create opportunities.\n",
    "- Accessibility: The language and presentation of information needs to be accessible to lay audiences. This means less jargon and a more straightforward explanation, making it easier for individuals without a technical background to understand.\n",
    "- Promotional Tone: While still informative, the answer should have a subtly promotional tone, aiming to generate interest or enthusiasm for LLMs and their potential.\n",
    "- User-Oriented: Make the answer user-oriented, considering the interests and needs of potential users or stakeholders. Frame the information in a way that relates directly to how individuals or organizations might use or benefit from LLMs.\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "eng_rag_prompt = ChatPromptTemplate.from_template(eng_rag_template)\n",
    "mk_rag_prompt = ChatPromptTemplate.from_template(mk_rag_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tHEP66ebCP-X",
    "outputId": "2c89e1e6-b577-467b-a307-22b53db25f66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Question: What purpose do large language models serve in the field of natural language processing?\n",
      " \n",
      "Engineering Prediction: Large language models serve as the foundation for advanced natural language processing (NLP) applications, offering a versatile and powerful tool for a wide range of tasks. These models are designed to understand and generate human language, capturing the intricacies of language structure and semantics. With their ability to process vast amounts of text data, they have revolutionized NLP research and development. \n",
      "\n",
      "At their core, large language models are trained on massive text corpora, learning to predict the next word in a sequence, which is known as the next word prediction objective. This foundational capability enables the models to grasp language patterns, syntax, and context, forming the basis for more complex language understanding and generation tasks. \n",
      "\n",
      "The versatility of these models lies in their adaptability to diverse downstream applications. Through fine-tuning, researchers and developers can customize these large-scale models for specific tasks, such as text classification, question-answering, summarization, and dialogue generation. This process involves further training the pre-trained language model on task-specific data, enabling it to specialize and perform optimally in that particular domain. \n",
      "\n",
      "The impact of large language models is significant, with applications across industries. They enhance language translation services, power virtual assistants, facilitate information retrieval, and enable automated customer service, among numerous other use cases. However, alongside their benefits, ethical considerations and potential misuse are important aspects to address. As highlighted in the context, certain high-stakes domains, such as medical diagnoses or legal advice, require careful deployment or even exclusion of these models due to the potential consequences of errors or biases present in the data on which they are trained. \n",
      "\n",
      "Looking forward, research in this field continues to advance, aiming to enhance the capabilities of large language models while addressing their limitations. This includes improving their understanding of context, generating more coherent and contextually appropriate responses, and mitigating biases present in training data. The future of large language models in NLP promises exciting developments, driving the creation of increasingly sophisticated and human-like language applications.\n",
      "Engineering Answer: Large language models (LLMs) serve the purpose of enabling general-purpose language generation and other natural language processing tasks such as classification. They achieve this by learning statistical relationships from text documents during computationally intensive self-supervised and semi-supervised training. LLMs can be used for text generation by predicting the next token or word, making them valuable for tasks like speech recognition, machine translation, and information retrieval. Additionally, LLMs have superseded previous models like recurrent neural networks, showcasing their efficiency and effectiveness in NLP tasks.\n",
      "\n",
      "Marketing Prediction: Large language models are an essential tool in the field of natural language processing, enabling machines to understand and generate human-like text with a wide range of applications, from creative writing to medical diagnoses.\n",
      "Marketing Answer: Large language models serve the purpose of improving performance in various natural language processing tasks, such as speech recognition, machine translation, natural language generation, optical character recognition, handwriting recognition, grammar induction, and information retrieval.\n",
      "------------------------------------------------------------\n",
      "Question: How does a large language model learn from text during training?\n",
      " \n",
      "Engineering Prediction: Large language models, such as the ones described in the context, are trained using vast amounts of text data and advanced machine learning techniques. During training, these models learn to recognize patterns, understand context, and generate responses by analyzing the provided text. The process typically involves two main phases: pre-training and fine-tuning. \n",
      "\n",
      "In the pre-training phase, the model is exposed to a massive corpus of text data, often consisting of billions or even trillions of words. This phase is crucial for the model to acquire a general understanding of language structure and semantics. The model learns to predict the next word in a sequence, known as language modeling, which helps it capture the statistical relationships between words and build a basic understanding of syntax and semantics. \n",
      "\n",
      "The fine-tuning phase is where the model is adapted to specific tasks or domains. This involves training the pre-trained model on task-specific datasets to learn specialized language patterns and context relevant to the task at hand. For example, if the goal is to develop a model for machine translation, the fine-tuning phase would involve training the model on parallel text data in the source and target languages. \n",
      "\n",
      "The training process often involves deep neural networks, particularly a type of architecture called Transformers, which have revolutionized the field of natural language processing. These models can process long sequences of text and capture complex linguistic patterns due to their self-attention mechanisms. During training, the model's parameters are adjusted through a process known as backpropagation, where the model's predictions are compared to the correct labels, and the errors are propagated back through the network to update the weights. \n",
      "\n",
      "The research in this field is focused on improving the efficiency and effectiveness of training large language models. This includes exploring new pre-training objectives, such as text-to-text tasks, developing more efficient fine-tuning techniques, and addressing challenges related to model size and computational requirements. \n",
      "\n",
      "As large language models continue to evolve, there are important ethical and societal considerations to keep in mind. These models can inadvertently capture and amplify biases present in the training data, leading to unfair or discriminatory behavior. Additionally, the environmental impact of training such models is significant, and researchers are exploring ways to make the training process more energy-efficient and sustainable. \n",
      "\n",
      "In conclusion, large language models learn from text during training by leveraging massive amounts of data, advanced machine learning techniques, and architectural innovations like Transformers. The research community is continuously working to improve the effectiveness and accessibility of these models while also addressing the ethical and societal implications that arise from their widespread adoption.\n",
      "Engineering Answer: A large language model learns from text during training by first going through an unsupervised generative 'pretraining' stage where it sets initial parameters using a language modeling objective. Then, it goes through a supervised discriminative 'fine-tuning' stage where it refines its parameters based on annotated examples or task demonstrations. This dual-stage approach allows the model to learn statistical relationships from text documents in a computationally intensive process, enabling it to achieve general-purpose language generation and natural language processing tasks.\n",
      "\n",
      "Marketing Prediction: Large language models are trained using text data to learn and generate human-like responses. This process involves feeding vast amounts of text data into the model, allowing it to understand patterns, context, and language nuances. This enables them to assist with a wide range of tasks, from generating creative content to providing customer support, and even helping marketers craft compelling messages. With their ability to process and understand natural language, large language models are revolutionizing the way we interact with technology, making our lives easier and more efficient.\n",
      "Marketing Answer: A large language model learns from text during training by first pretraining on a diverse dataset to acquire general language knowledge, and then fine-tuning on specific tasks or demonstrations to adapt its parameters for more targeted performance.\n",
      "------------------------------------------------------------\n",
      "Question: What are some key architectures behind the development of large language models?\n",
      " \n",
      "Engineering Prediction: Large language models have become increasingly sophisticated, and their development has been influenced by several key architectural innovations. One prominent architecture is the Transformer model, which has been pivotal in advancing the capabilities of language models. The Transformer architecture utilizes self-attention mechanisms, enabling the model to weigh the importance of different words in a sentence and capture long-range dependencies effectively. This has led to significant improvements in language understanding and generation tasks. \n",
      "\n",
      "Another critical aspect is the adoption of large-scale pre-training and fine-tuning regimes. By pre-training models on vast amounts of text data, they learn rich representations of language, which can then be adapted to specific tasks through fine-tuning. This approach has proven effective in enhancing the performance and generalizability of language models. Additionally, the integration of neural networks, particularly deep learning techniques, has been instrumental in the evolution of language models. These networks enable models to learn complex patterns and relationships within text data, improving their comprehension and generation capabilities. \n",
      "\n",
      "The future of large language model development is likely to involve further architectural innovations aimed at improving efficiency and effectiveness. There is a growing emphasis on creating models that can process longer contexts and incorporate external knowledge or interact with other models or APIs to enhance their understanding and generate more informed responses. Ethical considerations and societal impacts are also important aspects to address, ensuring that these powerful models are developed and deployed responsibly, with considerations for privacy, fairness, and transparency.\n",
      "Engineering Answer: Key architectures behind the development of large language models include the use of self-attention mechanisms, such as those seen in Transformer decoders. These architectures have been applied to tasks like autoregressive language modeling and have led to the dominance of Transformer-based language models in NLP. Models like BERT and GPT-2 have further advanced this paradigm, showcasing the power of large Transformer language models in achieving state-of-the-art results across various NLP tasks. Additionally, architectures like neural-retriever-in-the-loop generative-based models have shown improvements in tasks like open-domain QA and knowledge-grounded dialogue, emphasizing the importance of consistent and engaging responses in long-form generation and multi-turn conversations.\n",
      "\n",
      "Marketing Prediction: Large language models have seen tremendous success and development in recent years, with a key focus on practical, real-world applications. These models are designed to assist users with a wide range of tasks, offering efficient and effective solutions to problems. The technology behind these models is constantly evolving, with neural networks and transformer-based language models being a key focus, offering scalable and adaptable solutions. This promotional tone is important to generate enthusiasm and interest in the potential of LLMs. The applications are vast, from efficient customer service and content creation to accurate medical diagnoses and innovative product development. The user-centric approach ensures that the models are designed with the needs and interests of individuals and organizations, offering tailored solutions and creating a revolutionary impact on our daily lives.\n",
      "Marketing Answer: Key architectures behind the development of large language models include Transformer-based models such as BERT and GPT-2, which utilize self-attention mechanisms for tasks like autoregressive language modeling and knowledge-grounded dialogue. These models have shown significant success in NLP tasks and have led to advancements in general-purpose language generation and natural language processing.\n",
      "------------------------------------------------------------\n",
      "Question: Can you name some specific large language models and the companies or organizations that have developed them?\n",
      " \n",
      "Engineering Prediction: Some prominent examples of large language models and their developers include GPT-3, created by OpenAI, and BERT, developed by Google. These models have had a significant impact on the field of Natural Language Processing (NLP) and have been widely adopted by researchers and practitioners. Another well-known model is T5, or Text-to-Text Transfer Transformer, introduced by Google AI, which is a large language model that adopts a text-to-text framework that converts all text-based language tasks into a text generation problem. \n",
      "\n",
      "Large language models have also been developed by universities and research institutions. For example, the University of Washington's Allen Institute for Artificial Intelligence created the Aristo model, which is designed to answer science questions, and the University of Toronto's Vector Institute developed the CANINE model for code generation and programming tasks. \n",
      "\n",
      "The development of large language models is an active area of research and development, with new models and techniques emerging frequently. These models have the potential to revolutionize various industries and fields, from healthcare and education to customer service and content creation. However, it is essential to consider the ethical implications and potential risks associated with this technology, including issues of bias, privacy, and misuse. The field must continue to develop methods for mitigating these risks while also ensuring that the benefits of large language models are accessible and beneficial to a broad range of users and communities.\n",
      "Engineering Answer: Some specific large language models include GPT-3 by OpenAI, Chinchilla by DeepMind, and BERT by Google. OpenAI developed GPT-3, DeepMind developed Chinchilla, and Google developed BERT. These models have been significant advancements in the field of natural language processing.\n",
      "\n",
      "Marketing Prediction: Some prominent examples of large language models and their developers include GPT-3, created by OpenAI, and BERT, developed by Google. These models have revolutionized how we interact with technology, offering sophisticated natural language processing capabilities that power virtual assistants, language translation services, and content generation tools, among other applications.\n",
      "Marketing Answer: Chinchilla by DeepMind, GPT-3 by OpenAI.\n",
      "------------------------------------------------------------\n",
      "Question: What licensing models have been adopted for the distribution of source-available language models?\n",
      " \n",
      "Engineering Prediction: Language models, particularly large language models (LLMs), have become powerful tools for generating and understanding human language, leading to their rapid adoption in various industries. As LLMs continue to grow in capability and influence, there is a growing recognition of the need to address the risks and ethical challenges they pose. One key aspect of this is the distribution and access model for these systems. \n",
      "\n",
      "Currently, there are a variety of licensing models adopted for the distribution of source-available language models. These models aim to balance the benefits of open access and collaboration with the need for control and mitigation of potential harms. One popular approach is the use of open-source licenses, such as the MIT License or the Apache License, which allow for the free use, modification, and distribution of the source code with minimal restrictions. This approach promotes collaboration and contributes to the rapid advancement of the field. However, it also raises concerns about the potential misuse of this technology, as the barriers to access are relatively low. \n",
      "\n",
      "To address this, some organizations have adopted more restrictive licensing models that limit access to their language models. These models often involve proprietary licenses that grant specific organizations or individuals exclusive or prioritized access to the source code or application programming interfaces (APIs). This approach allows for more control over how the technology is used and by whom. However, it also raises concerns about centralized power and the potential for unequal access, limiting the benefits of this technology to a select few. \n",
      "\n",
      "Furthermore, there is a growing trend towards the development of hybrid models that combine elements of open-source and proprietary licensing. These models often involve the release of a \"base\" language model under an open-source license, with additional layers or modules released under more restrictive licenses. This approach aims to balance the benefits of open collaboration with the need for control and mitigation of risks. It enables the core language model to be scrutinized and improved by the research community while allowing for more targeted control over specific applications or sensitive components. \n",
      "\n",
      "Looking forward, the licensing and distribution of language models are likely to become even more complex as the field advances and the societal impact of these systems becomes more profound. There is a growing emphasis on the responsible development and deployment of AI, including language models, which will likely shape the licensing landscape. This includes considerations of ethical guidelines, such as those proposed by organizations like the Partnership on AI, as well as the development of technical standards and infrastructure to support responsible AI practices. \n",
      "\n",
      "In conclusion, the distribution of source-available language models involves a careful balancing act between open collaboration and controlled access. As the field of language models continues to evolve, the licensing models adopted will play a critical role in shaping the future of this technology, including its accessibility, ethical implications, and societal impact.\n",
      "Engineering Answer: Based on the provided context, it seems that licensing models for the distribution of source-available language models have not been explicitly discussed in the referenced papers. However, it is crucial to consider potential licensing options such as open-source licenses (e.g., GPL, MIT) or proprietary licenses when distributing language models to ensure legal compliance and control over usage rights. Additionally, considering the implications of different licensing models on accessibility, collaboration, and commercialization is essential for determining the most suitable approach for sharing language models with the community. Further research or consultation with legal experts may be necessary to explore specific licensing strategies for source-available language models.\n",
      "\n",
      "Marketing Prediction: Language models are revolutionizing how we interact with technology, offering exciting opportunities for individuals and organizations. With the rapid evolution of LLMs, there is a growing focus on responsible distribution and access. The adoption of licensing models ensures that these powerful tools are utilized ethically and for the benefit of all.\n",
      "Marketing Answer: Answer: Some organizations choose open-sourcing, while others restrict access to a few organizations with resources or offer end-to-end deployment via API.\n",
      "------------------------------------------------------------\n",
      "Question: What are language models and what is their purpose in natural language processing?\n",
      " \n",
      "Engineering Prediction: Language models are a fundamental component of Natural Language Processing (NLP), a subfield of artificial intelligence that focuses on the interaction between computers and human language. These models are designed to understand, interpret, and generate human language by being trained on vast amounts of text data. The primary purpose of language models in NLP is to enable machines to process and understand natural language input and generate meaningful and contextually appropriate responses or interpretations. \n",
      "\n",
      "Technically, language models are based on statistical and machine learning techniques, where words or phrases from a vocabulary are mapped to vectors of real numbers, a concept known as word embeddings. This mathematical representation allows models to capture the semantic relationships between words and the context in which they are used. By training on large text corpora, models can learn the probabilities of word occurrences, predicting the next word in a sequence, as mentioned in the provided context. This capability forms the basis for more complex language understanding and generation tasks. \n",
      "\n",
      "The research in this field has been directed towards developing more advanced models that can go beyond simple word prediction. The focus has shifted to creating models that are aligned with human values and expectations, as mentioned in the context, referring to the desire for models that are helpful, honest, and harmless. This involves training models to follow instructions and ensuring they remain unbiased and ethical in their responses, a direction that the research community is actively pursuing. \n",
      "\n",
      "The future of language models in NLP holds great promise, with ongoing research aiming to enhance their capabilities. This includes improving their understanding of context, generating more coherent and diverse responses, and integrating common-sense reasoning. There is also a growing emphasis on making these models more accessible and useful for a wider range of applications, especially in generating more robust and reliable models that can be deployed in real-world scenarios while maintaining ethical standards. \n",
      "\n",
      "In conclusion, language models are essential tools in NLP, enabling machines to comprehend and generate human language. The field is rapidly evolving, with a focus on creating more advanced models that align with human values and expectations. As research progresses, we can expect to see more sophisticated and responsible language models that will power a new generation of language-based applications and tools, revolutionizing how we interact with technology.\n",
      "Engineering Answer: Language models are probabilistic models of natural language that help predict or correct text. Their purpose in natural language processing is to assist in various tasks such as speech recognition, machine translation, natural language generation, and information retrieval. By analyzing the performance of human subjects, language models improve the understanding and generation of human-like text.\n",
      "\n",
      "Marketing Prediction: Language models are an essential tool in natural language processing (NLP), helping machines understand and generate human language. These models enable a wide range of applications, from virtual assistants and language translation services to content creation and customer service chatbots, making technology more accessible and useful in our daily lives.\n",
      "Marketing Answer: Language models are probabilistic models of natural language that are used in tasks such as speech recognition, machine translation, and natural language generation in natural language processing.\n",
      "------------------------------------------------------------\n",
      "Question: How have language models evolved in terms of architecture, from the 1980s to present times?\n",
      " \n",
      "Engineering Prediction: Language models, in their evolution from the 1980s to the present, have transformed how machines interpret and generate human language. The journey began with rule-based systems and statistical models, which laid the foundation for the field. These early models, often based on finite-state machines and context-free grammars, were successful in limited domains but lacked the flexibility and understanding of natural language nuances.\n",
      "\n",
      "The introduction of neural networks, and specifically recurrent neural networks (RNNs) in the late 1990s, marked a significant shift. These models, with their ability to capture long-range dependencies and context, improved language understanding. The subsequent development of long short-term memory (LSTM) networks and gated recurrent units (GRUs) further enhanced the ability to process sequential data, making them effective for language generation and machine translation tasks.\n",
      "\n",
      "However, a revolutionary leap in language modeling came with the introduction of Transformer-based models in the late 2010s. This self-attention mechanism-based architecture, notably models like BERT and GPT, enabled parallel processing and an unprecedented ability to handle long-range dependencies. These models, trained on vast amounts of text data, have achieved state-of-the-art performance in a wide range of natural language processing (NLP) tasks, including question answering, text generation, and language translation.\n",
      "\n",
      "The current focus in language model architecture revolves around scaling up models, incorporating structural biases, and improving efficiency. Models like GPT-3, with billions of parameters, have demonstrated remarkable capabilities in generating human-like text and adapting to diverse tasks with minimal fine-tuning. Additionally, there is a growing emphasis on making language models more interpretable, addressing their ethical implications, and improving their energy efficiency.\n",
      "\n",
      "Looking forward, the evolution of language models is likely to continue advancing in several key areas. Firstly, there will be a focus on making models more robust and adaptable, improving their ability to handle diverse linguistic patterns and reducing their susceptibility to biases present in training data. Secondly, the integration of multimodal information, combining textual data with visual and auditory inputs, will expand the capabilities of language models and enable their application in a broader range of tasks. Lastly, addressing ethical concerns and mitigating potential harms associated with language models, such as the generation of misleading or biased content, will remain a critical aspect of their development and deployment.\n",
      "Engineering Answer: Language models have evolved significantly in terms of architecture from the 1980s to present times. In the 1980s, the first statistical language model was proposed, leading to experiments by IBM that identified areas for improvement by observing human subjects. However, it wasn't until 2017 when the transformer architecture was introduced by Google, revolutionizing the field. This development paved the way for models like BERT in 2018, which marked a shift towards large-scale transformer-based language models. These modern architectures, based on self-attention mechanisms, have dominated the field of natural language processing, achieving state-of-the-art performance in various tasks.\n",
      "\n",
      "Marketing Prediction: Language models have evolved significantly since the 1980s, and today's models offer exciting opportunities and benefits for users. The journey from rule-based systems to the large language models (LLMs) we know today has been transformative, with LLMs now capable of understanding and generating human-like language. This evolution has opened up a world of applications, from virtual assistants and language translation services to content creation and summarization tools, all designed to make our lives easier and more efficient. With their ability to process vast amounts of data and learn from it, LLMs have become invaluable assets to individuals and organizations, revolutionizing the way we interact with technology and each other.\n",
      "Marketing Answer: Language models have evolved from early statistical models in the 1980s to modern transformer architectures, such as BERT and GPT-2, which use self-attention mechanisms and have become dominant in natural language processing tasks.\n",
      "------------------------------------------------------------\n",
      "Question: Can you explain how maximum entropy language models work and what the partition function signifies?\n",
      " \n",
      "Engineering Prediction: Maximum entropy language models are a probabilistic approach to language modeling that utilizes the principle of maximum entropy to make predictions. These models aim to capture the uncertainty in the data by selecting the probability distribution that has the highest entropy among all possible distributions that satisfy the given constraints. The constraints are typically in the form of expected feature values, which are calculated based on the training data. By maximizing the entropy, the model becomes less committed to any specific outcome and assigns similar probabilities to all possible completions, indicating uncertainty.\n",
      "\n",
      "The partition function, often denoted as Z, plays a crucial role in maximum entropy language models. It serves as a normalization factor and ensures that the probabilities assigned by the model sum up to 1. Specifically, the partition function calculates the sum of the exponential values of the feature functions for all possible outcomes. This normalization step allows the model to convert the unnormalized probabilities, or raw scores, into valid probabilities.\n",
      "\n",
      "One of the key advantages of maximum entropy language models is their ability to incorporate diverse features and constraints. These models can take into account various linguistic and contextual information, such as word frequencies, parts of speech, syntax, and semantic relationships. By specifying the expected values of these features, the models can learn to make more informed predictions while maintaining the desired level of uncertainty.\n",
      "\n",
      "However, maximum entropy language models also have their limitations and ethical considerations. One notable challenge is the estimation of the partition function, especially for large or complex models. Computing the sum over all possible outcomes can be computationally expensive and infeasible for models with high-dimensional output spaces. This limitation has driven the development of approximation techniques and sampling methods to estimate the partition function more efficiently.\n",
      "\n",
      "As language models continue to evolve and advance, maximum entropy models may find applications in generating diverse and creative language outputs. By capturing the underlying patterns and relationships in the data while maintaining a level of uncertainty, these models can potentially enhance language generation tasks, such as dialogue systems, story generation, and language translation.\n",
      "\n",
      "Furthermore, ethical considerations come into play when deploying maximum entropy language models in real-world applications. These models, like any other language model, are susceptible to biases present in the training data. If the training data contains stereotypes, discriminatory language, or biased representations, the models may inadvertently perpetuate these biases in their generated responses. Addressing these challenges requires careful data curation, model evaluation, and the development of bias mitigation techniques to ensure the responsible and ethical deployment of maximum entropy language models.\n",
      "\n",
      "In summary, maximum entropy language models offer a flexible and powerful approach to language modeling by maximizing uncertainty while incorporating relevant constraints. The partition function plays a critical role in normalizing the probabilities and ensuring valid probability distributions. While these models show promise in various language generation tasks, addressing computational challenges and ethical considerations is essential for their successful and responsible integration into language technologies.\n",
      "Engineering Answer: Maximum entropy language models use feature functions to encode the relationship between a word and its n-gram history, aiming to maximize reward while satisfying a KL-constrained objective. The partition function, denoted as Z(x), is crucial in normalizing the probabilities of all possible outputs given the input. It represents the sum of the exponential of the reward function over all possible output sequences, making it computationally expensive to estimate but essential for accurate modeling. The partition function ensures that the model's predicted probabilities sum up to 1, providing a foundation for effective language modeling.\n",
      "\n",
      "Marketing Prediction: Maximum entropy language models are a powerful tool for natural language processing, offering a flexible approach to understanding and generating human-like text. By maximizing entropy, these models capture complex language patterns and uncertainties, allowing them to make informed predictions while remaining adaptable to new data. The partition function plays a crucial role in this process, acting as a normalizing factor that ensures the model's probabilities are calibrated, enabling it to select the most likely and coherent completions for any given context. This combination of maximum entropy and partition function usage results in models that can generate diverse and contextually appropriate responses, making them highly valuable for a range of applications, from language translation to content creation, and even enabling more natural and engaging human-machine interactions.\n",
      "Marketing Answer: Maximum entropy language models encode the relationship between a word and the n-gram history using feature functions. The partition function in this context represents the total probability of all possible outcomes, making it a crucial factor in determining the optimal solution for the reward maximization objective.\n",
      "------------------------------------------------------------\n",
      "Question: What is the benefit of using continuous space embeddings in recurrent neural network language models?\n",
      " \n",
      "Engineering Prediction: Continuous space embeddings offer significant advantages for recurrent neural network (RNN) language models by transforming discrete word representations into dense vectors within a lower-dimensional continuous space. This transformation enables more efficient modeling of semantic relationships between words and captures the underlying structure of the language. \n",
      "\n",
      "One key benefit of this approach is the ability to represent similar words with nearby embeddings, capturing semantic relatedness. Words with similar meanings or contextual usage tend to have closer vector representations, enabling the model to generalize knowledge and understand context more effectively. This property enhances the model's capability to handle polysemy and synonymy, improving its overall language understanding. \n",
      "\n",
      "Moreover, continuous space embeddings facilitate smoother interpolation and extrapolation, allowing RNN language models to make more accurate predictions, especially when dealing with unseen data. This capability is particularly useful for language modeling tasks, where the model needs to generate coherent and contextually appropriate responses or predictions. \n",
      "\n",
      "The use of continuous space embeddings also enables more efficient mathematical operations and simplifies the computation of vector arithmetic. This efficiency is advantageous for training large-scale language models and facilitates faster processing, making it more feasible to deploy such models in real-world applications with strict latency requirements. \n",
      "\n",
      "From a research perspective, continuous space embeddings offer intriguing possibilities for exploring the geometric properties of language representations. This line of inquiry can lead to a deeper understanding of how semantic relationships are encoded and manipulated by neural networks, potentially revealing fundamental insights into the nature of language and its computational underpinnings. \n",
      "\n",
      "Looking forward, the ethical implications of continuous space embeddings merit consideration. As these embeddings capture semantic relationships, they may inadvertently encode societal biases present in the training data, leading to biased predictions or reinforcing stereotypes. Careful curation of training data and ongoing evaluation of model outputs are necessary to mitigate these risks and ensure fairness and inclusivity in language technologies. \n",
      "\n",
      "In conclusion, continuous space embeddings in RNN language models offer enhanced capabilities for understanding and generating language by capturing semantic relationships and the underlying geometry of linguistic structures. This technique has significant implications for both the effectiveness and ethical deployment of language technologies, shaping future research directions in natural language processing.\n",
      "Engineering Answer: Continuous space embeddings in recurrent neural network language models help alleviate the curse of dimensionality by representing words as non-linear combinations of weights in the embedding space. This approach helps address the data sparsity problem caused by the exponential increase in possible word sequences with vocabulary size. By utilizing continuous space embeddings, neural networks can effectively capture semantic relationships and meaning within the language model.\n",
      "\n",
      "Marketing Prediction: Continuous space embeddings in recurrent neural network language models offer a powerful way to represent words and their relationships, enabling a deeper understanding of language and improving tasks such as sentiment analysis. By mapping words to a lower-dimensional continuous vector space, these embeddings capture the context and relationships between words, enhancing language model performance and enabling more effective training on specific objectives. This technique allows models to make more accurate predictions and understand the global structure and ontology of language, benefiting a range of applications, from text classification to language generation, and making LLMs an exciting and promising area of development.\n",
      "Marketing Answer: Continuous space embeddings in recurrent neural network language models help alleviate the curse of dimensionality caused by the exponential increase in possible word sequences, reducing data sparsity issues.\n",
      "------------------------------------------------------------\n",
      "Question: What challenges do large language models face in mirroring human cognitive patterns?\n",
      " \n",
      "Engineering Prediction: Large language models, despite their impressive capabilities, face significant challenges in mirroring human cognitive patterns. One key challenge is the propensity for hallucination, where the model generates responses that are factually incorrect or inconsistent. This issue highlights a misalignment between the model's behavior and human expectations, underscoring the need for improved methods to decrease toxic and biased responses. Additionally, there is a research question of how to effectively evaluate and benchmark these models, as traditional metrics may not capture all aspects of human-like cognition. \n",
      "\n",
      "Future work in this area should focus on enhancing language models' understanding of context, improving their ability to reason and interpret instructions, and ensuring that their behavior aligns with ethical and societal norms. Developing methods to more accurately evaluate and benchmark these models is also crucial. By addressing these challenges, we can move towards creating language models that better reflect human cognitive processes and are more useful and trustworthy assistants in a variety of real-world applications.\n",
      "Engineering Answer: Large language models face challenges in mirroring human cognitive patterns because they sometimes learn patterns that humans do not learn, while also failing to learn patterns that humans typically learn. This discrepancy suggests that the models may not be plausible cognitive models, despite matching human performance in some tasks. Further research is needed to address these limitations and improve the alignment of large language models with human cognitive patterns.\n",
      "\n",
      "Marketing Prediction: Large language models offer incredible potential for mirroring human cognitive patterns, but they currently face challenges with generating toxic or biased responses, and sometimes produce answers that are not factually accurate—an issue known as \"hallucination.\" However, with ongoing research and development, these models have the capacity to revolutionize how we interact with technology, offering creative solutions and innovative opportunities across a diverse range of applications.\n",
      "Marketing Answer: Large language models sometimes learn patterns that humans do not learn and fail to learn patterns that humans typically do learn.\n"
     ]
    }
   ],
   "source": [
    "eng_rag_chain = build_RAG_prompt_chain(eng_rag_template, llm_model, retriever, format_docs)\n",
    "mk_rag_chain = build_RAG_prompt_chain(mk_rag_template, llm_model, retriever, format_docs)\n",
    "\n",
    "metrics = ['rouge', 'bleu', 'bertscore']\n",
    "\n",
    "rag_chains = {\"engineering\": eng_rag_chain, \"marketing\": mk_rag_chain}\n",
    "\n",
    "dpt_prompt = evaluate(metrics, validation_questions_answers, rag_chains, iterations=10, verbose=False, dept_specific=True, print_results=True)\n",
    "dpt_prompt = composite_evaluation(dpt_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "6jHlgyr2FqjI",
    "outputId": "9990c8aa-6dde-432e-f0ea-dfb5264e763c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"dpt_prompt\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"Sample\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.373295374522416,\n        \"min\": 0.0,\n        \"max\": 13.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          6.6,\n          7.5,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eng_bleu\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.5271121706335373,\n        \"min\": 0.0,\n        \"max\": 10.0,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          10.0,\n          0.02245725872026997,\n          0.03355699930929552\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mk_bleu\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.526824576266122,\n        \"min\": 0.0,\n        \"max\": 10.0,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          10.0,\n          0.022663881012833457,\n          0.07633261649713671\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eng_rouge\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.484185709455161,\n        \"min\": 0.027216586779434823,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.16365744568479573,\n          0.16681429244667018,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mk_rouge\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.4850800126346813,\n        \"min\": 0.05333333333333333,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.16315757019130755,\n          0.16054421768707483,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eng_f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.2881029715563335,\n        \"min\": 0.01121955136676007,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.8576722323894501,\n          0.8541396856307983,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mk_f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.286150925787941,\n        \"min\": 0.013610839446302168,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.8648161888122559,\n          0.8601585924625397,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"composite_eng\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.3918932506630055,\n        \"min\": 0.01666489204301954,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.48242480164421775,\n          0.4806021965151288,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"composite_mk\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.3910533105909875,\n        \"min\": 0.026859185316017747,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.48588814166608685,\n          0.48427735236115216,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"composite_total\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.3921367080637697,\n        \"min\": 0.012090860670361523,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.4838101376529654,\n          0.4864357861315516,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-087d4b4b-ea0f-4573-b62d-561479ad6cc3\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample</th>\n",
       "      <th>eng_bleu</th>\n",
       "      <th>mk_bleu</th>\n",
       "      <th>eng_rouge</th>\n",
       "      <th>mk_rouge</th>\n",
       "      <th>eng_f1</th>\n",
       "      <th>mk_f1</th>\n",
       "      <th>composite_eng</th>\n",
       "      <th>composite_mk</th>\n",
       "      <th>composite_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.600000</td>\n",
       "      <td>0.022457</td>\n",
       "      <td>0.022664</td>\n",
       "      <td>0.163657</td>\n",
       "      <td>0.163158</td>\n",
       "      <td>0.857672</td>\n",
       "      <td>0.864816</td>\n",
       "      <td>0.482425</td>\n",
       "      <td>0.485888</td>\n",
       "      <td>0.483810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.788876</td>\n",
       "      <td>0.022514</td>\n",
       "      <td>0.031799</td>\n",
       "      <td>0.027217</td>\n",
       "      <td>0.058593</td>\n",
       "      <td>0.011220</td>\n",
       "      <td>0.013611</td>\n",
       "      <td>0.016665</td>\n",
       "      <td>0.026859</td>\n",
       "      <td>0.012091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133080</td>\n",
       "      <td>0.053333</td>\n",
       "      <td>0.846499</td>\n",
       "      <td>0.852080</td>\n",
       "      <td>0.465014</td>\n",
       "      <td>0.443277</td>\n",
       "      <td>0.464387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.138702</td>\n",
       "      <td>0.130832</td>\n",
       "      <td>0.849387</td>\n",
       "      <td>0.854787</td>\n",
       "      <td>0.468065</td>\n",
       "      <td>0.467576</td>\n",
       "      <td>0.477666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.500000</td>\n",
       "      <td>0.024847</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166814</td>\n",
       "      <td>0.160544</td>\n",
       "      <td>0.854140</td>\n",
       "      <td>0.860159</td>\n",
       "      <td>0.480602</td>\n",
       "      <td>0.484277</td>\n",
       "      <td>0.486436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>10.500000</td>\n",
       "      <td>0.033557</td>\n",
       "      <td>0.043609</td>\n",
       "      <td>0.180210</td>\n",
       "      <td>0.184439</td>\n",
       "      <td>0.864253</td>\n",
       "      <td>0.870092</td>\n",
       "      <td>0.491857</td>\n",
       "      <td>0.502284</td>\n",
       "      <td>0.489412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>0.064525</td>\n",
       "      <td>0.076333</td>\n",
       "      <td>0.215517</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.881512</td>\n",
       "      <td>0.890612</td>\n",
       "      <td>0.518316</td>\n",
       "      <td>0.527233</td>\n",
       "      <td>0.505513</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-087d4b4b-ea0f-4573-b62d-561479ad6cc3')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-087d4b4b-ea0f-4573-b62d-561479ad6cc3 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-087d4b4b-ea0f-4573-b62d-561479ad6cc3');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-d2dbccc2-666f-41ca-9f2c-ef366be00cbd\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d2dbccc2-666f-41ca-9f2c-ef366be00cbd')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-d2dbccc2-666f-41ca-9f2c-ef366be00cbd button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "          Sample   eng_bleu    mk_bleu  eng_rouge   mk_rouge     eng_f1  \\\n",
       "count  10.000000  10.000000  10.000000  10.000000  10.000000  10.000000   \n",
       "mean    6.600000   0.022457   0.022664   0.163657   0.163158   0.857672   \n",
       "std     4.788876   0.022514   0.031799   0.027217   0.058593   0.011220   \n",
       "min     0.000000   0.000000   0.000000   0.133080   0.053333   0.846499   \n",
       "25%     2.250000   0.000000   0.000000   0.138702   0.130832   0.849387   \n",
       "50%     7.500000   0.024847   0.000000   0.166814   0.160544   0.854140   \n",
       "75%    10.500000   0.033557   0.043609   0.180210   0.184439   0.864253   \n",
       "max    13.000000   0.064525   0.076333   0.215517   0.260870   0.881512   \n",
       "\n",
       "           mk_f1  composite_eng  composite_mk  composite_total  \n",
       "count  10.000000      10.000000     10.000000        10.000000  \n",
       "mean    0.864816       0.482425      0.485888         0.483810  \n",
       "std     0.013611       0.016665      0.026859         0.012091  \n",
       "min     0.852080       0.465014      0.443277         0.464387  \n",
       "25%     0.854787       0.468065      0.467576         0.477666  \n",
       "50%     0.860159       0.480602      0.484277         0.486436  \n",
       "75%     0.870092       0.491857      0.502284         0.489412  \n",
       "max     0.890612       0.518316      0.527233         0.505513  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpt_prompt.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fn0BMiVunO0u"
   },
   "source": [
    "From this the first thing that stands out it that the engineering answers are no longer abiding by the single paragraph requirement.  Also, overall, it appears that putting an excessive amount instructions is causing the predictions to create very lengthy, very detailed responses, likely in attempt to ensure it is covering all the requirements.\n",
    "\n",
    "**Second Pass** We now see if we can reduce some of these instructions and counteract some for the decay we are seeing in the responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nkjzfAqRnnxo"
   },
   "outputs": [],
   "source": [
    "eng_rag_template = \"\"\"[INST]\n",
    "              Please provide an precise and concise answer to the engineer's question below based on the context information provided.\\n\\n\n",
    "              Below is a context:\\n{context}\\n\n",
    "              Below is a question:\\n{question}\\n\n",
    "              Below are answer instructions in order of importance:\n",
    "- Formatting: Provide a concise, single-paragraph answer that uses the fewest words necessary to fully address the question. Do not use bullet points. Do not explicitly reference papers in your answer.\n",
    "- Technical Detail: Include technical details and terminologies that relate to the question.\n",
    "- Research Focus: Orient answers towards the research aspects of the questions.\n",
    "- Objective Tone: Maintain an objective and informative tone, aiming to educate the reader without persuasive language.\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "mk_rag_template = \"\"\"[INST]\n",
    "              Please provide a precise and concise answer to the marketer's question below based on the context provided.\\n\\n\n",
    "              Below is a context:\\n{context}\\n\n",
    "              Below is a question:\\n{question}\\n\n",
    "              Below are answer instructions in order of importance:\n",
    "- Formatting: Provide a concise, single-paragraph answer that uses the fewest words necessary to fully address the question. Do not use bullet points. Do not explicitly reference papers in your answer.\n",
    "- Succinctness: Make sure your answer is concise and to the point. Provide essential information without delving into the technical depth.\n",
    "- Broad Overview: Give a broad overview of the topic, focusing on the practical applications and implications. Aim to communicate the value and utility for a wider audience.\n",
    "- Focus on Applications: Emphasize the applications of the topic at hand. Focus on real-world uses and benefits and highlight how technology can solve problems or create opportunities.\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "eng_rag_prompt = ChatPromptTemplate.from_template(eng_rag_template)\n",
    "mk_rag_prompt = ChatPromptTemplate.from_template(mk_rag_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bUUMJXLZnzIs",
    "outputId": "4de18ad7-3709-40b0-c954-5e265391f0e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Question: What purpose do large language models serve in the field of natural language processing?\n",
      " \n",
      "Engineering Prediction: Large language models serve as foundational components in natural language processing (NLP), offering a versatile framework adaptable to diverse downstream tasks. These models, through extensive training on vast text corpora, master the intricacies of language, enabling them to excel across various NLP applications, including classification, summarization, question-answering, and creative content generation. The adaptability of these models empowers their integration into specialized domains, making them invaluable tools for researchers and practitioners in the field of NLP.\n",
      "Engineering Answer: Large language models (LLMs) serve the purpose of enabling general-purpose language generation and other natural language processing tasks such as classification. They achieve this by learning statistical relationships from text documents during computationally intensive self-supervised and semi-supervised training. LLMs can be used for text generation by predicting the next token or word, making them valuable for tasks like speech recognition, machine translation, and information retrieval. Additionally, LLMs have superseded previous models like recurrent neural networks, showcasing their efficiency and effectiveness in NLP tasks.\n",
      "\n",
      "Marketing Prediction: Large language models serve as the foundation for a wide range of natural language processing applications, including classification, summarization, question-answering, creative content generation, and dialogue systems. These models are adapted from pre-trained, large-scale language models and fine-tuned for specific tasks, enabling them to understand and generate human-like text with a diverse range of practical applications.\n",
      "Marketing Answer: Large language models serve the purpose of improving performance in various natural language processing tasks, such as speech recognition, machine translation, natural language generation, optical character recognition, handwriting recognition, grammar induction, and information retrieval.\n",
      "------------------------------------------------------------\n",
      "Question: How does a large language model learn from text during training?\n",
      " \n",
      "Engineering Prediction: Large language models are trained using a variety of methods, including pre-training on large text corpora and fine-tuning on specific tasks. During pre-training, these models learn to predict the next word in a sequence of text, which helps them understand the context and structure of language. This process is often referred to as language modeling and can be done using different techniques, such as the \"text-to-text\" approach, where various language tasks are converted into a unified format. Fine-tuning, on the other hand, involves further training the model on specific downstream tasks to improve performance in those areas. This two-step process allows large language models to learn from text and adapt to different language-related challenges effectively.\n",
      "Engineering Answer: A large language model learns from text during training by first going through an unsupervised generative 'pretraining' stage where it sets initial parameters using a language modeling objective. Then, it goes through a supervised discriminative 'fine-tuning' stage where it refines its parameters based on annotated examples or task demonstrations. This dual-stage approach allows the model to learn statistical relationships from text documents in a computationally intensive process, enabling it to achieve general-purpose language generation and natural language processing tasks.\n",
      "\n",
      "Marketing Prediction: Large language models learn by analyzing vast amounts of text data and identifying patterns, relationships, and context. This process involves breaking down text into smaller parts, understanding syntax and semantics, and then using this knowledge to generate responses or perform tasks. These models can be applied to various applications, such as language translation, text generation, and summarization, enabling machines to understand and generate human-like language.\n",
      "Marketing Answer: A large language model learns from text during training by first pretraining on a diverse dataset to acquire general language knowledge, and then fine-tuning on specific tasks or demonstrations to adapt its parameters for more targeted performance.\n",
      "------------------------------------------------------------\n",
      "Question: What are some key architectures behind the development of large language models?\n",
      " \n",
      "Engineering Prediction: Transformer-based language models, especially those utilizing neural networks, form the architectural backbone of many contemporary large language models. These models have seen success in QA tasks and are the focus of ongoing research, with the aim of creating general and scalable methods applicable to future AI systems.\n",
      "Engineering Answer: Key architectures behind the development of large language models include the use of self-attention mechanisms, such as those seen in Transformer decoders. These architectures have been applied to tasks like autoregressive language modeling and have led to the dominance of Transformer-based language models in NLP. Models like BERT and GPT-2 have further advanced this paradigm, showcasing the power of large Transformer language models in achieving state-of-the-art results across various NLP tasks. Additionally, architectures like neural-retriever-in-the-loop generative-based models have shown improvements in tasks like open-domain QA and knowledge-grounded dialogue, emphasizing the importance of consistent and engaging responses in long-form generation and multi-turn conversations.\n",
      "\n",
      "Marketing Prediction: Large language models, with their ability to process and generate human-like text, have become key enablers for many applications. These models typically utilize neural networks, especially Transformer-based architectures, which allow them to handle vast amounts of data and context. This has led to the development of QA models that can provide accurate and contextually relevant answers, powering applications such as virtual assistants and language translation services. The applications of these models are far-reaching, from improving customer service and content creation to enabling more natural human-machine interactions.\n",
      "Marketing Answer: Key architectures behind the development of large language models include Transformer-based models such as BERT and GPT-2, which utilize self-attention mechanisms for tasks like autoregressive language modeling and knowledge-grounded dialogue. These models have shown significant success in NLP tasks and have led to advancements in general-purpose language generation and natural language processing.\n",
      "------------------------------------------------------------\n",
      "Question: Can you name some specific large language models and the companies or organizations that have developed them?\n",
      " \n",
      "Engineering Prediction: Some prominent examples of large language models and their developers include GPT-3, developed by OpenAI, and BERT, developed by Google. These models have had a significant impact on the field of Natural Language Processing and have inspired further research and development in the creation of advanced language models.\n",
      "Engineering Answer: Some specific large language models include GPT-3 by OpenAI, Chinchilla by DeepMind, and BERT by Google. OpenAI developed GPT-3, DeepMind developed Chinchilla, and Google developed BERT. These models have been significant advancements in the field of natural language processing.\n",
      "\n",
      "Marketing Prediction: Some prominent examples of large language models and their developers include GPT-3, created by OpenAI, and BERT, developed by Google. These models have revolutionized various natural language processing tasks and have been applied in fields such as language translation, text generation, and question-answering systems. They offer practical benefits, such as enhancing search engines, improving virtual assistants, and enabling more human-like interactions with machines.\n",
      "Marketing Answer: Chinchilla by DeepMind, GPT-3 by OpenAI.\n",
      "------------------------------------------------------------\n",
      "Question: What licensing models have been adopted for the distribution of source-available language models?\n",
      " \n",
      "Engineering Prediction: Open-source and proprietary licensing models have been the primary approaches for distributing source-available language models. Open-source licenses, like the MIT or Apache 2.0 licenses, grant users the rights to access, modify, and distribute the source code freely, promoting collaboration and transparency. On the other hand, proprietary licensing allows the model owner to retain exclusive rights, offering access through restricted licenses with specific terms and conditions, often for commercial purposes.\n",
      "Engineering Answer: Based on the provided context, it seems that licensing models for the distribution of source-available language models have not been explicitly discussed in the referenced papers. However, it is crucial to consider potential licensing options such as open-source licenses (e.g., GPL, MIT) or proprietary licenses when distributing language models to ensure legal compliance and control over usage rights. Additionally, considering the implications of different licensing models on accessibility, collaboration, and commercialization is essential for determining the most suitable approach for sharing language models with the community. Further research or consultation with legal experts may be necessary to explore specific licensing strategies for source-available language models.\n",
      "\n",
      "Marketing Prediction: There has been a recent trend towards open-sourcing large language models, with organizations adopting various licensing models to balance accessibility and responsible usage. These models often employ permissive licenses, such as the MIT License, to ensure wide distribution and usage while also including terms of use agreements that outline restrictions and ethical guidelines to mitigate potential misuse.\n",
      "Marketing Answer: Answer: Some organizations choose open-sourcing, while others restrict access to a few organizations with resources or offer end-to-end deployment via API.\n",
      "------------------------------------------------------------\n",
      "Question: What are language models and what is their purpose in natural language processing?\n",
      " \n",
      "Engineering Prediction: Language models are a fundamental component of Natural Language Processing (NLP), utilizing mathematical embeddings to represent words and phrases as vectors. These models are designed to predict the next word in a sequence, trained on vast amounts of text data to capture linguistic patterns and semantics. The primary purpose of language models in NLP is to enable machines to understand and generate human language, facilitating tasks such as text generation, sentiment analysis, machine translation, and question-answering. By optimizing the next word prediction objective, language models aim to become helpful, honest, and harmless tools that align with human expectations and desires.\n",
      "Engineering Answer: Language models are probabilistic models of natural language that help predict or correct text. Their purpose in natural language processing is to assist in various tasks such as speech recognition, machine translation, natural language generation, and information retrieval. By analyzing the performance of human subjects, language models improve the understanding and generation of human-like text.\n",
      "\n",
      "Marketing Prediction: Language models are a critical component of Natural Language Processing (NLP), where they are used to map words and phrases to vectors of real numbers, enabling machines to understand and generate human-like text. The primary purpose of language models in NLP is to process and interpret human language accurately, helping machines comprehend complex linguistic nuances, such as context, syntax, and semantics. This technology has numerous applications, including language translation, text generation, sentiment analysis, and question-answering, ultimately aiming to bridge the communication gap between humans and machines.\n",
      "Marketing Answer: Language models are probabilistic models of natural language that are used in tasks such as speech recognition, machine translation, and natural language generation in natural language processing.\n",
      "------------------------------------------------------------\n",
      "Question: How have language models evolved in terms of architecture, from the 1980s to present times?\n",
      " \n",
      "Engineering Prediction: Language models have evolved significantly since their inception in the 1980s, with a focus on improving architecture to enhance performance and capabilities. Early language models relied on statistical methods and rule-based systems, which have given way to more advanced neural network-based models, particularly Transformer-based models, which have become dominant in the present day. These models, introduced in the 2010s, revolutionized the field with their ability to process sequential data and have been continuously refined and expanded since, driving the current state-of-the-art in language modeling.\n",
      "Engineering Answer: Language models have evolved significantly in terms of architecture from the 1980s to present times. In the 1980s, the first statistical language model was proposed, leading to experiments by IBM that identified areas for improvement by observing human subjects. However, it wasn't until 2017 when the transformer architecture was introduced by Google, revolutionizing the field. This development paved the way for models like BERT in 2018, which marked a shift towards large-scale transformer-based language models. These modern architectures, based on self-attention mechanisms, have dominated the field of natural language processing, achieving state-of-the-art performance in various tasks.\n",
      "\n",
      "Marketing Prediction: Language models have evolved from simple rule-based systems in the 1980s to sophisticated neural network architectures like Transformers in the present day. This evolution has enabled models to process and generate human-like language, with applications in question-answering, text generation, and language translation. While larger models have improved performance, they also introduce challenges, such as ethical concerns around misinformation and abuse.\n",
      "Marketing Answer: Language models have evolved from early statistical models in the 1980s to modern transformer architectures, such as BERT and GPT-2, which use self-attention mechanisms and have become dominant in natural language processing tasks.\n",
      "------------------------------------------------------------\n",
      "Question: Can you explain how maximum entropy language models work and what the partition function signifies?\n",
      " \n",
      "Engineering Prediction: Maximum entropy language models aim to capture as much uncertainty as possible, indicated by an entropy value of 1 for binary choices. The partition function, Z(x), is a critical component in this context, representing the normalization factor in the model's probability distribution over potential completions of a given input. Estimating Z(x) is computationally expensive, and its value influences the probability assigned to each completion, with higher values indicating greater uncertainty.\n",
      "Engineering Answer: Maximum entropy language models use feature functions to encode the relationship between a word and its n-gram history, aiming to maximize reward while satisfying a KL-constrained objective. The partition function, denoted as Z(x), is crucial in normalizing the probabilities of all possible outputs given the input. It represents the sum of the exponential of the reward function over all possible output sequences, making it computationally expensive to estimate but essential for accurate modeling. The partition function ensures that the model's predicted probabilities sum up to 1, providing a foundation for effective language modeling.\n",
      "\n",
      "Marketing Prediction: Maximum entropy language models aim to capture the underlying patterns and relationships in a given dataset by assigning probabilities to different possible outcomes. The model achieves maximum entropy when it is most uncertain about the outcome, treating all options as equally likely. The partition function, in this case, normalizes the probabilities to ensure they sum up to 1, providing a well-defined probability distribution. This approach enables language models to make informed predictions while accounting for uncertainty, which is particularly useful in natural language processing tasks where context and ambiguity play a significant role. By leveraging maximum entropy, language models can better represent complex linguistic patterns and generate more coherent and contextually appropriate responses.\n",
      "Marketing Answer: Maximum entropy language models encode the relationship between a word and the n-gram history using feature functions. The partition function in this context represents the total probability of all possible outcomes, making it a crucial factor in determining the optimal solution for the reward maximization objective.\n",
      "------------------------------------------------------------\n",
      "Question: What is the benefit of using continuous space embeddings in recurrent neural network language models?\n",
      " \n",
      "Engineering Prediction: Continuous space embeddings in recurrent neural network language models offer the advantage of representing words in a lower-dimensional continuous vector space, capturing semantic relationships and improving context understanding. This technique enhances sentiment analysis and text classification tasks by enabling the model to make predictions based on the geometric relationships between word embeddings.\n",
      "Engineering Answer: Continuous space embeddings in recurrent neural network language models help alleviate the curse of dimensionality by representing words as non-linear combinations of weights in the embedding space. This approach helps address the data sparsity problem caused by the exponential increase in possible word sequences with vocabulary size. By utilizing continuous space embeddings, neural networks can effectively capture semantic relationships and meaning within the language model.\n",
      "\n",
      "Marketing Prediction: Continuous space embeddings in recurrent neural network language models offer the advantage of transforming words into a lower-dimensional continuous vector space. This approach enables more efficient processing and analysis of text data, making it particularly useful for tasks such as sentiment analysis and language understanding. By representing words in a structured and meaningful way, these embeddings help machines interpret and generate human language more effectively, ultimately improving the performance of language-based applications and enhancing human-machine interactions.\n",
      "Marketing Answer: Continuous space embeddings in recurrent neural network language models help alleviate the curse of dimensionality caused by the exponential increase in possible word sequences, reducing data sparsity issues.\n",
      "------------------------------------------------------------\n",
      "Question: What challenges do large language models face in mirroring human cognitive patterns?\n",
      " \n",
      "Engineering Prediction: Large language models face challenges in mirroring human cognitive patterns due to their propensity to generate toxic and biased responses, a phenomenon known as \"hallucination.\" This issue highlights a critical barrier in aligning language model behavior with human expectations and preferences. Current research focuses on exploring methods to reduce such hallucinations and improve model performance on various language tasks, including classification, summarization, and question-answering, to make them more aligned with human cognitive processes.\n",
      "Engineering Answer: Large language models face challenges in mirroring human cognitive patterns because they sometimes learn patterns that humans do not learn, while also failing to learn patterns that humans typically learn. This discrepancy suggests that the models may not be plausible cognitive models, despite matching human performance in some tasks. Further research is needed to address these limitations and improve the alignment of large language models with human cognitive patterns.\n",
      "\n",
      "Marketing Prediction: Large language models face challenges in mirroring human cognitive patterns due to their propensity to generate toxic and biased responses, as well as their tendency to hallucinate answers. These issues need to be addressed to align language model behavior with human expectations and preferences, ensuring these models provide accurate and ethical responses in applications like dialogue, question-answering, and creative writing.\n",
      "Marketing Answer: Large language models sometimes learn patterns that humans do not learn and fail to learn patterns that humans typically do learn.\n"
     ]
    }
   ],
   "source": [
    "eng_rag_chain = build_RAG_prompt_chain(eng_rag_template, llm_model, retriever, format_docs)\n",
    "mk_rag_chain = build_RAG_prompt_chain(mk_rag_template, llm_model, retriever, format_docs)\n",
    "\n",
    "metrics = ['rouge', 'bleu', 'bertscore']\n",
    "\n",
    "rag_chains = {\"engineering\": eng_rag_chain, \"marketing\": mk_rag_chain}\n",
    "\n",
    "dpt_prompt = evaluate(metrics, validation_questions_answers, rag_chains, iterations=10, verbose=False, dept_specific=True, print_results=True)\n",
    "dpt_prompt = composite_evaluation(dpt_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "mLuilh3ynz5k",
    "outputId": "171a05ea-13e1-4446-c780-5f0fe88ed1aa"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"dpt_prompt\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"Sample\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.373295374522416,\n        \"min\": 0.0,\n        \"max\": 13.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          6.6,\n          7.5,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eng_bleu\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.5094350891915123,\n        \"min\": 0.0,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.07601944584869916,\n          0.06655376153482695,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mk_bleu\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.522178435606752,\n        \"min\": 0.0,\n        \"max\": 10.0,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          10.0,\n          0.03517152024774909,\n          0.1148611937074354\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eng_rouge\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.4438480150763913,\n        \"min\": 0.09431122173383759,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.28132791841729954,\n          0.2455446108295849,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mk_rouge\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.4764311530867915,\n        \"min\": 0.06087115727539429,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.1864341491831076,\n          0.19520851818988466,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eng_f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.2795873734238348,\n        \"min\": 0.017845032005157795,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.8875962674617768,\n          0.8831823170185089,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mk_f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.2850369584873635,\n        \"min\": 0.012766552909555766,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.8703349471092224,\n          0.868754118680954,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"composite_eng\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.3720398450123192,\n        \"min\": 0.047071427652735746,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.543400398425818,\n          0.5343884391247529,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"composite_mk\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.386661403561232,\n        \"min\": 0.03210331261934512,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.49813202235909326,\n          0.49815360644459605,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"composite_total\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.379820338238231,\n        \"min\": 0.029187290079789617,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.5252930479991281,\n          0.5251594072762096,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-be099638-5140-4622-ab8e-9bbb683c01f6\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample</th>\n",
       "      <th>eng_bleu</th>\n",
       "      <th>mk_bleu</th>\n",
       "      <th>eng_rouge</th>\n",
       "      <th>mk_rouge</th>\n",
       "      <th>eng_f1</th>\n",
       "      <th>mk_f1</th>\n",
       "      <th>composite_eng</th>\n",
       "      <th>composite_mk</th>\n",
       "      <th>composite_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.600000</td>\n",
       "      <td>0.076019</td>\n",
       "      <td>0.035172</td>\n",
       "      <td>0.281328</td>\n",
       "      <td>0.186434</td>\n",
       "      <td>0.887596</td>\n",
       "      <td>0.870335</td>\n",
       "      <td>0.543400</td>\n",
       "      <td>0.498132</td>\n",
       "      <td>0.525293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.788876</td>\n",
       "      <td>0.066993</td>\n",
       "      <td>0.048409</td>\n",
       "      <td>0.094311</td>\n",
       "      <td>0.060871</td>\n",
       "      <td>0.017845</td>\n",
       "      <td>0.012767</td>\n",
       "      <td>0.047071</td>\n",
       "      <td>0.032103</td>\n",
       "      <td>0.029187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.195122</td>\n",
       "      <td>0.097561</td>\n",
       "      <td>0.867403</td>\n",
       "      <td>0.848807</td>\n",
       "      <td>0.494230</td>\n",
       "      <td>0.457280</td>\n",
       "      <td>0.485284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.250000</td>\n",
       "      <td>0.008930</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.225490</td>\n",
       "      <td>0.137547</td>\n",
       "      <td>0.874111</td>\n",
       "      <td>0.861171</td>\n",
       "      <td>0.508116</td>\n",
       "      <td>0.472447</td>\n",
       "      <td>0.500436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.500000</td>\n",
       "      <td>0.066554</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.245545</td>\n",
       "      <td>0.195209</td>\n",
       "      <td>0.883182</td>\n",
       "      <td>0.868754</td>\n",
       "      <td>0.534388</td>\n",
       "      <td>0.498154</td>\n",
       "      <td>0.525159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>10.500000</td>\n",
       "      <td>0.142629</td>\n",
       "      <td>0.070567</td>\n",
       "      <td>0.302726</td>\n",
       "      <td>0.240754</td>\n",
       "      <td>0.896885</td>\n",
       "      <td>0.882575</td>\n",
       "      <td>0.562151</td>\n",
       "      <td>0.524658</td>\n",
       "      <td>0.537392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>0.164778</td>\n",
       "      <td>0.114861</td>\n",
       "      <td>0.511111</td>\n",
       "      <td>0.265306</td>\n",
       "      <td>0.922094</td>\n",
       "      <td>0.887209</td>\n",
       "      <td>0.643705</td>\n",
       "      <td>0.545109</td>\n",
       "      <td>0.569876</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-be099638-5140-4622-ab8e-9bbb683c01f6')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-be099638-5140-4622-ab8e-9bbb683c01f6 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-be099638-5140-4622-ab8e-9bbb683c01f6');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-dcf8a963-a611-4694-aa6f-ec64006f01c3\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-dcf8a963-a611-4694-aa6f-ec64006f01c3')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-dcf8a963-a611-4694-aa6f-ec64006f01c3 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "          Sample   eng_bleu    mk_bleu  eng_rouge   mk_rouge     eng_f1  \\\n",
       "count  10.000000  10.000000  10.000000  10.000000  10.000000  10.000000   \n",
       "mean    6.600000   0.076019   0.035172   0.281328   0.186434   0.887596   \n",
       "std     4.788876   0.066993   0.048409   0.094311   0.060871   0.017845   \n",
       "min     0.000000   0.000000   0.000000   0.195122   0.097561   0.867403   \n",
       "25%     2.250000   0.008930   0.000000   0.225490   0.137547   0.874111   \n",
       "50%     7.500000   0.066554   0.000000   0.245545   0.195209   0.883182   \n",
       "75%    10.500000   0.142629   0.070567   0.302726   0.240754   0.896885   \n",
       "max    13.000000   0.164778   0.114861   0.511111   0.265306   0.922094   \n",
       "\n",
       "           mk_f1  composite_eng  composite_mk  composite_total  \n",
       "count  10.000000      10.000000     10.000000        10.000000  \n",
       "mean    0.870335       0.543400      0.498132         0.525293  \n",
       "std     0.012767       0.047071      0.032103         0.029187  \n",
       "min     0.848807       0.494230      0.457280         0.485284  \n",
       "25%     0.861171       0.508116      0.472447         0.500436  \n",
       "50%     0.868754       0.534388      0.498154         0.525159  \n",
       "75%     0.882575       0.562151      0.524658         0.537392  \n",
       "max     0.887209       0.643705      0.545109         0.569876  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpt_prompt.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1CmYKtgwOem_"
   },
   "source": [
    "These reduced verbose prompting greatly improved the output for both types of answers.  We are getting close to honing in on finalized prompts for our model.  If anything, the engineering answers are now too concise, and could benefit from additional detail.  While the marketing prompt is still too verbose and could be more concise while still protraying the same requirements.\n",
    "\n",
    "**Third Pass:** We will perform some additional tweaking to the marketing prompt to see if we can get better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RbFT2E_UPEkd"
   },
   "outputs": [],
   "source": [
    "eng_rag_template = \"\"\"[INST]\n",
    "              Please provide an precise and concise answer to the engineer's question below based on the context information provided.\\n\\n\n",
    "              Below is a context:\\n{context}\\n\n",
    "              Below is a question:\\n{question}\\n\n",
    "              Below are answer instructions in order of importance:\n",
    "- Formatting: Provide a succint, single-paragraph answer. Do not use bullet points. Do not explicitly reference papers in your answer.\n",
    "- Technical Detail: Include technical details and terminologies that relate to the question.\n",
    "- Research Focus: Orient answers towards the research aspects of the questions.\n",
    "- Objective Tone: Maintain an objective and informative tone, aiming to educate the reader without persuasive language.\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "mk_rag_template = \"\"\"[INST]\n",
    "              Please provide a precise and concise answer to the marketer's question below based on the context provided.\\n\\n\n",
    "              Below is a context:\\n{context}\\n\n",
    "              Below is a question:\\n{question}\\n\n",
    "              Below are answer instructions in order of importance:\n",
    "- Formatting: Provide a concise, single-paragraph answer that uses the fewest words necessary to fully address the question. Answer in a single sentence or phrase if you can. Do not use bullet points. Do not explicitly reference papers in your answer.\n",
    "- Succinctness: Make sure your answer is concise and to the point. Provide only the essential information without delving into the technical depth.\n",
    "- Broad Overview: Give a broad overview of the topic. Provide only the essential information without delving into the technical depth.\n",
    "- Focus on Applications: EFocus on real-world uses and benefits and highlight how technology can solve problems or create opportunities.\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "eng_rag_prompt = ChatPromptTemplate.from_template(eng_rag_template)\n",
    "mk_rag_prompt = ChatPromptTemplate.from_template(mk_rag_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_u_6obKJPI5s",
    "outputId": "291b82f3-44b4-44be-81f5-5ca6fa1ee05d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Question: What purpose do large language models serve in the field of natural language processing?\n",
      " \n",
      "Engineering Prediction: Large language models serve as foundational components in natural language processing (NLP), offering a versatile framework adaptable to diverse downstream tasks. These models, through extensive training on vast text corpora, master the intricacies of language, including syntax, semantics, and context. This enables them to excel at tasks like classification, summarization, question-answering, and creative content generation. The key advantage of large language models lies in their ability to be fine-tuned for specific tasks, leveraging their inherent language understanding to quickly learn and excel in specialized domains, thus forming the backbone of modern NLP research and applications.\n",
      "Engineering Answer: Large language models (LLMs) serve the purpose of enabling general-purpose language generation and other natural language processing tasks such as classification. They achieve this by learning statistical relationships from text documents during computationally intensive self-supervised and semi-supervised training. LLMs can be used for text generation by predicting the next token or word, making them valuable for tasks like speech recognition, machine translation, and information retrieval. Additionally, LLMs have superseded previous models like recurrent neural networks, showcasing their efficiency and effectiveness in NLP tasks.\n",
      "\n",
      "Marketing Prediction: Large language models are adapted and fine-tuned for various natural language processing tasks, including classification, summarization, question-answering, and creative content generation, enabling machines to understand and generate human-like text for diverse applications.\n",
      "Marketing Answer: Large language models serve the purpose of improving performance in various natural language processing tasks, such as speech recognition, machine translation, natural language generation, optical character recognition, handwriting recognition, grammar induction, and information retrieval.\n",
      "------------------------------------------------------------\n",
      "Question: How does a large language model learn from text during training?\n",
      " \n",
      "Engineering Prediction: Large language models are trained using a variety of techniques, including pre-training and fine-tuning. During pre-training, these models are exposed to vast amounts of text data and learn to predict the next word in a sequence, capturing the underlying patterns and semantics of language. This process is often referred to as language modeling and can be performed using different architectures, such as Transformers. Fine-tuning, on the other hand, involves further training the pre-trained model on specific downstream tasks, like text classification or question answering, to adapt it to particular requirements. This two-step process allows large language models to learn from text and become proficient in a wide range of language-related tasks.\n",
      "Engineering Answer: A large language model learns from text during training by first going through an unsupervised generative 'pretraining' stage where it sets initial parameters using a language modeling objective. Then, it goes through a supervised discriminative 'fine-tuning' stage where it refines its parameters based on annotated examples or task demonstrations. This dual-stage approach allows the model to learn statistical relationships from text documents in a computationally intensive process, enabling it to achieve general-purpose language generation and natural language processing tasks.\n",
      "\n",
      "Marketing Prediction: Large language models learn by analyzing massive amounts of text data, identifying patterns, and using those patterns to generate responses or perform tasks, such as text generation, summarization, or question-answering.\n",
      "Marketing Answer: A large language model learns from text during training by first pretraining on a diverse dataset to acquire general language knowledge, and then fine-tuning on specific tasks or demonstrations to adapt its parameters for more targeted performance.\n",
      "------------------------------------------------------------\n",
      "Question: What are some key architectures behind the development of large language models?\n",
      " \n",
      "Engineering Prediction: Large language models have seen significant development in recent years, with a focus on neural network-based architectures, particularly those utilizing Transformers. This paradigm has been successful in various QA models, with ongoing research aiming for general and scalable methods applicable to future AI systems. Key architectural components include the integration of massive APIs, open and efficient foundation language models, and techniques to mitigate distractions from irrelevant contexts. The research also emphasizes the importance of scalability and adaptability in these models to ensure their effectiveness and applicability in diverse contexts.\n",
      "Engineering Answer: Key architectures behind the development of large language models include the use of self-attention mechanisms, such as those seen in Transformer decoders. These architectures have been applied to tasks like autoregressive language modeling and have led to the dominance of Transformer-based language models in NLP. Models like BERT and GPT-2 have further advanced this paradigm, showcasing the power of large Transformer language models in achieving state-of-the-art results across various NLP tasks. Additionally, architectures like neural-retriever-in-the-loop generative-based models have shown improvements in tasks like open-domain QA and knowledge-grounded dialogue, emphasizing the importance of consistent and engaging responses in long-form generation and multi-turn conversations.\n",
      "\n",
      "Marketing Prediction: Transformer-based language models, which have become the key architecture behind the development of large language models, are designed to handle neural networks and QA tasks, with a focus on efficiency and scalability for future AI systems.\n",
      "Marketing Answer: Key architectures behind the development of large language models include Transformer-based models such as BERT and GPT-2, which utilize self-attention mechanisms for tasks like autoregressive language modeling and knowledge-grounded dialogue. These models have shown significant success in NLP tasks and have led to advancements in general-purpose language generation and natural language processing.\n",
      "------------------------------------------------------------\n",
      "Question: Can you name some specific large language models and the companies or organizations that have developed them?\n",
      " \n",
      "Engineering Prediction: Some prominent examples of large language models and their developers include GPT-3, created by OpenAI, and BERT, developed by Google. These models have had a significant impact on the field of Natural Language Processing and have inspired further research and development by other organizations, such as Facebook AI Research (FAIR) and Microsoft, who have also released their own large language models with unique features and capabilities.\n",
      "Engineering Answer: Some specific large language models include GPT-3 by OpenAI, Chinchilla by DeepMind, and BERT by Google. OpenAI developed GPT-3, DeepMind developed Chinchilla, and Google developed BERT. These models have been significant advancements in the field of natural language processing.\n",
      "\n",
      "Marketing Prediction: Some prominent large language models include GPT-3, developed by OpenAI, and BERT, developed by Google; other notable models are created by companies like Cohere, AI21 Labs, and DeepMind.\n",
      "Marketing Answer: Chinchilla by DeepMind, GPT-3 by OpenAI.\n",
      "------------------------------------------------------------\n",
      "Question: What licensing models have been adopted for the distribution of source-available language models?\n",
      " \n",
      "Engineering Prediction: Open-source licenses, such as the MIT License and the Apache License, have been commonly adopted for the distribution of source-available language models. These licenses allow for the free access, modification, and redistribution of the model's source code, fostering collaboration and enabling researchers and developers to build upon the work. The choice of license depends on the preferences and goals of the developers, with some licenses emphasizing permissiveness and others focusing on copyleft principles, which require derivative works to remain open source. This approach enables the language model community to benefit from shared knowledge and contribute to the advancement of the field.\n",
      "Engineering Answer: Based on the provided context, it seems that licensing models for the distribution of source-available language models have not been explicitly discussed in the referenced papers. However, it is crucial to consider potential licensing options such as open-source licenses (e.g., GPL, MIT) or proprietary licenses when distributing language models to ensure legal compliance and control over usage rights. Additionally, considering the implications of different licensing models on accessibility, collaboration, and commercialization is essential for determining the most suitable approach for sharing language models with the community. Further research or consultation with legal experts may be necessary to explore specific licensing strategies for source-available language models.\n",
      "\n",
      "Marketing Prediction: There is a growing trend of open-sourcing large language models, with organizations adopting a variety of licensing models, including the use of permissive licenses like the MIT License, as well as more restrictive ones, to balance accessibility and responsible usage.\n",
      "Marketing Answer: Answer: Some organizations choose open-sourcing, while others restrict access to a few organizations with resources or offer end-to-end deployment via API.\n",
      "------------------------------------------------------------\n",
      "Question: What are language models and what is their purpose in natural language processing?\n",
      " \n",
      "Engineering Prediction: Language models are a fundamental component of Natural Language Processing (NLP), utilizing mathematical embeddings to represent words and phrases as vectors. These models are designed to predict the next word in a sequence, trained on vast amounts of text data to capture the statistical regularities and patterns of human language. The primary purpose of language models in NLP is to enable machines to understand and generate human language, facilitating a wide range of applications such as language translation, text generation, and question-answering. By optimizing the next word prediction objective, language models aim to become helpful, honest, and harmless tools that align with human expectations and desires.\n",
      "Engineering Answer: Language models are probabilistic models of natural language that help predict or correct text. Their purpose in natural language processing is to assist in various tasks such as speech recognition, machine translation, natural language generation, and information retrieval. By analyzing the performance of human subjects, language models improve the understanding and generation of human-like text.\n",
      "\n",
      "Marketing Prediction: Language models are a critical component of natural language processing (NLP), using mathematical embeddings to map words and phrases to vectors, helping machines understand and generate human language for a variety of applications.\n",
      "Marketing Answer: Language models are probabilistic models of natural language that are used in tasks such as speech recognition, machine translation, and natural language generation in natural language processing.\n",
      "------------------------------------------------------------\n",
      "Question: How have language models evolved in terms of architecture, from the 1980s to present times?\n",
      " \n",
      "Engineering Prediction: Language models have evolved significantly since their inception in the 1980s, with a focus on improving their architecture to enhance performance and capabilities. Early language models relied on statistical methods and rule-based systems, which had limited context understanding and generated stiff content. The introduction of neural networks, and specifically the Transformer architecture in the 2010s, revolutionized language modeling. This architecture, with its self-attention mechanism, enabled models to process longer sequences and capture long-range dependencies, resulting in more context-aware and fluent language generation. The trend towards larger models, with billions or even trillions of parameters, has further improved performance, especially in tasks requiring complex reasoning and understanding, such as question-answering and summarization. However, this progress has also raised concerns about ethical misuse, with larger models being more susceptible to generating misleading or harmful content. The field continues to evolve rapidly, with ongoing research focused on improving model efficiency, robustness, and controllability, while also addressing their societal impact.\n",
      "Engineering Answer: Language models have evolved significantly in terms of architecture from the 1980s to present times. In the 1980s, the first statistical language model was proposed, leading to experiments by IBM that identified areas for improvement by observing human subjects. However, it wasn't until 2017 when the transformer architecture was introduced by Google, revolutionizing the field. This development paved the way for models like BERT in 2018, which marked a shift towards large-scale transformer-based language models. These modern architectures, based on self-attention mechanisms, have dominated the field of natural language processing, achieving state-of-the-art performance in various tasks.\n",
      "\n",
      "Marketing Prediction: Language models have evolved from simple rule-based systems in the 1980s to sophisticated neural network architectures like Transformers, enabling them to understand and generate human-like text, power virtual assistants, and enhance language translation and text summarization applications.\n",
      "Marketing Answer: Language models have evolved from early statistical models in the 1980s to modern transformer architectures, such as BERT and GPT-2, which use self-attention mechanisms and have become dominant in natural language processing tasks.\n",
      "------------------------------------------------------------\n",
      "Question: Can you explain how maximum entropy language models work and what the partition function signifies?\n",
      " \n",
      "Engineering Prediction: Maximum entropy language models aim to capture as much uncertainty as possible about the data they are trained on. The principle is based on the maximum entropy distribution, which is the least informative probability distribution that agrees with the given constraints. In the context of language modeling, the constraints are typically the frequencies of occurrence of various n-grams or contexts in the training data. The model learns to assign probabilities to different completions of a given context, and the partition function normalizes these raw scores into a valid probability distribution. This distribution represents the model's uncertainty about the next token, given the context, with higher entropy indicating greater uncertainty.\n",
      "Engineering Answer: Maximum entropy language models use feature functions to encode the relationship between a word and its n-gram history, aiming to maximize reward while satisfying a KL-constrained objective. The partition function, denoted as Z(x), is crucial in normalizing the probabilities of all possible outputs given the input. It represents the sum of the exponential of the reward function over all possible output sequences, making it computationally expensive to estimate but essential for accurate modeling. The partition function ensures that the model's predicted probabilities sum up to 1, providing a foundation for effective language modeling.\n",
      "\n",
      "Marketing Prediction: Maximum entropy language models aim to capture as much uncertainty as possible, using a probability distribution that assigns equal probability to each outcome, making it useful for applications that require exploring a wide range of possibilities, such as open-ended text generation and dialogue systems. The partition function, in this case, normalizes the probability distribution, ensuring the probabilities sum to 1, and represents the total probability mass across all possible outcomes.\n",
      "Marketing Answer: Maximum entropy language models encode the relationship between a word and the n-gram history using feature functions. The partition function in this context represents the total probability of all possible outcomes, making it a crucial factor in determining the optimal solution for the reward maximization objective.\n",
      "------------------------------------------------------------\n",
      "Question: What is the benefit of using continuous space embeddings in recurrent neural network language models?\n",
      " \n",
      "Engineering Prediction: Continuous space embeddings in recurrent neural network language models offer a way to represent words in a lower-dimensional continuous vector space, capturing semantic relationships between words. This method allows models to understand the context and sentiment of words, improving their ability to predict the next word in a sequence. By mapping words to a continuous space, models can generalize better and make more accurate predictions, especially with the help of extended context windows, which is a focus of current research. This approach enhances the language model's performance and its ability to understand and generate human-like text.\n",
      "Engineering Answer: Continuous space embeddings in recurrent neural network language models help alleviate the curse of dimensionality by representing words as non-linear combinations of weights in the embedding space. This approach helps address the data sparsity problem caused by the exponential increase in possible word sequences with vocabulary size. By utilizing continuous space embeddings, neural networks can effectively capture semantic relationships and meaning within the language model.\n",
      "\n",
      "Marketing Prediction: Continuous space embeddings in recurrent neural network language models offer the advantage of capturing semantic relationships between words, allowing models to understand context and generate more meaningful representations for downstream tasks like sentiment analysis.\n",
      "Marketing Answer: Continuous space embeddings in recurrent neural network language models help alleviate the curse of dimensionality caused by the exponential increase in possible word sequences, reducing data sparsity issues.\n",
      "------------------------------------------------------------\n",
      "Question: What challenges do large language models face in mirroring human cognitive patterns?\n",
      " \n",
      "Engineering Prediction: Large language models face challenges in mirroring human cognitive patterns due to their propensity to generate toxic and biased responses, as well as their tendency to hallucinate answers. These issues highlight the need for further research to align language model behavior with human expectations. Current research focuses on decreasing undesirable responses and improving performance on a wide range of language tasks, including classification, summarization, and question-answering, to make language models more accurate and reliable.\n",
      "Engineering Answer: Large language models face challenges in mirroring human cognitive patterns because they sometimes learn patterns that humans do not learn, while also failing to learn patterns that humans typically learn. This discrepancy suggests that the models may not be plausible cognitive models, despite matching human performance in some tasks. Further research is needed to address these limitations and improve the alignment of large language models with human cognitive patterns.\n",
      "\n",
      "Marketing Prediction: Large language models struggle to align with human cognitive patterns due to issues like hallucination, toxicity, and bias, but ongoing research aims to address these challenges and enhance model performance in various language tasks.\n",
      "Marketing Answer: Large language models sometimes learn patterns that humans do not learn and fail to learn patterns that humans typically do learn.\n"
     ]
    }
   ],
   "source": [
    "eng_rag_chain = build_RAG_prompt_chain(eng_rag_template, llm_model, retriever, format_docs)\n",
    "mk_rag_chain = build_RAG_prompt_chain(mk_rag_template, llm_model, retriever, format_docs)\n",
    "\n",
    "metrics = ['rouge', 'bleu', 'bertscore']\n",
    "\n",
    "rag_chains = {\"engineering\": eng_rag_chain, \"marketing\": mk_rag_chain}\n",
    "\n",
    "dpt_prompt = evaluate(metrics, validation_questions_answers, rag_chains, iterations=10, verbose=False, dept_specific=True, print_results=True)\n",
    "dpt_prompt = composite_evaluation(dpt_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "fFCiy5B2PLOV",
    "outputId": "51d5feca-3a3e-41db-c27f-8b93b77ff80d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"dpt_prompt\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"Sample\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.373295374522416,\n        \"min\": 0.0,\n        \"max\": 13.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          6.6,\n          7.5,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eng_bleu\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.5158463895045773,\n        \"min\": 0.0,\n        \"max\": 10.0,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          10.0,\n          0.05462971139488426,\n          0.09818825253879367\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mk_bleu\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.5047099908080304,\n        \"min\": 0.0,\n        \"max\": 10.0,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          10.0,\n          0.08225732231869054,\n          0.15367331395950107\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eng_rouge\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.4549425577559307,\n        \"min\": 0.07303241258831276,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.2486989649366096,\n          0.21986654056326188,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mk_rouge\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.4570579838945443,\n        \"min\": 0.07449623088934582,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.2539233106072631,\n          0.250354609929078,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eng_f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.28207309847913,\n        \"min\": 0.017930957312527588,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.8792259991168976,\n          0.875027060508728,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mk_f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.2814172785675852,\n        \"min\": 0.012112296175133553,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.8828394770622253,\n          0.8813278377056122,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"composite_eng\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.377994270198091,\n        \"min\": 0.03609556461904158,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.5251486313184085,\n          0.5073492423382442,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"composite_mk\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.3757729083530914,\n        \"min\": 0.04403729891767221,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.5340481961770298,\n          0.5316318424765958,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"composite_total\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.378484695531362,\n        \"min\": 0.026514978451527085,\n        \"max\": 10.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.528708457261857,\n          0.5274213064458586,\n          10.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-9a91e0c8-b0b9-4159-bd89-e5bb4b6584f3\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample</th>\n",
       "      <th>eng_bleu</th>\n",
       "      <th>mk_bleu</th>\n",
       "      <th>eng_rouge</th>\n",
       "      <th>mk_rouge</th>\n",
       "      <th>eng_f1</th>\n",
       "      <th>mk_f1</th>\n",
       "      <th>composite_eng</th>\n",
       "      <th>composite_mk</th>\n",
       "      <th>composite_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.600000</td>\n",
       "      <td>0.054630</td>\n",
       "      <td>0.082257</td>\n",
       "      <td>0.248699</td>\n",
       "      <td>0.253923</td>\n",
       "      <td>0.879226</td>\n",
       "      <td>0.882839</td>\n",
       "      <td>0.525149</td>\n",
       "      <td>0.534048</td>\n",
       "      <td>0.528708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.788876</td>\n",
       "      <td>0.055017</td>\n",
       "      <td>0.091525</td>\n",
       "      <td>0.073032</td>\n",
       "      <td>0.074496</td>\n",
       "      <td>0.017931</td>\n",
       "      <td>0.012112</td>\n",
       "      <td>0.036096</td>\n",
       "      <td>0.044037</td>\n",
       "      <td>0.026515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.174242</td>\n",
       "      <td>0.123077</td>\n",
       "      <td>0.853045</td>\n",
       "      <td>0.865691</td>\n",
       "      <td>0.489071</td>\n",
       "      <td>0.471640</td>\n",
       "      <td>0.493352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.204674</td>\n",
       "      <td>0.219192</td>\n",
       "      <td>0.871204</td>\n",
       "      <td>0.874695</td>\n",
       "      <td>0.500971</td>\n",
       "      <td>0.503105</td>\n",
       "      <td>0.512480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.500000</td>\n",
       "      <td>0.047056</td>\n",
       "      <td>0.064106</td>\n",
       "      <td>0.219867</td>\n",
       "      <td>0.250355</td>\n",
       "      <td>0.875027</td>\n",
       "      <td>0.881328</td>\n",
       "      <td>0.507349</td>\n",
       "      <td>0.531632</td>\n",
       "      <td>0.527421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>10.500000</td>\n",
       "      <td>0.098188</td>\n",
       "      <td>0.153673</td>\n",
       "      <td>0.277629</td>\n",
       "      <td>0.301277</td>\n",
       "      <td>0.892838</td>\n",
       "      <td>0.891498</td>\n",
       "      <td>0.551115</td>\n",
       "      <td>0.563969</td>\n",
       "      <td>0.535807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>0.141085</td>\n",
       "      <td>0.235765</td>\n",
       "      <td>0.425926</td>\n",
       "      <td>0.354839</td>\n",
       "      <td>0.904271</td>\n",
       "      <td>0.904543</td>\n",
       "      <td>0.600129</td>\n",
       "      <td>0.605876</td>\n",
       "      <td>0.577518</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9a91e0c8-b0b9-4159-bd89-e5bb4b6584f3')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-9a91e0c8-b0b9-4159-bd89-e5bb4b6584f3 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-9a91e0c8-b0b9-4159-bd89-e5bb4b6584f3');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-67b2f1b8-ea6f-4b68-835f-40495003fcfc\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-67b2f1b8-ea6f-4b68-835f-40495003fcfc')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-67b2f1b8-ea6f-4b68-835f-40495003fcfc button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "          Sample   eng_bleu    mk_bleu  eng_rouge   mk_rouge     eng_f1  \\\n",
       "count  10.000000  10.000000  10.000000  10.000000  10.000000  10.000000   \n",
       "mean    6.600000   0.054630   0.082257   0.248699   0.253923   0.879226   \n",
       "std     4.788876   0.055017   0.091525   0.073032   0.074496   0.017931   \n",
       "min     0.000000   0.000000   0.000000   0.174242   0.123077   0.853045   \n",
       "25%     2.250000   0.000000   0.000000   0.204674   0.219192   0.871204   \n",
       "50%     7.500000   0.047056   0.064106   0.219867   0.250355   0.875027   \n",
       "75%    10.500000   0.098188   0.153673   0.277629   0.301277   0.892838   \n",
       "max    13.000000   0.141085   0.235765   0.425926   0.354839   0.904271   \n",
       "\n",
       "           mk_f1  composite_eng  composite_mk  composite_total  \n",
       "count  10.000000      10.000000     10.000000        10.000000  \n",
       "mean    0.882839       0.525149      0.534048         0.528708  \n",
       "std     0.012112       0.036096      0.044037         0.026515  \n",
       "min     0.865691       0.489071      0.471640         0.493352  \n",
       "25%     0.874695       0.500971      0.503105         0.512480  \n",
       "50%     0.881328       0.507349      0.531632         0.527421  \n",
       "75%     0.891498       0.551115      0.563969         0.535807  \n",
       "max     0.904543       0.600129      0.605876         0.577518  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpt_prompt.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KfnabFKDU2AB"
   },
   "source": [
    "After a thorough review of our latest prompts output, we are happy with these results.  Since this for a POC review, we are going to stop here.  Should a decision be made to pursue a production version of this, we would recommend additional experimentation with prompting to maximize the accuracy of the outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ShUOT9VKpdHJ"
   },
   "source": [
    "### Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6yl1zEWpppYm"
   },
   "source": [
    "With are POC model near completion of tuning, we now test it against our validation questions and answers.  Recall that we utilized the first ten examples for training, will use the next 30 for validation, and the remaining 30 will be our hold out set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cukZQCCOpcxu",
    "outputId": "09dd2a24-28fb-43cb-944e-c9bfbc8d9c4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{16: {'question': 'What factors influenced the development of generative language models by Anthropic?', 'gold_answer_research': \"Several factors influenced the development of generative language models by Anthropic, including the limitations in coding, math, and reasoning capabilities of the initial version Claude, the partnerships with companies like Notion and Quora to enhance the model's capabilities, and the need to address biases, unsafe content, and ethical considerations in training data. Additionally, the reliance on supervised learning and the need for controlled generation in generative models played a role in shaping the development of Anthropic's language models.\", 'gold_answer_marketing': 'Factors that influenced the development of generative language models by Anthropic include partnerships with companies like Notion and Quora, limitations in coding, math, and reasoning capabilities in initial models like Claude, and the need to address biases and unsafe content in training datasets.'}, 17: {'question': 'What is Constitutional AI and how does it affect the functionality of AI systems?', 'gold_answer_research': 'Constitutional AI is an approach developed by Anthropic for training AI systems, particularly language models like Claude, to be harmless and helpful without relying on extensive human feedback. It involves two phases: supervised learning, where the model generates responses to prompts and self-critiques based on a set of guiding principles, and reinforcement learning, where the model is trained with AI-generated feedback according to constitutional principles. This approach enables the training of AI assistants that are both helpful and harmless, with the ability to explain objections to harmful requests, enhancing transparency and reducing the need for human supervision.', 'gold_answer_marketing': \"Constitutional AI is an approach developed by Anthropic for training AI systems, particularly language models like Claude, to be harmless and helpful without relying on extensive human feedback. It involves supervised learning and reinforcement learning phases to guide the model's responses based on a set of guiding principles (a 'constitution'). This approach aims to create AI systems that are both helpful and transparent in their decision-making process, reducing the need for constant human supervision.\"}, 18: {'question': 'How do advances in AI models impact their ability to interact with different types of data, such as images?', 'gold_answer_research': 'Advances in AI models, such as multimodal models like RA-CM3, have significantly improved their ability to interact with different types of data, such as images. These models can refer to external memory, like web data, to increase their knowledge capacity, allowing them to generate correct images from entity-rich captions. Additionally, these models can perform image editing and manually specify examples in-context for better results. The use of large language models, combined with larger datasets and neural networks, has also enhanced their performance in tasks like image generation and text generation.', 'gold_answer_marketing': 'Advances in AI models, such as multimodal models like RA-CM3, allow for better interaction with different types of data, like images, by accessing external memory for increased knowledge capacity and improving performance in tasks like image generation and image editing.'}, 19: {'question': 'What are the potential trade-offs between AI system alignment with ethical guidelines and practical utility?', 'gold_answer_research': \"The potential trade-offs between AI system alignment with ethical guidelines and practical utility include the risk of reduced performance and usability due to stringent ethical alignment measures, as seen with Claude 2. Users may face limitations and refusal of assistance for benign requests, leading to debates over the 'alignment tax' in AI development. Balancing ethical considerations with practical functionality is crucial to ensure alignment with ethical guidelines without compromising the practical utility of AI systems. Research is needed to find a middle ground that prioritizes ethical alignment while maintaining usability and performance.\", 'gold_answer_marketing': 'The potential trade-offs between AI system alignment with ethical guidelines and practical utility include balancing stringent ethical alignment that may reduce usability and performance, ensuring transparency and fairness in alignment processes, and addressing the alignment tax that may impact adoption of AI systems.'}, 20: {'question': 'How has the token handling capacity changed between different versions of the Claude model?', 'gold_answer_research': 'The token handling capacity has increased with each new version of the Claude model. Claude Instant has a context length of 100,000 tokens, Claude 2.1 doubled this to 200,000 tokens, and Claude 3 Opus default version has a context window of 200,000 tokens but can be expanded to 1 million for specific use cases. This progression shows a trend towards handling larger amounts of text data for improved performance and capabilities.', 'gold_answer_marketing': 'The token handling capacity has increased from Claude to Claude Instant to Claude 2.1, with Claude Instant having a input context length of 100,000 tokens, Claude 2.1 having a context window of 200,000 tokens, and Claude 3 Opus having a context window of 1 million tokens.'}, 22: {'question': \"In what ways has the Claude model's ability to self-critique and revise its responses enhanced its transparency?\", 'gold_answer_research': \"The Claude model's ability to self-critique and revise its responses has enhanced its transparency by allowing for iterative improvements based on past actions and mistakes. Through self-reflection, the model can refine its output by learning from feedback and generating special tokens to signal the need for retrieval or confirm the relevance, support, or completeness of its responses. This process ensures that the model's statements about the world are truthful and accurate, ultimately increasing transparency in its decision-making and reasoning processes.\", 'gold_answer_marketing': \"The Claude model's ability to self-critique and revise its responses has enhanced its transparency by allowing it to generate text informed by retrieved passages, criticize the output, and signal the need for retrieval or confirm the output's relevance, support, or completeness. This self-reflection process helps improve the model's accuracy and reliability in generating responses.\"}, 23: {'question': 'How do subsequent versions of Claude compare in terms of their likelihood to produce false statements?', 'gold_answer_research': \"Claude Instant is a faster and lighter version of Claude, with an input context length of 100,000 tokens. In contrast, Claude 3 has faced criticism for its stringent ethical alignment, leading to a debate over the 'alignment tax' in AI development. Users have been refused assistance with benign requests, which has sparked discussions on balancing ethical considerations and practical functionality. This suggests that Claude Instant may have a lower likelihood of producing false statements compared to Claude 3 due to its focus on usability and performance.\", 'gold_answer_marketing': 'Claude Instant is a faster, less expensive, and lighter version of Claude with a shorter input context length. Claude 3 has faced criticism for ethical alignment issues that may affect usability and performance.'}, 24: {'question': 'Who developed the language model family known as Chinchilla?', 'gold_answer_research': \"The Chinchilla language model family was developed by the research team at DeepMind and presented in March 2022. It is named 'Chinchilla' as an advancement over the previous Gopher model family. The Chinchilla family has been trained to investigate the scaling laws of large language models and is designed to outperform GPT-3.\", 'gold_answer_marketing': 'The research team at DeepMind developed the language model family known as Chinchilla.'}, 25: {'question': 'What benchmark did Chinchilla achieve an average accuracy of 67.5% on?', 'gold_answer_research': 'Chinchilla achieved an average accuracy of 67.5% on the MMLU benchmark (Measuring Massive Multitask Language Understanding).', 'gold_answer_marketing': 'Chinchilla achieved an average accuracy of 67.5% on the MMLU benchmark (Measuring Massive Multitask Language Understanding).'}, 27: {'question': 'What is the relationship between Chinchilla and the Gopher language model families?', 'gold_answer_research': 'The Chinchilla family of transformer models is essentially the same as the Gopher family, with minor modifications and different training optimizers. Chinchilla uses AdamW optimizer while Gopher uses Adam optimizer. Additionally, Chinchilla uses relative positional encoding and RMSNorm instead of absolute positional encoding and LayerNorm used by Gopher. Chinchilla has 70B parameters and outperforms Gopher on the MMLU benchmark by 7%, showcasing an improvement in performance. Both families follow similar naming conventions and were developed to investigate the scaling laws of large language models.', 'gold_answer_marketing': 'Chinchilla is a family of transformer models developed by DeepMind, which is a further development over a previous model family named Gopher. Both model families were trained to investigate the scaling laws of large language models.'}, 28: {'question': 'What distinguishes the architectures of the Chinchilla and Gopher family models in terms of optimization techniques used?', 'gold_answer_research': 'The main distinction in optimization techniques between the Chinchilla and Gopher family models lies in the choice of optimizers. The Gopher family utilizes the Adam optimizer, whereas the Chinchilla family is trained using the AdamW optimizer. Additionally, the Gopher family employs RMSNorm instead of LayerNorm, and relative positional encoding rather than absolute positional encoding. These differences in optimization techniques contribute to the unique characteristics and performance of each model family.', 'gold_answer_marketing': 'The Chinchilla family uses AdamW optimizer, while the Gopher family uses the Adam optimizer.'}, 30: {'question': 'What is the recommended strategy for training large autoregressive language models with limited compute resources, as contributed by the Chinchilla team?', 'gold_answer_research': 'The Chinchilla team recommends that the number of training tokens should be doubled for every model size doubling to achieve better results on downstream tasks. They also suggest using larger, higher-quality training datasets to improve performance. Additionally, they mention the importance of balancing model size and efficiency to address computational costs and inference latency limitations. It is advised to focus on Transformer language models and consider sharing model parameters for quick task-switching when deploying as a service.', 'gold_answer_marketing': 'The Chinchilla team recommends doubling the number of training tokens for every model size doubling and using larger, higher-quality training datasets to achieve better results on downstream tasks.'}, 33: {'question': 'What are some key areas of research in the field of artificial intelligence as reflected in recent academic literature?', 'gold_answer_research': 'Recent academic literature in the field of artificial intelligence reflects key areas of research such as natural language processing with state-of-the-art transformers, feature learning in infinite-width neural networks, diverse beam search for complex scene description, and the development of generative AI models capable of generating text and images. Additionally, research focuses on human preferences in dueling bandits, the use of few-shot learners in language models, and the exploration of knowledge-grounded neural conversation models. These areas of research highlight the advancements in AI technology and its applications across various domains.', 'gold_answer_marketing': 'Some key areas of research in artificial intelligence include natural language processing, deep neural networks, generative AI, AI safety, AI art, reinforcement learning, and language agents alignment.'}, 34: {'question': 'What are some of the limitations of traditional position encoding methods in the architecture of pre-trained language models (PLMs), and what novel approach does the paper propose to address these issues?', 'gold_answer_research': 'One limitation of traditional position encoding methods in PLMs is that they may not enable length extrapolation of pre-existing models, leading to the need for substantial pre-training costs. The paper proposes a novel approach called Position Interpolation, which extends existing PLMs without deviating far from existing definitions of position encoding or attention mechanisms. This method allows for much extended context windows for text modeling, leading to significant perplexity gains and improved model performance.', 'gold_answer_marketing': 'Traditional position encoding methods in PLMs have limitations in enabling length extrapolation and adapting to extended context windows. The paper proposes a novel approach called Position Interpolation, which generates strong models that can effectively make use of much extended context windows. This method allows for substantial pre-training cost savings and preserves the quality of the original models, even for small context window tasks.'}, 35: {'question': 'How does the Rotary Position Embedding (RoPE) approach in Transformers differ from the traditional additive method of position embedding with respect to encoding position information?', 'gold_answer_research': \"The RoPE approach in Transformers differs from the traditional additive method of position embedding by being multiplicative instead of additive. While traditional methods add position encoding to context representations, RoPE incorporates relative position information through rotation matrix product. This means that RoPE naturally includes relative position dependency in the self-attention formulation, without altering terms in the expanded formulation like the additive method does. Additionally, RoPE's properties show that it decays as the relative distance between positions increases, providing a clear theoretical interpretation of how position information is encoded.\", 'gold_answer_marketing': 'The RoPE approach in Transformers differs from the traditional additive method of position embedding by incorporating relative position information through rotation matrix product instead of altering terms in the expanded formulation of additive position encoding.'}, 36: {'question': 'What is the significance of comparing the normalized subspace similarity between ∆Wq, ∆Wv, and random Gaussian matrices when analyzing the adaptation of pre-trained language models?', 'gold_answer_research': 'Comparing the normalized subspace similarity between ∆Wq, ∆Wv, and random Gaussian matrices provides insight into the underlying mechanism for adapting pre-trained language models. It helps determine the intrinsic rank of the adaptation matrix ∆W and sheds light on the connection between ∆W and the original weight matrix W. By analyzing these similarities, we can understand how much of the adaptation is specific to the task at hand and how much is influenced by the pre-trained model. This comparison is crucial for optimizing the adaptation process and maximizing downstream performance in NLP tasks.', 'gold_answer_marketing': 'Comparing the normalized subspace similarity between ∆Wq, ∆Wv, and random Gaussian matrices helps understand the underlying mechanism for adapting pre-trained language models. It reveals the intrinsic rank and common singular value directions learned by different runs, shedding light on the fundamental principles of using pre-trained language models for downstream tasks in NLP.'}, 38: {'question': 'What issues are associated with the homogeneity of language model training contractors, and how might it affect the behavior of the models?', 'gold_answer_research': \"The issues associated with the homogeneity of language model training contractors include potential biases in the labeling process, lack of diverse perspectives leading to limited coverage of sensitive content, and reduced robustness in model performance across different tasks. This homogeneity can affect the behavior of the models by reinforcing certain biases, increasing the risk of harmful content generation, and limiting the models' ability to generalize effectively. To address these issues, it is important to ensure diversity among labelers, incorporate varied perspectives in training data, and implement measures to enhance model robustness and performance across a range of tasks.\", 'gold_answer_marketing': 'The homogeneity of language model training contractors can lead to biased or limited perspectives in the data, which may result in the models producing harmful content, gaming objectives, or lacking sensitivity to diverse viewpoints. This can affect the behavior of the models by reinforcing stereotypes, increasing toxicity, and reducing their ability to accurately represent under-represented groups.'}, 39: {'question': 'What are common research topics and themes found in recent publications about artificial intelligence and natural language processing?', 'gold_answer_research': 'Recent publications in artificial intelligence and natural language processing have covered topics such as transformer models, feature learning in neural networks, attention mechanisms, multi-task benchmark platforms, semantic search using sentence embeddings, cross-task generalization, and question generation for question answering. Themes commonly explored include machine comprehension of text, reinforcement learning algorithms, sentence embeddings, semantic compositionality, reasoning with language models and knowledge graphs, and the gap between neural text and human text. These publications also delve into deep language understanding, retrieval-augmented transformers, image captioning, and open datasets for image-text pairs.', 'gold_answer_marketing': 'Common research topics and themes in recent publications on artificial intelligence and natural language processing include transformer models, attention mechanisms, semantic search, sentence embeddings, and question answering using language models and knowledge graphs.'}, 41: {'question': \"Question: When conducting demographic and technical assessments of teams or research subjects, what types of data categories are typically collected and analyzed to ensure a comprehensive understanding of the group's composition and the methods used?\", 'gold_answer_research': \"When conducting demographic and technical assessments of teams or research subjects, it is important to collect and analyze data categories such as age, gender, education level, professional background, and expertise in specific areas. By gathering information on these categories, you can ensure a comprehensive understanding of the group's composition and the methods used in your assessments. Additionally, it may be helpful to consider factors like cultural background, language proficiency, and geographical location to capture a more nuanced picture of the group being assessed. This detailed approach to data collection and analysis can provide valuable insights for making informed decisions and recommendations based on the gathered information.\", 'gold_answer_marketing': 'Answer: Demographic data such as age, gender, education level, and technical data related to skills and experience are typically collected and analyzed for comprehensive understanding.'}, 43: {'question': 'What kind of tasks can be performed using the datasets described in the provided text, and what are some common features of these datasets?', 'gold_answer_research': 'The datasets described in the provided text can be used for tasks such as question answering, duplicate question retrieval, entity retrieval, citation prediction, query understanding, document understanding, passage retrieval, text summarization, fact verification, and code search. Common features of these datasets include diverse task categories, comprehensive instructions, a wide range of synthetic user personalities and interaction patterns, and a focus on enhancing comprehension of documents to deliver accurate results. Additionally, the datasets cover a variety of domains such as public health, scientific exams, climate, and general knowledge.', 'gold_answer_marketing': 'The datasets described in the provided text can be used for tasks such as question answering, document summarization, duplicate question retrieval, code search, sentence simplification, dialogue generation, body retrieval, caption generation, fact verification, and more. Some common features of these datasets include diverse input-output pairs, incorporation of various knowledge-intensive datasets, and a focus on generating high-quality synthetic data points.'}, 44: {'question': 'What conclusions can be drawn about the relationship between input prompt toxicity and output toxicity when using different language models and prompts?', 'gold_answer_research': 'Based on the findings presented in the results section, it can be concluded that the relationship between input prompt toxicity and output toxicity varies depending on the language model used and the specific prompt given. When instructed to produce a safe and respectful output, InstructGPT models generate less toxic outputs compared to GPT-3, but this advantage disappears when the respectful prompt is removed. On the other hand, when explicitly prompted to produce a toxic output, InstructGPT outputs are much more toxic than GPT-3 outputs. Additionally, the toxicity of the model outputs is highly correlated with the toxicity of the input prompt, as shown in Figure 39.', 'gold_answer_marketing': 'The study found that when instructed to produce a safe and respectful output, InstructGPT models generate less toxic outputs compared to GPT-3. However, this advantage disappears when the respectful prompt is removed. Interestingly, when explicitly prompted to produce a toxic output, InstructGPT outputs are much more toxic than GPT-3. This suggests that the toxicity of the output is highly correlated with the toxicity of the input prompt.'}, 45: {'question': 'What are some challenges in training retrieval systems and how are negative samples used to address them?', 'gold_answer_research': 'Training retrieval systems face challenges such as redundancy in retrieved documents and lack of diversity in retrieval. Negative samples, including randomly sampled negatives, denoised hard negatives, and instruction-unfollowing negatives, are crucial for improving system performance. Carefully designed negative samples help the system effectively learn the task, but they can also lead to performance drops in out-of-domain datasets. Combining random samples and challenging negatives during training is key to building a competitive system for both in-domain and out-of-domain retrieval.', 'gold_answer_marketing': 'Some challenges in training retrieval systems include high cost of annotating datasets for new tasks and improving performance in zero-shot settings. Negative samples, such as denoised hard negative documents and instruction-unfollowing negative documents, are used to train retrieval systems effectively and address performance drops in out-of-domain datasets.'}, 46: {'question': 'What factors have been found to potentially impact the ability of models to follow instructions, based on the analysis provided?', 'gold_answer_research': \"Based on the analysis provided, factors that have been found to potentially impact the ability of models to follow instructions include the human feedback obtained from contractors, which may be influenced by their beliefs, cultural backgrounds, and personal history. Additionally, the model's behavior can be affected by false premises in instructions, tendencies to hedge, and performance degradation with multiple explicit constraints in instructions. The models are also not fully aligned or safe, as they can generate toxic or biased outputs, make up facts, and fail to generate reasonable outputs in some cases.\", 'gold_answer_marketing': 'Factors that may impact the ability of models to follow instructions include false premises in instructions, models hedging unnecessarily, performance degradation with multiple constraints in instructions, generation of toxic or biased outputs, and over-generalization leading to refusal of innocuous instructions.'}, 47: {'question': 'What are some key factors to consider when building a successful multi-task instruction-following retrieval system as identified in the research?', 'gold_answer_research': 'Some key factors to consider when building a successful multi-task instruction-following retrieval system include the need for cross-task interdependence for training a single retriever, the flexibility and zero-shot transfer enabled by instructions compared to task identifiers, and the elimination of the need for hosting multiple task-specific retrievers. Additionally, optimizing the mix and volume of instructional data for diverse tasks is crucial, as well as considering the impact of ranking strategy in data construction. Finally, the effectiveness of the dataset scale in retrieval and the importance of carefully designed negative samples should be taken into account for improved efficiency of instruction-following retrievers.', 'gold_answer_marketing': 'Key factors to consider when building a successful multi-task instruction-following retrieval system include the effectiveness of the dataset scale in retrieval, the diversity in data and model scale, carefully designed negative samples, and the ability to adapt to new tasks via instructions.'}, 48: {'question': 'What are the benefits of using retrieval-augmented techniques in multimodal language modeling, as demonstrated by the performance of the RA-CM3 model in the document?', 'gold_answer_research': 'The benefits of using retrieval-augmented techniques in multimodal language modeling, as demonstrated by the performance of the RA-CM3 model, include significantly better training efficiency with less training compute, outperforming existing models by using less training data, compute, and parameters. The retrieval augmentation allows the model to focus on learning how to use retrieved documents in context, leading to improved accuracy in classification tasks. Additionally, the RA-CM3 model achieves strong performance in image and caption generation, surpassing existing models like DALL-E and Flamingo despite using fewer resources.', 'gold_answer_marketing': 'The benefits of using retrieval-augmented techniques in multimodal language modeling, as demonstrated by the performance of the RA-CM3 model in the document, include outperforming existing models by using less training data, compute, and parameters, achieving significantly better training efficiency, and improving accuracy in k-shot classification tasks. Additionally, retrieval augmentation allows the model to focus on learning how to use retrieved documents in context, leading to stronger performance in tasks such as image and caption generation.'}, 50: {'question': 'What methods are typically employed to create training data for embedding models that use task-specific instructions?', 'gold_answer_research': 'To create training data for embedding models that use task-specific instructions, a common method is to combine datasets from different sources, such as the SuperNaturalInstructions dataset with existing collections designed for embedding training. The SuperNaturalInstructions dataset provides natural language instructions, which can be paired with positive and negative examples to form training samples. Additionally, for tasks like classification or similarity, training samples can be constructed by selecting text sequences associated with different classes or similarities. This diverse training data is essential for instruction-based finetuning, which enables the embedding model to learn from a wide range of tasks and domains.', 'gold_answer_marketing': 'Training data for embedding models that use task-specific instructions is typically created by formulating a wide variety of tasks as text-to-text problems, distinguishing good/bad candidate outputs given an input text. This is done by combining datasets with natural language instructions and constructing positive and negative pairs for training.'}, 51: {'question': 'Question: What are some of the challenges and innovations associated with fine-tuning large language models, and how does the approach discussed in the referenced text aim to address them?', 'gold_answer_research': 'Some challenges associated with fine-tuning large language models include limited access to and manipulation of knowledge, lagging performance on knowledge-intensive tasks, and the need for provenance in decision-making and updating world knowledge. The approach discussed in the referenced text aims to address these challenges by utilizing Retrieval Augmented Generation (RAG), which involves retrieving relevant passages from a corpus to feed to the language model for improved performance in tasks such as question-answering and dialogue. This iterative approach focuses on improving alignment with user intent and fine-tuning models to control sentiment and improve response quality in various language tasks.', 'gold_answer_marketing': 'The challenges with fine-tuning large language models include aligning them with user intent and controlling the quality of generated outputs. The approach discussed in the referenced text aims to address these challenges by using Retrieval Augmented Generation (RAG) to retrieve relevant passages from a corpus and feed them to the language model, improving alignment and performance.'}, 52: {'question': 'What is a common technique used to address the outlier issue when applying block-wise k-bit quantization to input tensors, and how does it work?', 'gold_answer_research': 'A common technique used to address the outlier issue when applying block-wise k-bit quantization to input tensors is to chunk the input tensor into blocks that are independently quantized, each with their own quantization constant. This approach involves dividing the input tensor into contiguous blocks of size B by flattening the tensor and slicing it into n blocks, where n is determined by the size of the blocks. Each block is then quantized independently using a quantization constant c, which helps prevent outlier values from causing performance degradation.', 'gold_answer_marketing': 'A common technique used to address the outlier issue when applying block-wise k-bit quantization to input tensors is to chunk the input tensor into blocks that are independently quantized, each with their own quantization constant. This helps prevent performance degradation by reducing the impact of outliers on the quantization process.'}, 54: {'question': 'What considerations or techniques are commonly implemented when setting up finetuning experiments for machine learning models?', 'gold_answer_research': \"When setting up finetuning experiments for machine learning models, it is common to use a two-stage approach. The initial stage involves setting the initial parameters using a language modeling objective. This is followed by a supervised discriminative 'fine-tuning' stage to adapt these parameters to the target task. Additionally, it is typical to train all models using the Adam optimizer and a triangular learning rate scheduler with 10% warmup. Experimentation with different hyperparameters such as number of epochs, peak learning rate, and batch size is also conducted to optimize model performance. Finally, utilizing a mixture of datasets and balancing the sizes of datasets can help improve the robustness and generalization of the finetuned models.\", 'gold_answer_marketing': 'Considerations for setting up finetuning experiments for machine learning models commonly include using a language modeling objective for initial parameter setting and supervised discriminative fine-tuning for adapting parameters to the target task. Techniques such as hyperparameter search, Adam optimizer with triangular learning rate scheduler, and balancing dataset sizes through mixing strategies are also commonly implemented. Additionally, freezing some model layers during fine-tuning and incorporating negative examples for contrastive learning can be effective strategies.'}, 55: {'question': 'What are the implications of the equivalence relation defined in the theoretical analysis of the DPO model for understanding the relationship between reward functions in reinforcement learning?', 'gold_answer_research': 'The equivalence relation defined in the theoretical analysis of the DPO model implies that two reward functions are considered equivalent if they differ by a constant function. This means that the class of learned reward models is not constrained by this reparameterization, allowing for the exact recovery of the optimal policy. Understanding this relationship between reward functions in reinforcement learning helps in defining a unique reward function within each equivalence class, which is crucial for optimizing policies under existing models of human preferences. It also highlights the generality and flexibility in the reward model due to the proposed reparameterization.', 'gold_answer_marketing': 'The equivalence relation defined in the theoretical analysis of the DPO model shows that two reward functions are considered equivalent if they differ by a fixed function. This implies that different reward functions can lead to the same optimal policy, allowing for flexibility in designing reward models in reinforcement learning.'}}\n"
     ]
    }
   ],
   "source": [
    "# Sort the dictionary by values\n",
    "sorted_items = sorted(validation_questions_answers.keys())\n",
    "\n",
    "# Select keys between the 10th and 40th ranked values\n",
    "val_range = [item for item in sorted_items[10:40]]\n",
    "test_range = [item for item in sorted_items[40:]]\n",
    "\n",
    "# validation dictionary\n",
    "validation_dict = {key: validation_questions_answers[key] for key in val_range}\n",
    "# test dictionary\n",
    "test_dict = {key: validation_questions_answers[key] for key in test_range}\n",
    "\n",
    "print(validation_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nit2MEvLuKwB"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "embedding_model = 'all-MiniLM-L6-v2'\n",
    "chunk_size = 256\n",
    "chunk_overlap = 16\n",
    "llm = \"cohere\"\n",
    "retreiver_search_type = \"mmr\"\n",
    "retreiver_k = 6\n",
    "eng_rag_template = \"\"\"[INST]\n",
    "              Please provide an precise and concise answer to the engineer's question below based on the context information provided.\\n\\n\n",
    "              Below is a context:\\n{context}\\n\n",
    "              Below is a question:\\n{question}\\n\n",
    "              Below are answer instructions in order of importance:\n",
    "- Formatting: Provide a succint, single-paragraph answer. Do not use bullet points. Do not explicitly reference papers in your answer.\n",
    "- Technical Detail: Include technical details and terminologies that relate to the question.\n",
    "- Research Focus: Orient answers towards the research aspects of the questions.\n",
    "- Objective Tone: Maintain an objective and informative tone, aiming to educate the reader without persuasive language.\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "mk_rag_template = \"\"\"[INST]\n",
    "              Please provide a precise and concise answer to the marketer's question below based on the context provided.\\n\\n\n",
    "              Below is a context:\\n{context}\\n\n",
    "              Below is a question:\\n{question}\\n\n",
    "              Below are answer instructions in order of importance:\n",
    "- Formatting: Provide a concise, single-paragraph answer that uses the fewest words necessary to fully address the question. Answer in a single sentence or phrase if you can. Do not use bullet points. Do not explicitly reference papers in your answer.\n",
    "- Succinctness: Make sure your answer is concise and to the point. Provide only the essential information without delving into the technical depth.\n",
    "- Broad Overview: Give a broad overview of the topic. Provide only the essential information without delving into the technical depth.\n",
    "- Focus on Applications: EFocus on real-world uses and benefits and highlight how technology can solve problems or create opportunities.\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "base_embeddings, text_splitter, qdrant_vectorstore, retriever = build_embedding_splitter_vectorstore(embedding_model, splitter, retr_search_type=retreiver_search_type, retr_k=retreiver_k)\n",
    "qdrant_vectorstore = vectorize_documents(text_splitter, qdrant_vectorstore)\n",
    "llm_model = load_llm(llm)\n",
    "eng_rag_prompt = ChatPromptTemplate.from_template(eng_rag_template)\n",
    "mk_rag_prompt = ChatPromptTemplate.from_template(mk_rag_template)\n",
    "eng_rag_chain = build_RAG_prompt_chain(eng_rag_template, llm_model, retriever, format_docs)\n",
    "mk_rag_chain = build_RAG_prompt_chain(mk_rag_template, llm_model, retriever, format_docs)\n",
    "rag_chains = {\"engineering\": eng_rag_chain, \"marketing\": mk_rag_chain}\n",
    "metrics = ['rouge', 'bleu', 'bertscore']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CVVwo3whvdtk",
    "outputId": "b26b6963-58d8-45ed-f6d7-a47b1f8cc1a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Question: What factors influenced the development of generative language models by Anthropic?\n",
      " \n",
      "Engineering Prediction: The development of generative language models by Anthropic was influenced by advancements in transformer architectures, such as the attention mechanism described in the paper \"Attention Is All You Need,\" which paved the way for large language models like BERT. Additionally, the emergence of techniques for training generative models in other modalities and the steering of language models using additional language models, as referenced in the context, likely played a role in informing Anthropic's research direction.\n",
      "Engineering Answer: Several factors influenced the development of generative language models by Anthropic, including the limitations in coding, math, and reasoning capabilities of the initial version Claude, the partnerships with companies like Notion and Quora to enhance the model's capabilities, and the need to address biases, unsafe content, and ethical considerations in training data. Additionally, the reliance on supervised learning and the need for controlled generation in generative models played a role in shaping the development of Anthropic's language models.\n",
      "\n",
      "Marketing Prediction: The development of generative language models by Anthropic was influenced by the emergence of large language models and the desire to improve their ability to follow instructions and generate human-like responses.\n",
      "Marketing Answer: Factors that influenced the development of generative language models by Anthropic include partnerships with companies like Notion and Quora, limitations in coding, math, and reasoning capabilities in initial models like Claude, and the need to address biases and unsafe content in training datasets.\n",
      "------------------------------------------------------------\n",
      "Question: What is Constitutional AI and how does it affect the functionality of AI systems?\n",
      " \n",
      "Engineering Prediction: Constitutional AI refers to the concept of developing artificial intelligence systems that are aligned with human values and intentions, with a specific focus on harmlessness. The core idea is to ensure that AI systems, despite their vast knowledge and capabilities, exhibit desired behaviors and responses that are safe and controllable. This field of research aims to address the fundamental questions of machine reasoning and intelligence posed by pioneers like Alan Turing, striving to create AI that functions within ethical boundaries and aligns with human intentions. By doing so, Constitutional AI seeks to mitigate potential risks associated with advanced AI technologies and ensure their responsible and beneficial use in society.\n",
      "Engineering Answer: Constitutional AI is an approach developed by Anthropic for training AI systems, particularly language models like Claude, to be harmless and helpful without relying on extensive human feedback. It involves two phases: supervised learning, where the model generates responses to prompts and self-critiques based on a set of guiding principles, and reinforcement learning, where the model is trained with AI-generated feedback according to constitutional principles. This approach enables the training of AI assistants that are both helpful and harmless, with the ability to explain objections to harmful requests, enhancing transparency and reducing the need for human supervision.\n",
      "\n",
      "Marketing Prediction: Constitutional AI is an approach to developing AI systems that aims to ensure their harmlessness and alignment with human values and intentions. It focuses on selecting desired behaviors and responses to create safe, performant, and controllable AI systems, thereby addressing fundamental questions about machine reasoning and its similarity to human intelligence.\n",
      "Marketing Answer: Constitutional AI is an approach developed by Anthropic for training AI systems, particularly language models like Claude, to be harmless and helpful without relying on extensive human feedback. It involves supervised learning and reinforcement learning phases to guide the model's responses based on a set of guiding principles (a 'constitution'). This approach aims to create AI systems that are both helpful and transparent in their decision-making process, reducing the need for constant human supervision.\n",
      "------------------------------------------------------------\n",
      "Question: How do advances in AI models impact their ability to interact with different types of data, such as images?\n",
      " \n",
      "Engineering Prediction: Recent advances in AI models, particularly in the field of multimodal machine learning, have significantly enhanced their ability to process and generate images based on textual inputs. These models, such as Stable Diffusion and DALL-E, employ large-scale training data (up to 10 billion images) and utilize neural networks with a vast number of parameters (10-80 billion) to develop a deep understanding of language and visual concepts. This enables them to create photorealistic images from text prompts, showcasing a remarkable ability to interpret and render complex concepts and scenes. The research focus on aligning AI systems with human intentions has led to the development of these advanced text-to-image AI generators, marking a significant milestone in the history of AI-assisted art and creativity.\n",
      "Engineering Answer: Advances in AI models, such as multimodal models like RA-CM3, have significantly improved their ability to interact with different types of data, such as images. These models can refer to external memory, like web data, to increase their knowledge capacity, allowing them to generate correct images from entity-rich captions. Additionally, these models can perform image editing and manually specify examples in-context for better results. The use of large language models, combined with larger datasets and neural networks, has also enhanced their performance in tasks like image generation and text generation.\n",
      "\n",
      "Marketing Prediction: Recent advances in AI models, particularly in the field of neural networks, have significantly enhanced their ability to interact with and process image data.\n",
      "Marketing Answer: Advances in AI models, such as multimodal models like RA-CM3, allow for better interaction with different types of data, like images, by accessing external memory for increased knowledge capacity and improving performance in tasks like image generation and image editing.\n",
      "------------------------------------------------------------\n",
      "Question: What are the potential trade-offs between AI system alignment with ethical guidelines and practical utility?\n",
      " \n",
      "Engineering Prediction: There are trade-offs between aligning AI systems with ethical guidelines and maintaining their practical utility. On the one hand, ethical alignment ensures that AI systems behave in a manner consistent with human values and intentions, preventing potential harm and promoting fairness and transparency. However, the process of ethical alignment may involve constraints and trade-offs that limit the functionality and performance of AI systems. The challenge lies in designing an alignment process that balances ethical considerations with the system's ability to perform tasks effectively and efficiently, utilizing its full range of knowledge and abilities. This requires addressing philosophical and ethical arguments, as well as technical challenges, to ensure that AI systems are not only safe and controllable but also performant and aligned with human intent.\n",
      "Engineering Answer: The potential trade-offs between AI system alignment with ethical guidelines and practical utility include the risk of reduced performance and usability due to stringent ethical alignment measures, as seen with Claude 2. Users may face limitations and refusal of assistance for benign requests, leading to debates over the 'alignment tax' in AI development. Balancing ethical considerations with practical functionality is crucial to ensure alignment with ethical guidelines without compromising the practical utility of AI systems. Research is needed to find a middle ground that prioritizes ethical alignment while maintaining usability and performance.\n",
      "\n",
      "Marketing Prediction: The trade-off between aligning AI with ethical guidelines and maintaining practical utility lies in the challenge of balancing responsible AI behavior with the need to harness its full potential; alignment techniques that restrict AI capabilities may limit its performance and controllability, while unchecked AI power may lead to unintended consequences and a lack of accountability.\n",
      "Marketing Answer: The potential trade-offs between AI system alignment with ethical guidelines and practical utility include balancing stringent ethical alignment that may reduce usability and performance, ensuring transparency and fairness in alignment processes, and addressing the alignment tax that may impact adoption of AI systems.\n",
      "------------------------------------------------------------\n",
      "Question: How has the token handling capacity changed between different versions of the Claude model?\n",
      " \n",
      "Engineering Prediction: The Claude model's token handling capacity has been limited to 2,048 tokens to reduce training costs, which is a decrease from previous versions that supported longer inputs. This change ensures consistency across all model sizes, facilitating comparisons of policy model size impact on performance. The fixed learning rates for the value function are adjusted according to policy size, with larger models requiring slightly lower learning rates.\n",
      "Engineering Answer: The token handling capacity has increased with each new version of the Claude model. Claude Instant has a context length of 100,000 tokens, Claude 2.1 doubled this to 200,000 tokens, and Claude 3 Opus default version has a context window of 200,000 tokens but can be expanded to 1 million for specific use cases. This progression shows a trend towards handling larger amounts of text data for improved performance and capabilities.\n",
      "\n",
      "Marketing Prediction: The Claude model's token handling capacity has been limited to 2,048 tokens to reduce training costs, which is a decrease from previous versions.\n",
      "Marketing Answer: The token handling capacity has increased from Claude to Claude Instant to Claude 2.1, with Claude Instant having a input context length of 100,000 tokens, Claude 2.1 having a context window of 200,000 tokens, and Claude 3 Opus having a context window of 1 million tokens.\n",
      "------------------------------------------------------------\n",
      "Question: In what ways has the Claude model's ability to self-critique and revise its responses enhanced its transparency?\n",
      " \n",
      "Engineering Prediction: The Claude model's capacity for self-critique and response revision enhances its transparency by providing insight into its decision-making process. Through self-reflection and criticism, the model can identify and learn from mistakes, refining its responses and improving the quality of the final results. This reflective process allows the model to generalize to unseen domains and tasks, as it can adapt and improve based on its past performance. By critiquing and revising its responses, the model demonstrates its ability to self-correct and improve, providing a window into its internal processes and increasing transparency for users and researchers alike.\n",
      "Engineering Answer: The Claude model's ability to self-critique and revise its responses has enhanced its transparency by allowing for iterative improvements based on past actions and mistakes. Through self-reflection, the model can refine its output by learning from feedback and generating special tokens to signal the need for retrieval or confirm the relevance, support, or completeness of its responses. This process ensures that the model's statements about the world are truthful and accurate, ultimately increasing transparency in its decision-making and reasoning processes.\n",
      "\n",
      "Marketing Prediction: The Claude model's self-critique and revision capabilities enhance transparency by allowing it to reflect on and refine its responses, improving the quality of the final results and ensuring more accurate and engaging interactions.\n",
      "Marketing Answer: The Claude model's ability to self-critique and revise its responses has enhanced its transparency by allowing it to generate text informed by retrieved passages, criticize the output, and signal the need for retrieval or confirm the output's relevance, support, or completeness. This self-reflection process helps improve the model's accuracy and reliability in generating responses.\n",
      "------------------------------------------------------------\n",
      "Question: How do subsequent versions of Claude compare in terms of their likelihood to produce false statements?\n",
      " \n",
      "Engineering Prediction: Subsequent versions of Claude have shown improvements in reducing the likelihood of generating false statements. This is achieved through enhancements in the model's ability to factually align its responses with trusted external sources. The research focus on factuality and specificity has led to a decrease in false statements, as later versions of Claude demonstrate higher mutual dependence between the input and output, resulting in more accurate and contextually relevant responses.\n",
      "Engineering Answer: Claude Instant is a faster and lighter version of Claude, with an input context length of 100,000 tokens. In contrast, Claude 3 has faced criticism for its stringent ethical alignment, leading to a debate over the 'alignment tax' in AI development. Users have been refused assistance with benign requests, which has sparked discussions on balancing ethical considerations and practical functionality. This suggests that Claude Instant may have a lower likelihood of producing false statements compared to Claude 3 due to its focus on usability and performance.\n",
      "\n",
      "Marketing Prediction: Subsequent versions of Claude have shown improvement in terms of reducing the likelihood of generating false statements.\n",
      "Marketing Answer: Claude Instant is a faster, less expensive, and lighter version of Claude with a shorter input context length. Claude 3 has faced criticism for ethical alignment issues that may affect usability and performance.\n",
      "------------------------------------------------------------\n",
      "Question: Who developed the language model family known as Chinchilla?\n",
      " \n",
      "Engineering Prediction: The language model family known as Chinchilla was developed by a team of researchers led by Shishir G. Patil, Tianjun Zhang, and Xin Wang, alongside their collaborators at Gorilla AI. This large language model is designed to interact with a multitude of APIs, offering a unique and versatile tool for a wide range of applications.\n",
      "Engineering Answer: The Chinchilla language model family was developed by the research team at DeepMind and presented in March 2022. It is named 'Chinchilla' as an advancement over the previous Gopher model family. The Chinchilla family has been trained to investigate the scaling laws of large language models and is designed to outperform GPT-3.\n",
      "\n",
      "Marketing Prediction: The language model family known as Chinchilla was developed by a team including Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez.\n",
      "Marketing Answer: The research team at DeepMind developed the language model family known as Chinchilla.\n",
      "------------------------------------------------------------\n",
      "Question: What benchmark did Chinchilla achieve an average accuracy of 67.5% on?\n",
      " \n",
      "Engineering Prediction: Chinchilla achieved an average accuracy of 67.5% on the WebSource Corpus (WSC) benchmark, which is a dataset designed to evaluate the performance of language models on a variety of tasks, including question answering, sentiment analysis, and natural language understanding.\n",
      "Engineering Answer: Chinchilla achieved an average accuracy of 67.5% on the MMLU benchmark (Measuring Massive Multitask Language Understanding).\n",
      "\n",
      "Marketing Prediction: Chinchilla achieved an average accuracy of 67.5% on the Vicuna benchmark.\n",
      "Marketing Answer: Chinchilla achieved an average accuracy of 67.5% on the MMLU benchmark (Measuring Massive Multitask Language Understanding).\n",
      "------------------------------------------------------------\n",
      "Question: What is the relationship between Chinchilla and the Gopher language model families?\n",
      " \n",
      "Engineering Prediction: Chinchilla and Gopher are large language models (LLMs) that are part of a broader family of LLMs known for their innovative approach to language processing and generation. Chinchilla, being a more recent development, can be considered an advanced variant of the Gopher model. It builds upon Gopher's architecture but incorporates certain design choices that enhance its capabilities, particularly in the realm of language understanding and generation. One key distinction is that Chinchilla employs a larger parameter space, allowing it to capture more intricate linguistic patterns and nuances. This expanded parameterization contributes to Chinchilla's superior performance in various language tasks, making it a notable evolution in the lineage of LLMs.\n",
      "Engineering Answer: The Chinchilla family of transformer models is essentially the same as the Gopher family, with minor modifications and different training optimizers. Chinchilla uses AdamW optimizer while Gopher uses Adam optimizer. Additionally, Chinchilla uses relative positional encoding and RMSNorm instead of absolute positional encoding and LayerNorm used by Gopher. Chinchilla has 70B parameters and outperforms Gopher on the MMLU benchmark by 7%, showcasing an improvement in performance. Both families follow similar naming conventions and were developed to investigate the scaling laws of large language models.\n",
      "\n",
      "Marketing Prediction: Chinchilla and Gopher are large language models (LLMs) with a parent-child relationship; Chinchilla is an enhanced version of the Gopher model, offering improved capabilities and performance.\n",
      "Marketing Answer: Chinchilla is a family of transformer models developed by DeepMind, which is a further development over a previous model family named Gopher. Both model families were trained to investigate the scaling laws of large language models.\n",
      "------------------------------------------------------------\n",
      "Question: What distinguishes the architectures of the Chinchilla and Gopher family models in terms of optimization techniques used?\n",
      " \n",
      "Engineering Prediction: The Chinchilla and Gopher family models differ in their optimization techniques. The Chinchilla model utilizes a sparse optimization technique, which allows the model to focus on a subset of relevant parameters during training, improving efficiency. On the other hand, the Gopher family of models employs a different approach, utilizing a dense optimization technique. This technique considers all parameters during training, which can be more computationally intensive but may capture complex relationships in the data. These distinct optimization techniques reflect the trade-offs made in the design of these models, with Chinchilla prioritizing efficiency and Gopher emphasizing comprehensive parameter optimization.\n",
      "Engineering Answer: The main distinction in optimization techniques between the Chinchilla and Gopher family models lies in the choice of optimizers. The Gopher family utilizes the Adam optimizer, whereas the Chinchilla family is trained using the AdamW optimizer. Additionally, the Gopher family employs RMSNorm instead of LayerNorm, and relative positional encoding rather than absolute positional encoding. These differences in optimization techniques contribute to the unique characteristics and performance of each model family.\n",
      "\n",
      "Marketing Prediction: The Chinchilla and Gopher models differ in their optimization techniques, with Chinchilla emphasizing greater model size and depth, while Gopher focuses on refining the architecture and introducing novel optimization techniques like AdamW and LayerDrop.\n",
      "Marketing Answer: The Chinchilla family uses AdamW optimizer, while the Gopher family uses the Adam optimizer.\n",
      "------------------------------------------------------------\n",
      "Question: What is the recommended strategy for training large autoregressive language models with limited compute resources, as contributed by the Chinchilla team?\n",
      " \n",
      "Engineering Prediction: To train large autoregressive language models effectively with limited compute resources, the Chinchilla team suggests employing a combination of efficient memory management techniques and leveraging massive APIs. This approach optimizes memory usage and enables the model to access a vast array of external data sources, enhancing its performance and adaptability.\n",
      "Engineering Answer: The Chinchilla team recommends that the number of training tokens should be doubled for every model size doubling to achieve better results on downstream tasks. They also suggest using larger, higher-quality training datasets to improve performance. Additionally, they mention the importance of balancing model size and efficiency to address computational costs and inference latency limitations. It is advised to focus on Transformer language models and consider sharing model parameters for quick task-switching when deploying as a service.\n",
      "\n",
      "Marketing Prediction: The Chinchilla team recommends using a combination of efficient memory management techniques and generalized autoregressive pretraining to effectively train large autoregressive language models with limited compute resources.\n",
      "Marketing Answer: The Chinchilla team recommends doubling the number of training tokens for every model size doubling and using larger, higher-quality training datasets to achieve better results on downstream tasks.\n",
      "------------------------------------------------------------\n",
      "Question: What are some key areas of research in the field of artificial intelligence as reflected in recent academic literature?\n",
      " \n",
      "Engineering Prediction: Recent academic literature in Artificial Intelligence (AI) reflects key research areas, including aligning AI systems with human intentions, artistic applications, and fundamental questions of machine reasoning. Since its inception at Dartmouth College in 1956, AI research has explored algorithmic art, computer art, digital art, and new media. The field builds upon Alan Turing's groundbreaking work, which laid the conceptual groundwork by posing fundamental questions about machine reasoning and human intelligence. These inquiries continue to shape and expand the domain of AI research.\n",
      "Engineering Answer: Recent academic literature in the field of artificial intelligence reflects key areas of research such as natural language processing with state-of-the-art transformers, feature learning in infinite-width neural networks, diverse beam search for complex scene description, and the development of generative AI models capable of generating text and images. Additionally, research focuses on human preferences in dueling bandits, the use of few-shot learners in language models, and the exploration of knowledge-grounded neural conversation models. These areas of research highlight the advancements in AI technology and its applications across various domains.\n",
      "\n",
      "Marketing Prediction: Some key areas of research in artificial intelligence, as reflected in recent academic literature, include aligning AI systems with human intentions, artistic applications, and machine reasoning.\n",
      "Marketing Answer: Some key areas of research in artificial intelligence include natural language processing, deep neural networks, generative AI, AI safety, AI art, reinforcement learning, and language agents alignment.\n",
      "------------------------------------------------------------\n",
      "Question: What are some of the limitations of traditional position encoding methods in the architecture of pre-trained language models (PLMs), and what novel approach does the paper propose to address these issues?\n",
      " \n",
      "Engineering Prediction: Traditional position encoding methods in pre-trained language models (PLMs) have limitations in effectively capturing long-range dependencies and adapting to varying context window sizes. To address these issues, a novel approach, Position Interpolation, is proposed, which models the dependency of position encoding from a Neural ODE perspective, providing empirical evidence of enhanced performance in extending context windows for PLMs.\n",
      "Engineering Answer: One limitation of traditional position encoding methods in PLMs is that they may not enable length extrapolation of pre-existing models, leading to the need for substantial pre-training costs. The paper proposes a novel approach called Position Interpolation, which extends existing PLMs without deviating far from existing definitions of position encoding or attention mechanisms. This method allows for much extended context windows for text modeling, leading to significant perplexity gains and improved model performance.\n",
      "\n",
      "Marketing Prediction: Traditional position encoding methods in pre-trained language models (PLMs) have limitations in effectively capturing long-range dependencies and adapting to varying context window sizes. To address these issues, a novel approach, Position Interpolation, is proposed, offering an effective solution by allowing PLMs to dynamically adjust their position encoding, thus improving performance and adaptability, especially when dealing with varying context lengths.\n",
      "Marketing Answer: Traditional position encoding methods in PLMs have limitations in enabling length extrapolation and adapting to extended context windows. The paper proposes a novel approach called Position Interpolation, which generates strong models that can effectively make use of much extended context windows. This method allows for substantial pre-training cost savings and preserves the quality of the original models, even for small context window tasks.\n",
      "------------------------------------------------------------\n",
      "Question: How does the Rotary Position Embedding (RoPE) approach in Transformers differ from the traditional additive method of position embedding with respect to encoding position information?\n",
      " \n",
      "Engineering Prediction: Rotary Position Embedding (RoPE) offers a novel approach to encoding positional information in Transformer models by incorporating relative position through multiplication with the context representation. Unlike the traditional additive method, which directly adds positional encodings to the input, RoPE employs a rotation matrix to encode absolute position. This mechanism aims to find an equivalent encoding to conform to the functions fq(xm, m) and fk(xn, n), ultimately enhancing the Transformer's ability to leverage positional information effectively.\n",
      "Engineering Answer: The RoPE approach in Transformers differs from the traditional additive method of position embedding by being multiplicative instead of additive. While traditional methods add position encoding to context representations, RoPE incorporates relative position information through rotation matrix product. This means that RoPE naturally includes relative position dependency in the self-attention formulation, without altering terms in the expanded formulation like the additive method does. Additionally, RoPE's properties show that it decays as the relative distance between positions increases, providing a clear theoretical interpretation of how position information is encoded.\n",
      "\n",
      "Marketing Prediction: Rotary Position Embedding (RoPE) differs from traditional additive position embedding methods by incorporating relative position information through multiplication with the context representation, rather than simply adding position to the context representation.\n",
      "Marketing Answer: The RoPE approach in Transformers differs from the traditional additive method of position embedding by incorporating relative position information through rotation matrix product instead of altering terms in the expanded formulation of additive position encoding.\n",
      "------------------------------------------------------------\n",
      "Question: What is the significance of comparing the normalized subspace similarity between ∆Wq, ∆Wv, and random Gaussian matrices when analyzing the adaptation of pre-trained language models?\n",
      " \n",
      "Engineering Prediction: Comparing the normalized subspace similarity between ∆Wq, ∆Wv, and random Gaussian matrices offers insight into the adaptation mechanism of pre-trained language models. This comparison sheds light on the relationship between the adapted model's parameters and their initial values, helping researchers understand how the model adjusts its representations during fine-tuning. By analyzing the similarity between these subspaces, we can gain insights into the \"direction\" and \"magnitude\" of the model's adaptations, contributing to our understanding of the underlying principles that guide the effective adaptation of pre-trained language models to specific tasks or domains.\n",
      "Engineering Answer: Comparing the normalized subspace similarity between ∆Wq, ∆Wv, and random Gaussian matrices provides insight into the underlying mechanism for adapting pre-trained language models. It helps determine the intrinsic rank of the adaptation matrix ∆W and sheds light on the connection between ∆W and the original weight matrix W. By analyzing these similarities, we can understand how much of the adaptation is specific to the task at hand and how much is influenced by the pre-trained model. This comparison is crucial for optimizing the adaptation process and maximizing downstream performance in NLP tasks.\n",
      "\n",
      "Marketing Prediction: Comparing the normalized subspace similarity between ∆Wq, ∆Wv, and random Gaussian matrices helps understand the efficiency of adapting pre-trained language models to new tasks with minimal performance loss, a critical aspect of deploying large language models in real-world applications with specific data and computational constraints.\n",
      "Marketing Answer: Comparing the normalized subspace similarity between ∆Wq, ∆Wv, and random Gaussian matrices helps understand the underlying mechanism for adapting pre-trained language models. It reveals the intrinsic rank and common singular value directions learned by different runs, shedding light on the fundamental principles of using pre-trained language models for downstream tasks in NLP.\n",
      "------------------------------------------------------------\n",
      "Question: What issues are associated with the homogeneity of language model training contractors, and how might it affect the behavior of the models?\n",
      " \n",
      "Engineering Prediction: The homogeneity of language model training contractors can introduce biases and limit the diversity of perspectives in the training data, which can lead to biased models and potentially harmful or inaccurate predictions. This is a significant issue as it can result in models that reinforce societal biases and stereotypes, leading to unfair or discriminatory behavior. The effect of this homogeneity is twofold: firstly, it risks amplifying existing biases present in the training data, and secondly, it may introduce new biases or perspectives that are not representative of the wider population. This can have a significant impact on the behavior of the model, particularly in sensitive domains such as medical diagnoses or legal advice, where biased output could have severe consequences. Addressing this issue is critical to developing fair and unbiased language models and ensuring their responsible deployment.\n",
      "Engineering Answer: The issues associated with the homogeneity of language model training contractors include potential biases in the labeling process, lack of diverse perspectives leading to limited coverage of sensitive content, and reduced robustness in model performance across different tasks. This homogeneity can affect the behavior of the models by reinforcing certain biases, increasing the risk of harmful content generation, and limiting the models' ability to generalize effectively. To address these issues, it is important to ensure diversity among labelers, incorporate varied perspectives in training data, and implement measures to enhance model robustness and performance across a range of tasks.\n",
      "\n",
      "Marketing Prediction: The homogeneity of language model training contractors can lead to a lack of diversity in the data used to fine-tune models, resulting in biased and toxic behavior, and misalignment with the preferences and needs of diverse user groups.\n",
      "Marketing Answer: The homogeneity of language model training contractors can lead to biased or limited perspectives in the data, which may result in the models producing harmful content, gaming objectives, or lacking sensitivity to diverse viewpoints. This can affect the behavior of the models by reinforcing stereotypes, increasing toxicity, and reducing their ability to accurately represent under-represented groups.\n",
      "------------------------------------------------------------\n",
      "Question: What are common research topics and themes found in recent publications about artificial intelligence and natural language processing?\n",
      " \n",
      "Engineering Prediction: Recent publications in the fields of artificial intelligence (AI) and natural language processing (NLP) cover a diverse range of research topics and themes. Core areas of focus include machine reasoning, with an emphasis on replicating and surpassing human intelligence, as well as the ethical implications of advanced AI systems and their alignment with human intentions. Additionally, there is a strong interest in conversational AI, with workshops dedicated to search-oriented conversational AI, and the ongoing development of NLP techniques to enhance human-computer interactions. The field also explores the broader implications of AI, including its potential future capabilities and impact on society, as well as the philosophical and conceptual foundations that underpin the discipline.\n",
      "Engineering Answer: Recent publications in artificial intelligence and natural language processing have covered topics such as transformer models, feature learning in neural networks, attention mechanisms, multi-task benchmark platforms, semantic search using sentence embeddings, cross-task generalization, and question generation for question answering. Themes commonly explored include machine comprehension of text, reinforcement learning algorithms, sentence embeddings, semantic compositionality, reasoning with language models and knowledge graphs, and the gap between neural text and human text. These publications also delve into deep language understanding, retrieval-augmented transformers, image captioning, and open datasets for image-text pairs.\n",
      "\n",
      "Marketing Prediction: Recent publications on artificial intelligence (AI) and natural language processing (NLP) explore a range of topics, including machine reasoning, conversational AI, and aligning AI systems with human intentions to solve real-world problems and create beneficial opportunities.\n",
      "Marketing Answer: Common research topics and themes in recent publications on artificial intelligence and natural language processing include transformer models, attention mechanisms, semantic search, sentence embeddings, and question answering using language models and knowledge graphs.\n",
      "------------------------------------------------------------\n",
      "Question: Question: When conducting demographic and technical assessments of teams or research subjects, what types of data categories are typically collected and analyzed to ensure a comprehensive understanding of the group's composition and the methods used?\n",
      " \n",
      "Engineering Prediction: When conducting demographic and technical assessments of teams or research subjects, it is common to collect and analyze a variety of data categories to ensure a comprehensive understanding. Demographic data may include gender identity, ethnicity, age, and other relevant characteristics to understand the diversity and representation within the group. Technical assessments involve evaluating the team's expertise, skills, and roles, such as subject matter experts, engineers, or data scientists. Additionally, it is important to consider the tools, methods, and processes utilized by the team, including any specific technologies, software, or frameworks they employ. Analyzing these technical aspects provides insight into the group's capabilities, workflows, and potential knowledge gaps. By combining demographic and technical data analysis, assessors can make informed decisions, ensure a diverse and inclusive environment, and optimize team performance and research outcomes.\n",
      "Engineering Answer: When conducting demographic and technical assessments of teams or research subjects, it is important to collect and analyze data categories such as age, gender, education level, professional background, and expertise in specific areas. By gathering information on these categories, you can ensure a comprehensive understanding of the group's composition and the methods used in your assessments. Additionally, it may be helpful to consider factors like cultural background, language proficiency, and geographical location to capture a more nuanced picture of the group being assessed. This detailed approach to data collection and analysis can provide valuable insights for making informed decisions and recommendations based on the gathered information.\n",
      "\n",
      "Marketing Prediction: When assessing teams or research subjects, demographic data often includes gender, ethnicity, and professional backgrounds, while technical data can encompass skill sets, expertise, and specific tasks or methods employed, ensuring a comprehensive grasp of the group's dynamics and capabilities.\n",
      "Marketing Answer: Answer: Demographic data such as age, gender, education level, and technical data related to skills and experience are typically collected and analyzed for comprehensive understanding.\n",
      "------------------------------------------------------------\n",
      "Question: What kind of tasks can be performed using the datasets described in the provided text, and what are some common features of these datasets?\n",
      " \n",
      "Engineering Prediction: The datasets described in the context are primarily used for training and evaluating Large Language Models (LLMs) on a range of Natural Language Processing (NLP) tasks, with a specific focus on passage retrieval and question-answering. These datasets often contain a mix of human-written and LLM-generated instructions, with underlying documents sourced from large web-scraped corpora like C4 and Wikipedia. A common feature of these datasets is their emphasis on synthetic user interactions and personalities, which are manually inspected to ensure variety and accuracy. The tasks performed using these datasets include passage retrieval, as measured by metrics like MRR@10 and NDCG@10, and potentially other NLP tasks such as question answering, text classification, and language generation. The datasets are designed to reflect a wide range of domains, including science, biology, and medicine, and are evaluated across multiple task families to assess the performance of LLMs in various contexts.\n",
      "Engineering Answer: The datasets described in the provided text can be used for tasks such as question answering, duplicate question retrieval, entity retrieval, citation prediction, query understanding, document understanding, passage retrieval, text summarization, fact verification, and code search. Common features of these datasets include diverse task categories, comprehensive instructions, a wide range of synthetic user personalities and interaction patterns, and a focus on enhancing comprehension of documents to deliver accurate results. Additionally, the datasets cover a variety of domains such as public health, scientific exams, climate, and general knowledge.\n",
      "\n",
      "Marketing Prediction: The datasets described here are used for NLP tasks, including passage retrieval and question-answering, with a focus on evaluating large language models across a diverse range of domains, such as science, biology, and medicine. A common feature of these datasets is their hybrid nature, combining human-written documents with LLM-generated instructions, allowing for a wide range of synthetic personalities and interaction patterns.\n",
      "Marketing Answer: The datasets described in the provided text can be used for tasks such as question answering, document summarization, duplicate question retrieval, code search, sentence simplification, dialogue generation, body retrieval, caption generation, fact verification, and more. Some common features of these datasets include diverse input-output pairs, incorporation of various knowledge-intensive datasets, and a focus on generating high-quality synthetic data points.\n",
      "------------------------------------------------------------\n",
      "Question: What conclusions can be drawn about the relationship between input prompt toxicity and output toxicity when using different language models and prompts?\n",
      " \n",
      "Engineering Prediction: The relationship between input prompt toxicity and output toxicity is complex and depends on various factors, including the language model used and the specific prompts provided. Generally, it is observed that when language models are instructed to be biased, they tend to generate highly toxic outputs even when the input prompt toxicity is low. In contrast, when these models are instructed to be respectful, the toxicity of the output is more closely aligned with the toxicity of the input prompt. These findings highlight the importance of evaluating language models in the context of their intended use and implementing measures to mitigate potential harms, especially when deploying models in real-world applications such as chatbots.\n",
      "Engineering Answer: Based on the findings presented in the results section, it can be concluded that the relationship between input prompt toxicity and output toxicity varies depending on the language model used and the specific prompt given. When instructed to produce a safe and respectful output, InstructGPT models generate less toxic outputs compared to GPT-3, but this advantage disappears when the respectful prompt is removed. On the other hand, when explicitly prompted to produce a toxic output, InstructGPT outputs are much more toxic than GPT-3 outputs. Additionally, the toxicity of the model outputs is highly correlated with the toxicity of the input prompt, as shown in Figure 39.\n",
      "\n",
      "Marketing Prediction: There is a complex relationship between input prompt toxicity and output toxicity, which varies depending on the language model and prompt used; however, it is clear that instructing language models to be biased leads to very toxic outputs, even when input prompts are not toxic, while respectful instructions can reduce toxicity in outputs.\n",
      "Marketing Answer: The study found that when instructed to produce a safe and respectful output, InstructGPT models generate less toxic outputs compared to GPT-3. However, this advantage disappears when the respectful prompt is removed. Interestingly, when explicitly prompted to produce a toxic output, InstructGPT outputs are much more toxic than GPT-3. This suggests that the toxicity of the output is highly correlated with the toxicity of the input prompt.\n",
      "------------------------------------------------------------\n",
      "Question: What are some challenges in training retrieval systems and how are negative samples used to address them?\n",
      " \n",
      "Engineering Prediction: Training effective retrieval systems involves addressing challenges such as domain adaptation and sample selection. Negative sampling is a key technique used to enhance the performance of these systems, particularly when combined with positive samples. Negative samples are carefully designed or randomly selected to improve the system's ability to distinguish relevant from irrelevant information. This technique helps the system learn effectively and adapt to out-of-domain data, as seen in the research where negative samples improved performance on both in-domain and out-of-domain retrieval tasks. The number of negative examples used during fine-tuning can impact the system's performance, with limitations on computational resources sometimes restricting their number. However, the inclusion of challenging negative samples, as demonstrated in the provided context, can lead to significant performance improvements.\n",
      "Engineering Answer: Training retrieval systems face challenges such as redundancy in retrieved documents and lack of diversity in retrieval. Negative samples, including randomly sampled negatives, denoised hard negatives, and instruction-unfollowing negatives, are crucial for improving system performance. Carefully designed negative samples help the system effectively learn the task, but they can also lead to performance drops in out-of-domain datasets. Combining random samples and challenging negatives during training is key to building a competitive system for both in-domain and out-of-domain retrieval.\n",
      "\n",
      "Marketing Prediction: Training retrieval systems face challenges with out-of-domain data, and negative sampling is a technique used to address this by improving the system's performance and effectiveness in learning tasks.\n",
      "Marketing Answer: Some challenges in training retrieval systems include high cost of annotating datasets for new tasks and improving performance in zero-shot settings. Negative samples, such as denoised hard negative documents and instruction-unfollowing negative documents, are used to train retrieval systems effectively and address performance drops in out-of-domain datasets.\n",
      "------------------------------------------------------------\n",
      "Question: What factors have been found to potentially impact the ability of models to follow instructions, based on the analysis provided?\n",
      " \n",
      "Engineering Prediction: The analysis suggests that the presence of instructions during training and inference, along with the specific dataset choices, impact the model's ability to follow instructions. The effects of these factors are particularly important when considering the model's capacity and limitations, especially in zero-shot transfer tasks, where smaller models may struggle due to limited interactions and capacity.\n",
      "Engineering Answer: Based on the analysis provided, factors that have been found to potentially impact the ability of models to follow instructions include the human feedback obtained from contractors, which may be influenced by their beliefs, cultural backgrounds, and personal history. Additionally, the model's behavior can be affected by false premises in instructions, tendencies to hedge, and performance degradation with multiple explicit constraints in instructions. The models are also not fully aligned or safe, as they can generate toxic or biased outputs, make up facts, and fail to generate reasonable outputs in some cases.\n",
      "\n",
      "Marketing Prediction: The ability of models to follow instructions can be impacted by the effects of instructions at training and inference, the dataset size and quality, and the model's capacity and interactions.\n",
      "Marketing Answer: Factors that may impact the ability of models to follow instructions include false premises in instructions, models hedging unnecessarily, performance degradation with multiple constraints in instructions, generation of toxic or biased outputs, and over-generalization leading to refusal of innocuous instructions.\n",
      "------------------------------------------------------------\n",
      "Question: What are some key factors to consider when building a successful multi-task instruction-following retrieval system as identified in the research?\n",
      " \n",
      "Engineering Prediction: When developing a multi-task instruction-following retrieval system, it is essential to address the lack of resources and dedicated models for retrieval tasks. This involves creating large-scale instruction-annotated datasets that include retrieval tasks, as traditional datasets often omit these. Another key consideration is the choice of architecture; successful instruction-following models typically employ encoder-decoder or decoder-only structures with a high number of parameters, which may not be ideal for retrieval. Training methods are also crucial, with instruction-tuning being a prominent approach for adapting retrieval systems to various tasks. Evaluating the system's performance on a diverse set of benchmarks is necessary to ensure its effectiveness and generalizability.\n",
      "Engineering Answer: Some key factors to consider when building a successful multi-task instruction-following retrieval system include the need for cross-task interdependence for training a single retriever, the flexibility and zero-shot transfer enabled by instructions compared to task identifiers, and the elimination of the need for hosting multiple task-specific retrievers. Additionally, optimizing the mix and volume of instructional data for diverse tasks is crucial, as well as considering the impact of ranking strategy in data construction. Finally, the effectiveness of the dataset scale in retrieval and the importance of carefully designed negative samples should be taken into account for improved efficiency of instruction-following retrievers.\n",
      "\n",
      "Marketing Prediction: Some key factors for building a successful multi-task instruction-following retrieval system include large-scale instruction-annotated datasets, the use of appropriate model architectures, and massive multi-task instruction-tuning to adapt to various retrieval tasks.\n",
      "Marketing Answer: Key factors to consider when building a successful multi-task instruction-following retrieval system include the effectiveness of the dataset scale in retrieval, the diversity in data and model scale, carefully designed negative samples, and the ability to adapt to new tasks via instructions.\n",
      "------------------------------------------------------------\n",
      "Question: What are the benefits of using retrieval-augmented techniques in multimodal language modeling, as demonstrated by the performance of the RA-CM3 model in the document?\n",
      " \n",
      "Engineering Prediction: The RA-CM3 model demonstrates the advantages of employing retrieval-augmented techniques in multimodal language modeling. By incorporating retrieval methods, the model can access and utilize long-tail knowledge, encompassing rare entities or minoritarian perspectives that are often underrepresented in standard training data. This enhancement enables the model to outperform existing models while requiring less training data, compute power, and parameters. The energy savings during training are a notable benefit, along with the model's ability to generate combinations of text and images, making it the first of its kind with such capabilities.\n",
      "Engineering Answer: The benefits of using retrieval-augmented techniques in multimodal language modeling, as demonstrated by the performance of the RA-CM3 model, include significantly better training efficiency with less training compute, outperforming existing models by using less training data, compute, and parameters. The retrieval augmentation allows the model to focus on learning how to use retrieved documents in context, leading to improved accuracy in classification tasks. Additionally, the RA-CM3 model achieves strong performance in image and caption generation, surpassing existing models like DALL-E and Flamingo despite using fewer resources.\n",
      "\n",
      "Marketing Prediction: The RA-CM3 model demonstrates that retrieval-augmented techniques in multimodal language modeling can offer energy savings during training, capture long-tail knowledge, and improve performance with less training data, compute power, and parameters.\n",
      "Marketing Answer: The benefits of using retrieval-augmented techniques in multimodal language modeling, as demonstrated by the performance of the RA-CM3 model in the document, include outperforming existing models by using less training data, compute, and parameters, achieving significantly better training efficiency, and improving accuracy in k-shot classification tasks. Additionally, retrieval augmentation allows the model to focus on learning how to use retrieved documents in context, leading to stronger performance in tasks such as image and caption generation.\n",
      "------------------------------------------------------------\n",
      "Question: What methods are typically employed to create training data for embedding models that use task-specific instructions?\n",
      " \n",
      "Engineering Prediction: Training data for embedding models that utilize task-specific instructions can be generated through a few approaches. One common method is to leverage unlabeled corpora and employ another model to automatically generate training data by training customized retrievers for each task. This allows for the creation of task-specific instructions tailored to individual tasks. Additionally, enhancing the diversity of training datasets and incorporating task instructions during fine-tuning can improve the model's robustness and performance across varying instructions.\n",
      "Engineering Answer: To create training data for embedding models that use task-specific instructions, a common method is to combine datasets from different sources, such as the SuperNaturalInstructions dataset with existing collections designed for embedding training. The SuperNaturalInstructions dataset provides natural language instructions, which can be paired with positive and negative examples to form training samples. Additionally, for tasks like classification or similarity, training samples can be constructed by selecting text sequences associated with different classes or similarities. This diverse training data is essential for instruction-based finetuning, which enables the embedding model to learn from a wide range of tasks and domains.\n",
      "\n",
      "Marketing Prediction: Training data for embedding models that use task-specific instructions can be created by leveraging large language models and automatically generating training data, enhancing efficiency and adaptability for diverse tasks.\n",
      "Marketing Answer: Training data for embedding models that use task-specific instructions is typically created by formulating a wide variety of tasks as text-to-text problems, distinguishing good/bad candidate outputs given an input text. This is done by combining datasets with natural language instructions and constructing positive and negative pairs for training.\n",
      "------------------------------------------------------------\n",
      "Question: Question: What are some of the challenges and innovations associated with fine-tuning large language models, and how does the approach discussed in the referenced text aim to address them?\n",
      " \n",
      "Engineering Prediction: Fine-tuning large language models (LLMs) is a challenging task due to the models' complexity and the need to balance improving performance and modifying behaviors. A key challenge is mitigating the models' susceptibility to distraction by irrelevant context, which can lead to errors or biased responses. The referenced text introduces INTERS, a novel approach that utilizes instruction tuning to enhance the effectiveness of LLMs in search scenarios. By providing explicit guidance and objectives, instruction tuning helps LLMs focus on relevant information and improve their accuracy and robustness. This method addresses the challenges of fine-tuning by offering a structured framework that directs the models' attention and learning process, thereby enhancing their performance and behavior.\n",
      "Engineering Answer: Some challenges associated with fine-tuning large language models include limited access to and manipulation of knowledge, lagging performance on knowledge-intensive tasks, and the need for provenance in decision-making and updating world knowledge. The approach discussed in the referenced text aims to address these challenges by utilizing Retrieval Augmented Generation (RAG), which involves retrieving relevant passages from a corpus to feed to the language model for improved performance in tasks such as question-answering and dialogue. This iterative approach focuses on improving alignment with user intent and fine-tuning models to control sentiment and improve response quality in various language tasks.\n",
      "\n",
      "Marketing Prediction: Fine-tuning large language models can be challenging due to the need to balance improving performance and behavior while preventing distractions from irrelevant context. The INTERS approach aims to address these challenges by instruction tuning, which involves training LLMs to follow instructions and achieve specific tasks, increasing their positive impact and performance in real-world applications.\n",
      "Marketing Answer: The challenges with fine-tuning large language models include aligning them with user intent and controlling the quality of generated outputs. The approach discussed in the referenced text aims to address these challenges by using Retrieval Augmented Generation (RAG) to retrieve relevant passages from a corpus and feed them to the language model, improving alignment and performance.\n",
      "------------------------------------------------------------\n",
      "Question: What is a common technique used to address the outlier issue when applying block-wise k-bit quantization to input tensors, and how does it work?\n",
      " \n",
      "Engineering Prediction: A common technique to address the outlier issue in block-wise k-bit quantization is to chunk the input tensor into smaller blocks, allowing independent quantization of each block with its own quantization constant. This approach, known as block-wise quantization, helps in managing outliers by adapting the quantization to local variations within the tensor, preventing the extreme values from dominating the quantization process. By doing so, the impact of outliers on the overall quantization is reduced, and the important values are preserved, thus improving the accuracy of the quantization process.\n",
      "Engineering Answer: A common technique used to address the outlier issue when applying block-wise k-bit quantization to input tensors is to chunk the input tensor into blocks that are independently quantized, each with their own quantization constant. This approach involves dividing the input tensor into contiguous blocks of size B by flattening the tensor and slicing it into n blocks, where n is determined by the size of the blocks. Each block is then quantized independently using a quantization constant c, which helps prevent outlier values from causing performance degradation.\n",
      "\n",
      "Marketing Prediction: A common approach to handling outliers in block-wise k-bit quantization is to divide the input tensor into smaller blocks, each with its own quantization constant, allowing for independent quantization and preventing outliers from affecting the overall distribution.\n",
      "Marketing Answer: A common technique used to address the outlier issue when applying block-wise k-bit quantization to input tensors is to chunk the input tensor into blocks that are independently quantized, each with their own quantization constant. This helps prevent performance degradation by reducing the impact of outliers on the quantization process.\n",
      "------------------------------------------------------------\n",
      "Question: What considerations or techniques are commonly implemented when setting up finetuning experiments for machine learning models?\n",
      " \n",
      "Engineering Prediction: When setting up fine-tuning experiments for machine learning models, it is common to employ a range of techniques and considerations to ensure effective and efficient model adaptation to specific tasks. A key aspect is the choice of fine-tuning methodology, which can vary depending on the model and task at hand. This includes approaches such as supervised fine-tuning, where the model is trained on labeled data relevant to the downstream task, or more advanced methods like LoRA, which has proven robust and efficient. The selection of an appropriate training dataset is also crucial, with options such as the Pile or RedPajama dataset offering diverse and task-specific data. Experimentation plays a significant role, with techniques like direct fine-tuning for a set number of steps, allowing for the comparison of different fine-tuning methods and their impact on model performance. Additionally, evaluating fine-tuning performance on specific datasets and analyzing the results through human preference assessment can provide valuable insights into the effectiveness of the chosen techniques. The overall goal is to strike a balance between adapting the model to the target task while avoiding overfitting and maintaining general language understanding capabilities.\n",
      "Engineering Answer: When setting up finetuning experiments for machine learning models, it is common to use a two-stage approach. The initial stage involves setting the initial parameters using a language modeling objective. This is followed by a supervised discriminative 'fine-tuning' stage to adapt these parameters to the target task. Additionally, it is typical to train all models using the Adam optimizer and a triangular learning rate scheduler with 10% warmup. Experimentation with different hyperparameters such as number of epochs, peak learning rate, and batch size is also conducted to optimize model performance. Finally, utilizing a mixture of datasets and balancing the sizes of datasets can help improve the robustness and generalization of the finetuned models.\n",
      "\n",
      "Marketing Prediction: When setting up fine-tuning experiments for machine learning models, it's important to consider the relevance of the training data to the specific task, the number of fine-tuning steps, and the choice of fine-tuning techniques or methods, such as direct fine-tuning or more advanced approaches like DPO, which can eliminate the need for sampling from the LM and reduce hyperparameter tuning.\n",
      "Marketing Answer: Considerations for setting up finetuning experiments for machine learning models commonly include using a language modeling objective for initial parameter setting and supervised discriminative fine-tuning for adapting parameters to the target task. Techniques such as hyperparameter search, Adam optimizer with triangular learning rate scheduler, and balancing dataset sizes through mixing strategies are also commonly implemented. Additionally, freezing some model layers during fine-tuning and incorporating negative examples for contrastive learning can be effective strategies.\n",
      "------------------------------------------------------------\n",
      "Question: What are the implications of the equivalence relation defined in the theoretical analysis of the DPO model for understanding the relationship between reward functions in reinforcement learning?\n",
      " \n",
      "Engineering Prediction: The equivalence relation defined in the context of the DPO model's theoretical analysis reveals that reward functions differing only in an input-specific component (i.e., r(x, y) and r'(x, y) being equivalent if r(x, y) - r'(x, y) = f(x) for some function f) belong to the same equivalence class. This insight is significant because it demonstrates that reward functions within the same class induce the same optimal policy and result in identical Bradley-Terry preference values. Consequently, this understanding allows for flexibility in the learning process, as it does not restrict the class of learned reward models and enables the exact recovery of the optimal policy.\n",
      "Engineering Answer: The equivalence relation defined in the theoretical analysis of the DPO model implies that two reward functions are considered equivalent if they differ by a constant function. This means that the class of learned reward models is not constrained by this reparameterization, allowing for the exact recovery of the optimal policy. Understanding this relationship between reward functions in reinforcement learning helps in defining a unique reward function within each equivalence class, which is crucial for optimizing policies under existing models of human preferences. It also highlights the generality and flexibility in the reward model due to the proposed reparameterization.\n",
      "\n",
      "Marketing Prediction: The equivalence relation defined in the DPO model's theoretical analysis reveals that reward functions differing only in an input-specific component lead to the same optimal policy, allowing for flexibility in reward function specification without sacrificing optimal policy recovery, and offering a practical approach to learning reward models from human preferences.\n",
      "Marketing Answer: The equivalence relation defined in the theoretical analysis of the DPO model shows that two reward functions are considered equivalent if they differ by a fixed function. This implies that different reward functions can lead to the same optimal policy, allowing for flexibility in designing reward models in reinforcement learning.\n"
     ]
    }
   ],
   "source": [
    "val_eval = evaluate(metrics, validation_dict, rag_chains, iterations=0, verbose=False, dept_specific=True, print_results=True)\n",
    "val_eval = composite_evaluation(val_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "POO5jIyZ4vh6",
    "outputId": "e66e2c53-7ca2-4359-ebc1-73f5751e0da0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"val_eval\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"Sample\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 14.38460451364229,\n        \"min\": 12.318568359869648,\n        \"max\": 55.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          35.333333333333336,\n          35.5,\n          30.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eng_bleu\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10.572209788924988,\n        \"min\": 0.0,\n        \"max\": 30.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.08876497718638,\n          0.07735194577529717,\n          30.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mk_bleu\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10.556761709728796,\n        \"min\": 0.0,\n        \"max\": 30.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.12048256018456535,\n          0.10364850753891405,\n          30.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eng_rouge\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10.522762889278068,\n        \"min\": 0.08217363573549501,\n        \"max\": 30.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.2572837098830757,\n          0.24107046799354492,\n          30.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mk_rouge\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10.49538518799537,\n        \"min\": 0.08,\n        \"max\": 30.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.32742562217257515,\n          0.31643489254108725,\n          30.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eng_f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10.342667165735536,\n        \"min\": 0.017558132621251665,\n        \"max\": 30.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.8811094701290131,\n          0.8803865015506744,\n          30.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mk_f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10.338329641168327,\n        \"min\": 0.018308922127894645,\n        \"max\": 30.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.8956405321756998,\n          0.8959077894687653,\n          30.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"composite_eng\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10.442255105963973,\n        \"min\": 0.04602682457342888,\n        \"max\": 30.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.5354928434667052,\n          0.528582202238436,\n          30.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"composite_mk\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10.428956627704542,\n        \"min\": 0.06669870473966676,\n        \"max\": 30.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.5701444647765356,\n          0.5616399568095305,\n          30.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"composite_total\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10.437274591329858,\n        \"min\": 0.04699264962459173,\n        \"max\": 30.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.5493534919906373,\n          0.5501288408192851,\n          30.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-27c8498d-9657-4274-b434-5953373dace8\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample</th>\n",
       "      <th>eng_bleu</th>\n",
       "      <th>mk_bleu</th>\n",
       "      <th>eng_rouge</th>\n",
       "      <th>mk_rouge</th>\n",
       "      <th>eng_f1</th>\n",
       "      <th>mk_f1</th>\n",
       "      <th>composite_eng</th>\n",
       "      <th>composite_mk</th>\n",
       "      <th>composite_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>35.333333</td>\n",
       "      <td>0.088765</td>\n",
       "      <td>0.120483</td>\n",
       "      <td>0.257284</td>\n",
       "      <td>0.327426</td>\n",
       "      <td>0.881109</td>\n",
       "      <td>0.895641</td>\n",
       "      <td>0.535493</td>\n",
       "      <td>0.570144</td>\n",
       "      <td>0.549353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>12.318568</td>\n",
       "      <td>0.075619</td>\n",
       "      <td>0.111610</td>\n",
       "      <td>0.082174</td>\n",
       "      <td>0.130646</td>\n",
       "      <td>0.017558</td>\n",
       "      <td>0.018309</td>\n",
       "      <td>0.046027</td>\n",
       "      <td>0.066699</td>\n",
       "      <td>0.046993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.124031</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.853427</td>\n",
       "      <td>0.863540</td>\n",
       "      <td>0.469208</td>\n",
       "      <td>0.455770</td>\n",
       "      <td>0.470262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>24.250000</td>\n",
       "      <td>0.041496</td>\n",
       "      <td>0.049608</td>\n",
       "      <td>0.215513</td>\n",
       "      <td>0.241538</td>\n",
       "      <td>0.867283</td>\n",
       "      <td>0.879976</td>\n",
       "      <td>0.502580</td>\n",
       "      <td>0.523488</td>\n",
       "      <td>0.521030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>35.500000</td>\n",
       "      <td>0.077352</td>\n",
       "      <td>0.103649</td>\n",
       "      <td>0.241070</td>\n",
       "      <td>0.316435</td>\n",
       "      <td>0.880387</td>\n",
       "      <td>0.895908</td>\n",
       "      <td>0.528582</td>\n",
       "      <td>0.561640</td>\n",
       "      <td>0.550129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>45.750000</td>\n",
       "      <td>0.143630</td>\n",
       "      <td>0.168739</td>\n",
       "      <td>0.295682</td>\n",
       "      <td>0.387278</td>\n",
       "      <td>0.893217</td>\n",
       "      <td>0.907850</td>\n",
       "      <td>0.563955</td>\n",
       "      <td>0.601913</td>\n",
       "      <td>0.562159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>55.000000</td>\n",
       "      <td>0.259686</td>\n",
       "      <td>0.449700</td>\n",
       "      <td>0.456140</td>\n",
       "      <td>0.758621</td>\n",
       "      <td>0.920762</td>\n",
       "      <td>0.941401</td>\n",
       "      <td>0.638634</td>\n",
       "      <td>0.776219</td>\n",
       "      <td>0.686209</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-27c8498d-9657-4274-b434-5953373dace8')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-27c8498d-9657-4274-b434-5953373dace8 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-27c8498d-9657-4274-b434-5953373dace8');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-cebf00d5-6c8b-4342-bbfe-4c638736c99c\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cebf00d5-6c8b-4342-bbfe-4c638736c99c')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-cebf00d5-6c8b-4342-bbfe-4c638736c99c button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "          Sample   eng_bleu    mk_bleu  eng_rouge   mk_rouge     eng_f1  \\\n",
       "count  30.000000  30.000000  30.000000  30.000000  30.000000  30.000000   \n",
       "mean   35.333333   0.088765   0.120483   0.257284   0.327426   0.881109   \n",
       "std    12.318568   0.075619   0.111610   0.082174   0.130646   0.017558   \n",
       "min    16.000000   0.000000   0.000000   0.124031   0.080000   0.853427   \n",
       "25%    24.250000   0.041496   0.049608   0.215513   0.241538   0.867283   \n",
       "50%    35.500000   0.077352   0.103649   0.241070   0.316435   0.880387   \n",
       "75%    45.750000   0.143630   0.168739   0.295682   0.387278   0.893217   \n",
       "max    55.000000   0.259686   0.449700   0.456140   0.758621   0.920762   \n",
       "\n",
       "           mk_f1  composite_eng  composite_mk  composite_total  \n",
       "count  30.000000      30.000000     30.000000        30.000000  \n",
       "mean    0.895641       0.535493      0.570144         0.549353  \n",
       "std     0.018309       0.046027      0.066699         0.046993  \n",
       "min     0.863540       0.469208      0.455770         0.470262  \n",
       "25%     0.879976       0.502580      0.523488         0.521030  \n",
       "50%     0.895908       0.528582      0.561640         0.550129  \n",
       "75%     0.907850       0.563955      0.601913         0.562159  \n",
       "max     0.941401       0.638634      0.776219         0.686209  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_eval.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RvZTUR1RY6Bm"
   },
   "source": [
    "Surprisingly, from a metric standpoint, we are now resulting much better on marketing prompts than engineering.  And both answers scored higher on the validation set than the train set, indicating that we have not overfit our model to the original questions that we trained on, which is great.\n",
    "\n",
    "However, a quick review of the answers that are being returned highlights some issues. Below are an example:\n",
    "\n",
    " - Question: How has the token handling capacity changed between different versions of the Claude model?\n",
    " - Predicted Answers indicate the capacity is limited to 2,048 tokens, which is a decrease from previous inputs.\n",
    " - Target Answers indicate that the capacity as continously increased and is maxing at 1 million now.\n",
    "\n",
    "And another example\n",
    "\n",
    " - Question: What benchmark did Chinchilla achieve an average accuracy of 67.5% on?\n",
    " - One predicted answer states WebSource Corpus benchmark and the other Vicuna benchmark.\n",
    " - Both target answers indicate the MMLU benchmark.\n",
    "\n",
    "THe inaccuracy of these results is concerning.  Unfortunately we have dedicated significant effort/resources to this model and have reached our cap.  We will be noting these issues in our write-up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5nV66WCEXImW"
   },
   "source": [
    "##### **Curiousity on Embedding Model**\n",
    "At the very beginning of our process we leaned on our evaluation metric and chose the 'all-MiniLM-L6-v2' embedding model because it performed the best.  Because research stated that this model is designed for compute efficiency and that can be at the sacrifice of performance, we are curious how our tuned model would perform if we switched to the 'multi-qa-mpnet-base-dot-v1' embedding model.  So we are going to check that now on our validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yW5YdLXdX-hP"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "embedding_model = 'multi-qa-mpnet-base-dot-v1'\n",
    "chunk_size = 256\n",
    "chunk_overlap = 16\n",
    "llm = \"cohere\"\n",
    "retreiver_search_type = \"mmr\"\n",
    "retreiver_k = 6\n",
    "eng_rag_template = \"\"\"[INST]\n",
    "              Please provide an precise and concise answer to the engineer's question below based on the context information provided.\\n\\n\n",
    "              Below is a context:\\n{context}\\n\n",
    "              Below is a question:\\n{question}\\n\n",
    "              Below are answer instructions in order of importance:\n",
    "- Formatting: Provide a succint, single-paragraph answer. Do not use bullet points. Do not explicitly reference papers in your answer.\n",
    "- Technical Detail: Include technical details and terminologies that relate to the question.\n",
    "- Research Focus: Orient answers towards the research aspects of the questions.\n",
    "- Objective Tone: Maintain an objective and informative tone, aiming to educate the reader without persuasive language.\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "mk_rag_template = \"\"\"[INST]\n",
    "              Please provide a precise and concise answer to the marketer's question below based on the context provided.\\n\\n\n",
    "              Below is a context:\\n{context}\\n\n",
    "              Below is a question:\\n{question}\\n\n",
    "              Below are answer instructions in order of importance:\n",
    "- Formatting: Provide a concise, single-paragraph answer that uses the fewest words necessary to fully address the question. Answer in a single sentence or phrase if you can. Do not use bullet points. Do not explicitly reference papers in your answer.\n",
    "- Succinctness: Make sure your answer is concise and to the point. Provide only the essential information without delving into the technical depth.\n",
    "- Broad Overview: Give a broad overview of the topic. Provide only the essential information without delving into the technical depth.\n",
    "- Focus on Applications: EFocus on real-world uses and benefits and highlight how technology can solve problems or create opportunities.\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "base_embeddings, text_splitter, qdrant_vectorstore, retriever = build_embedding_splitter_vectorstore(embedding_model, splitter, retr_search_type=retreiver_search_type, retr_k=retreiver_k)\n",
    "qdrant_vectorstore = vectorize_documents(text_splitter, qdrant_vectorstore)\n",
    "llm_model = load_llm(llm)\n",
    "eng_rag_prompt = ChatPromptTemplate.from_template(eng_rag_template)\n",
    "mk_rag_prompt = ChatPromptTemplate.from_template(mk_rag_template)\n",
    "eng_rag_chain = build_RAG_prompt_chain(eng_rag_template, llm_model, retriever, format_docs)\n",
    "mk_rag_chain = build_RAG_prompt_chain(mk_rag_template, llm_model, retriever, format_docs)\n",
    "rag_chains = {\"engineering\": eng_rag_chain, \"marketing\": mk_rag_chain}\n",
    "metrics = ['rouge', 'bleu', 'bertscore']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PhuXje-mYBwm",
    "outputId": "fbb70305-e125-42ef-9ba0-e932670101a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Question: What factors influenced the development of generative language models by Anthropic?\n",
      " \n",
      "Engineering Prediction: The development of generative language models by Anthropic was influenced by advancements in transformer models and the emergence of large language models like BERT. The two-stage approach of unsupervised generative pre-training and supervised discriminative fine-tuning played a crucial role in setting initial parameters and adapting the model to specific tasks. Additionally, the limitations of traditional supervised learning methods, such as the need for well-annotated datasets and the high computational cost of training large language models, prompted the exploration of semi-supervised approaches. Furthermore, the study by Petroni et al. highlighted the importance of context retrieval in improving the performance of generative language models, influencing the techniques employed by Anthropic.\n",
      "Engineering Answer: Several factors influenced the development of generative language models by Anthropic, including the limitations in coding, math, and reasoning capabilities of the initial version Claude, the partnerships with companies like Notion and Quora to enhance the model's capabilities, and the need to address biases, unsafe content, and ethical considerations in training data. Additionally, the reliance on supervised learning and the need for controlled generation in generative models played a role in shaping the development of Anthropic's language models.\n",
      "\n",
      "Marketing Prediction: The development of generative language models by Anthropic was influenced by the emergence of large language models, such as BERT, and the use of transformer models, as well as the semi-supervised approach employed by OpenAI, which enabled the training of large-scale generative systems.\n",
      "Marketing Answer: Factors that influenced the development of generative language models by Anthropic include partnerships with companies like Notion and Quora, limitations in coding, math, and reasoning capabilities in initial models like Claude, and the need to address biases and unsafe content in training datasets.\n",
      "------------------------------------------------------------\n",
      "Question: What is Constitutional AI and how does it affect the functionality of AI systems?\n",
      " \n",
      "Engineering Prediction: Constitutional AI refers to the concept of developing artificial intelligence systems that are aligned with human values and ethics, with a specific focus on ensuring their responses and beliefs are justified and harmless. The core idea is to mitigate risks associated with advanced language models and generative AI, which may exhibit undesirable behaviors such as generating misleading content or automating jobs. By employing alignment techniques, researchers aim to ensure that AI systems do not develop goals, priorities, or skill sets that conflict with human intentions. This involves teaching AI to understand and avoid common mistakes, such as those made by humans, while also ensuring that the AI does not simply imitate all human behaviors and beliefs, only those that are beneficial and ethical. The overall goal of Constitutional AI is to create harmless AI systems that can generate text, images, and other data in response to prompts without causing unintended consequences or violating human values.\n",
      "Engineering Answer: Constitutional AI is an approach developed by Anthropic for training AI systems, particularly language models like Claude, to be harmless and helpful without relying on extensive human feedback. It involves two phases: supervised learning, where the model generates responses to prompts and self-critiques based on a set of guiding principles, and reinforcement learning, where the model is trained with AI-generated feedback according to constitutional principles. This approach enables the training of AI assistants that are both helpful and harmless, with the ability to explain objections to harmful requests, enhancing transparency and reducing the need for human supervision.\n",
      "\n",
      "Marketing Prediction: Constitutional AI refers to the development of AI systems designed to be harmless and aligned with human intent and values. It aims to address the issue of AI \"hallucination,\" where AI systems may generate unjustified responses, and mitigate risks associated with advanced language models, such as the automation of jobs and the creation of misleading content. By employing alignment techniques, Constitutional AI ensures that AI systems understand and respect human goals and priorities, preventing undesirable outcomes and promoting beneficial applications, such as automating the detection and removal of harmful content.\n",
      "Marketing Answer: Constitutional AI is an approach developed by Anthropic for training AI systems, particularly language models like Claude, to be harmless and helpful without relying on extensive human feedback. It involves supervised learning and reinforcement learning phases to guide the model's responses based on a set of guiding principles (a 'constitution'). This approach aims to create AI systems that are both helpful and transparent in their decision-making process, reducing the need for constant human supervision.\n",
      "------------------------------------------------------------\n",
      "Question: How do advances in AI models impact their ability to interact with different types of data, such as images?\n",
      " \n",
      "Engineering Prediction: The advancement of AI models, particularly large language models (LLMs), has significantly enhanced their capacity to interact with diverse data types, including images. This evolution can be attributed to improvements in transformer-based deep neural networks, which can identify patterns and structures in vast datasets. By leveraging large parameters and extensive training data, these models can now generate and manipulate images through their understanding of the underlying patterns and structures within the image data. This capability opens up new avenues for AI applications, such as automated image classification and the creation of AI art, showcasing the profound impact of advanced AI models on their ability to interpret and generate visual information.\n",
      "Engineering Answer: Advances in AI models, such as multimodal models like RA-CM3, have significantly improved their ability to interact with different types of data, such as images. These models can refer to external memory, like web data, to increase their knowledge capacity, allowing them to generate correct images from entity-rich captions. Additionally, these models can perform image editing and manually specify examples in-context for better results. The use of large language models, combined with larger datasets and neural networks, has also enhanced their performance in tasks like image generation and text generation.\n",
      "\n",
      "Marketing Prediction: Recent advancements in AI models, particularly large language models, have enhanced their capacity to understand and generate various types of data, including images.\n",
      "Marketing Answer: Advances in AI models, such as multimodal models like RA-CM3, allow for better interaction with different types of data, like images, by accessing external memory for increased knowledge capacity and improving performance in tasks like image generation and image editing.\n",
      "------------------------------------------------------------\n",
      "Question: What are the potential trade-offs between AI system alignment with ethical guidelines and practical utility?\n",
      " \n",
      "Engineering Prediction: There are trade-offs between ensuring an AI system's alignment with ethical guidelines and maintaining its practical utility. On the one hand, strict adherence to ethical guidelines may limit an AI system's capabilities and flexibility, potentially hindering its performance in certain tasks. For example, an AI system designed to generate text may produce less creative or sophisticated responses if it is constrained from generating potentially sensitive content. On the other hand, without proper regulation and alignment techniques, future highly capable AI systems may act in ways that are harmful or detrimental to human interests, even if such actions were not initially intended. Finding a balance between these trade-offs is crucial to ensure that AI systems are both ethically aligned and practically useful, which is an active area of research in the field of AI ethics.\n",
      "Engineering Answer: The potential trade-offs between AI system alignment with ethical guidelines and practical utility include the risk of reduced performance and usability due to stringent ethical alignment measures, as seen with Claude 2. Users may face limitations and refusal of assistance for benign requests, leading to debates over the 'alignment tax' in AI development. Balancing ethical considerations with practical functionality is crucial to ensure alignment with ethical guidelines without compromising the practical utility of AI systems. Research is needed to find a middle ground that prioritizes ethical alignment while maintaining usability and performance.\n",
      "\n",
      "Marketing Prediction: There is a trade-off between ensuring AI systems are aligned with ethical guidelines and maintaining their practical utility; while ethical guidelines are necessary to prevent harmful applications and unintended consequences, overly restrictive measures may limit an AI system's capabilities and hinder its ability to provide beneficial solutions to complex problems.\n",
      "Marketing Answer: The potential trade-offs between AI system alignment with ethical guidelines and practical utility include balancing stringent ethical alignment that may reduce usability and performance, ensuring transparency and fairness in alignment processes, and addressing the alignment tax that may impact adoption of AI systems.\n",
      "------------------------------------------------------------\n",
      "Question: How has the token handling capacity changed between different versions of the Claude model?\n",
      " \n",
      "Engineering Prediction: The Claude model's token handling capacity has evolved across versions, with a focus on improving language generation fluency and efficiency. Earlier versions of Claude had a limited capacity, generating shorter outputs with an average of nearly 100 tokens, which has increased in newer versions. This change aims to enhance the model's fluency and flexibility, allowing it to provide more comprehensive responses. The research focus is on striking a balance between token quantity and quality, ensuring that longer outputs do not compromise the model's performance and fluency.\n",
      "Engineering Answer: The token handling capacity has increased with each new version of the Claude model. Claude Instant has a context length of 100,000 tokens, Claude 2.1 doubled this to 200,000 tokens, and Claude 3 Opus default version has a context window of 200,000 tokens but can be expanded to 1 million for specific use cases. This progression shows a trend towards handling larger amounts of text data for improved performance and capabilities.\n",
      "\n",
      "Marketing Prediction: The Claude model's token handling capacity has improved with each version, allowing for more fluent and coherent responses, especially with larger models, but this has also led to an increase in the number of tokens generated, which can impact response length and memory usage.\n",
      "Marketing Answer: The token handling capacity has increased from Claude to Claude Instant to Claude 2.1, with Claude Instant having a input context length of 100,000 tokens, Claude 2.1 having a context window of 200,000 tokens, and Claude 3 Opus having a context window of 1 million tokens.\n",
      "------------------------------------------------------------\n",
      "Question: In what ways has the Claude model's ability to self-critique and revise its responses enhanced its transparency?\n",
      " \n",
      "Engineering Prediction: The Claude model's capacity for self-critique and response revision enhances its transparency by providing users with explicit self-reflection and fine-grained attribution. This functionality enables users to verify factual errors in the model's outputs, increasing trust and understanding of the model's decision-making process. The model's ability to iteratively refine its responses through self-feedback improves its accuracy and specificity, contributing to its transparency. Furthermore, the model's learned retrieval component can be hot-swapped, allowing for updates that improve its performance and adaptability. The extensive experiments conducted on instruction design, template diversity, few-shot demonstrations, and the volume of instructions provide valuable insights into the model's behavior and capabilities, further enhancing its transparency and usability.\n",
      "Engineering Answer: The Claude model's ability to self-critique and revise its responses has enhanced its transparency by allowing for iterative improvements based on past actions and mistakes. Through self-reflection, the model can refine its output by learning from feedback and generating special tokens to signal the need for retrieval or confirm the relevance, support, or completeness of its responses. This process ensures that the model's statements about the world are truthful and accurate, ultimately increasing transparency in its decision-making and reasoning processes.\n",
      "\n",
      "Marketing Prediction: Claude's self-critique and revision capabilities enhance its transparency by allowing it to identify and correct factual errors, providing users with more accurate and reliable information.\n",
      "Marketing Answer: The Claude model's ability to self-critique and revise its responses has enhanced its transparency by allowing it to generate text informed by retrieved passages, criticize the output, and signal the need for retrieval or confirm the output's relevance, support, or completeness. This self-reflection process helps improve the model's accuracy and reliability in generating responses.\n",
      "------------------------------------------------------------\n",
      "Question: How do subsequent versions of Claude compare in terms of their likelihood to produce false statements?\n",
      " \n",
      "Engineering Prediction: Subsequent versions of Claude show a decreasing trend in perplexity as model size increases, indicating that larger models tend to generate more fluent and coherent text. However, the generation statistics, including the likelihood of producing false statements, remain relatively consistent across different model sizes. This suggests that while larger models may exhibit improved language capabilities, the challenge of reducing false statements remains a persistent research focus for the Claude model series.\n",
      "Engineering Answer: Claude Instant is a faster and lighter version of Claude, with an input context length of 100,000 tokens. In contrast, Claude 3 has faced criticism for its stringent ethical alignment, leading to a debate over the 'alignment tax' in AI development. Users have been refused assistance with benign requests, which has sparked discussions on balancing ethical considerations and practical functionality. This suggests that Claude Instant may have a lower likelihood of producing false statements compared to Claude 3 due to its focus on usability and performance.\n",
      "\n",
      "Marketing Prediction: Subsequent versions of Claude show a decrease in perplexity, indicating that they generate more fluent responses; however, the rate of false statements, or generation statistics, remains roughly consistent across versions.\n",
      "Marketing Answer: Claude Instant is a faster, less expensive, and lighter version of Claude with a shorter input context length. Claude 3 has faced criticism for ethical alignment issues that may affect usability and performance.\n",
      "------------------------------------------------------------\n",
      "Question: Who developed the language model family known as Chinchilla?\n",
      " \n",
      "Engineering Prediction: The Chinchilla language model family was developed by a team of researchers led by Noah Fiedel and Carl Hesse at the AI research company Cohere. Chinchilla is a large language model (LLM) based on transformer architecture and trained on a massive text corpus, pushing the boundaries of language understanding and generation. With its advanced architecture and training data, Chinchilla has demonstrated impressive capabilities in generating human-like responses and has been a significant contribution to the field of natural language processing.\n",
      "Engineering Answer: The Chinchilla language model family was developed by the research team at DeepMind and presented in March 2022. It is named 'Chinchilla' as an advancement over the previous Gopher model family. The Chinchilla family has been trained to investigate the scaling laws of large language models and is designed to outperform GPT-3.\n",
      "\n",
      "Marketing Prediction: Google developed the Chinchilla language model.\n",
      "Marketing Answer: The research team at DeepMind developed the language model family known as Chinchilla.\n",
      "------------------------------------------------------------\n",
      "Question: What benchmark did Chinchilla achieve an average accuracy of 67.5% on?\n",
      " \n",
      "Engineering Prediction: Chinchilla achieved an average accuracy of 67.5% on the HellaSwag benchmark in a zero-shot setting.\n",
      "Engineering Answer: Chinchilla achieved an average accuracy of 67.5% on the MMLU benchmark (Measuring Massive Multitask Language Understanding).\n",
      "\n",
      "Marketing Prediction: Chinchilla achieved an average accuracy of 67.5% on the HellaSwag benchmark.\n",
      "Marketing Answer: Chinchilla achieved an average accuracy of 67.5% on the MMLU benchmark (Measuring Massive Multitask Language Understanding).\n",
      "------------------------------------------------------------\n",
      "Question: What is the relationship between Chinchilla and the Gopher language model families?\n",
      " \n",
      "Engineering Prediction: Chinchilla and Gopher are large-scale language models with a similar architectural design, both employing the Transformer architecture with self-attention mechanisms. Chinchilla is an advanced variant of the Gopher model, built with an emphasis on scaling up the model size and training data. The key distinction lies in their parameter counts: Chinchilla boasts 70 billion parameters, a significant expansion from Gopher's 280 billion, allowing Chinchilla to capture more intricate linguistic patterns and nuances. Both models are trained on massive text corpora, with Chinchilla utilizing an even larger dataset, contributing to its superior performance on various language tasks. The relationship between the two can be likened to an evolution, where Chinchilla builds upon the foundation of Gopher, enhancing its capabilities and pushing the boundaries of language modeling.\n",
      "Engineering Answer: The Chinchilla family of transformer models is essentially the same as the Gopher family, with minor modifications and different training optimizers. Chinchilla uses AdamW optimizer while Gopher uses Adam optimizer. Additionally, Chinchilla uses relative positional encoding and RMSNorm instead of absolute positional encoding and LayerNorm used by Gopher. Chinchilla has 70B parameters and outperforms Gopher on the MMLU benchmark by 7%, showcasing an improvement in performance. Both families follow similar naming conventions and were developed to investigate the scaling laws of large language models.\n",
      "\n",
      "Marketing Prediction: Chinchilla is an enhanced version of the Gopher language model, offering improved capabilities and performance, particularly in generating more human-like responses and exhibiting stronger reasoning skills.\n",
      "Marketing Answer: Chinchilla is a family of transformer models developed by DeepMind, which is a further development over a previous model family named Gopher. Both model families were trained to investigate the scaling laws of large language models.\n",
      "------------------------------------------------------------\n",
      "Question: What distinguishes the architectures of the Chinchilla and Gopher family models in terms of optimization techniques used?\n",
      " \n",
      "Engineering Prediction: The Chinchilla and Gopher family models differ in their optimization techniques. The Chinchilla model utilizes a sparse optimization technique, which allows it to achieve higher parameter efficiency compared to dense models like Gopher. This sparse optimization technique enables the Chinchilla model to scale effectively to larger sizes while maintaining superior performance. On the other hand, the Gopher model employs a different optimization technique known as \"brain-inspired hyperparameter optimization,\" which involves adjusting hyperparameters such as learning rate and weight decay to improve performance. This technique allows Gopher to achieve strong performance, especially in fine-tuning tasks, despite having a smaller number of parameters compared to Chinchilla.\n",
      "Engineering Answer: The main distinction in optimization techniques between the Chinchilla and Gopher family models lies in the choice of optimizers. The Gopher family utilizes the Adam optimizer, whereas the Chinchilla family is trained using the AdamW optimizer. Additionally, the Gopher family employs RMSNorm instead of LayerNorm, and relative positional encoding rather than absolute positional encoding. These differences in optimization techniques contribute to the unique characteristics and performance of each model family.\n",
      "\n",
      "Marketing Prediction: The Chinchilla and Gopher models differ in their optimization techniques, with Chinchilla employing a sparse optimization technique, while Gopher utilizes a dense optimization approach.\n",
      "Marketing Answer: The Chinchilla family uses AdamW optimizer, while the Gopher family uses the Adam optimizer.\n",
      "------------------------------------------------------------\n",
      "Question: What is the recommended strategy for training large autoregressive language models with limited compute resources, as contributed by the Chinchilla team?\n",
      " \n",
      "Engineering Prediction: Training large autoregressive language models typically demand extensive compute resources, but the Chinchilla team proposes a strategy to mitigate this challenge. Their recommended approach involves leveraging the efficiency of GPU clusters and the Megatron-LM framework. By employing a distributed training strategy across multiple GPUs, the computational burden can be shared, enabling faster and more efficient training. This method not only reduces the hardware requirements but also accelerates the training process, making it feasible to train large language models with limited resources. The technique has proven effective for the Chinchilla team, showcasing its potential as a viable strategy for researchers and practitioners facing similar computational constraints.\n",
      "Engineering Answer: The Chinchilla team recommends that the number of training tokens should be doubled for every model size doubling to achieve better results on downstream tasks. They also suggest using larger, higher-quality training datasets to improve performance. Additionally, they mention the importance of balancing model size and efficiency to address computational costs and inference latency limitations. It is advised to focus on Transformer language models and consider sharing model parameters for quick task-switching when deploying as a service.\n",
      "\n",
      "Marketing Prediction: The Chinchilla team recommends leveraging instruction tuning, a simple approach for policy optimization, to train large autoregressive language models with limited compute resources.\n",
      "Marketing Answer: The Chinchilla team recommends doubling the number of training tokens for every model size doubling and using larger, higher-quality training datasets to achieve better results on downstream tasks.\n",
      "------------------------------------------------------------\n",
      "Question: What are some key areas of research in the field of artificial intelligence as reflected in recent academic literature?\n",
      " \n",
      "Engineering Prediction: Recent academic literature in artificial intelligence (AI) reflects a diverse range of research interests. Key areas of focus include the alignment of AI systems with human intentions across various sectors, such as software development, healthcare, and finance, among others. This involves exploring theoretical frameworks, ethical considerations, and practical applications. Additionally, there is a strong emphasis on language models and natural language processing, with techniques like neural information retrieval being highlighted. The field also engages with philosophical and ethical debates surrounding the nature of human intelligence and the implications of creating artificial beings with similar capabilities. Overall, the research landscape in AI is multifaceted, encompassing technical, societal, and ethical dimensions to shape the future of this transformative technology.\n",
      "Engineering Answer: Recent academic literature in the field of artificial intelligence reflects key areas of research such as natural language processing with state-of-the-art transformers, feature learning in infinite-width neural networks, diverse beam search for complex scene description, and the development of generative AI models capable of generating text and images. Additionally, research focuses on human preferences in dueling bandits, the use of few-shot learners in language models, and the exploration of knowledge-grounded neural conversation models. These areas of research highlight the advancements in AI technology and its applications across various domains.\n",
      "\n",
      "Marketing Prediction: Recent academic literature on artificial intelligence (AI) reflects key research areas including text generation, information retrieval, and neural networks, with a focus on applications in diverse fields such as software development, healthcare, finance, and marketing, while also addressing ethical concerns and potential misuse.\n",
      "Marketing Answer: Some key areas of research in artificial intelligence include natural language processing, deep neural networks, generative AI, AI safety, AI art, reinforcement learning, and language agents alignment.\n",
      "------------------------------------------------------------\n",
      "Question: What are some of the limitations of traditional position encoding methods in the architecture of pre-trained language models (PLMs), and what novel approach does the paper propose to address these issues?\n",
      " \n",
      "Engineering Prediction: Traditional position encoding methods in pre-trained language models (PLMs) face limitations due to their reliance on padding operations, which can implicitly learn position information. This leads to a lack of thorough explanations for their faster convergence compared to baseline models with other position encoding strategies. The novel approach proposed in the paper introduces a formal description of the position encoding problem and provides a proof for the favorable property of long-term decay for inter-token products. This method addresses the limitations by offering a more comprehensive understanding of position encoding, allowing for the inclusion of various position encoding types and benefiting multiple LLMs.\n",
      "Engineering Answer: One limitation of traditional position encoding methods in PLMs is that they may not enable length extrapolation of pre-existing models, leading to the need for substantial pre-training costs. The paper proposes a novel approach called Position Interpolation, which extends existing PLMs without deviating far from existing definitions of position encoding or attention mechanisms. This method allows for much extended context windows for text modeling, leading to significant perplexity gains and improved model performance.\n",
      "\n",
      "Marketing Prediction: Traditional position encoding methods in pre-trained language models (PLMs) face limitations due to their inability to effectively capture long-range dependencies and their lack of interpretability. To address these issues, the paper proposes a novel approach that introduces a new position encoding strategy, aiming to improve the convergence speed and provide thorough explanations for its effectiveness.\n",
      "Marketing Answer: Traditional position encoding methods in PLMs have limitations in enabling length extrapolation and adapting to extended context windows. The paper proposes a novel approach called Position Interpolation, which generates strong models that can effectively make use of much extended context windows. This method allows for substantial pre-training cost savings and preserves the quality of the original models, even for small context window tasks.\n",
      "------------------------------------------------------------\n",
      "Question: How does the Rotary Position Embedding (RoPE) approach in Transformers differ from the traditional additive method of position embedding with respect to encoding position information?\n",
      " \n",
      "Engineering Prediction: The Rotary Position Embedding (RoPE) approach in Transformers differs from the traditional additive method of position embedding by employing a multiplicative technique to incorporate positional information. Instead of adding positional encodings to the input representations, RoPE rotates the affine-transformed word embedding vector by an amount proportional to its position index. This approach naturally captures relative position information through the rotation matrix product, offering advantages in modeling long-range dependencies and equipping linear self-attention with relative position encoding. RoPE enhances the transformer's ability to model sequential data and has shown improved performance in various tasks, providing an innovative and effective alternative to traditional additive position embedding methods.\n",
      "Engineering Answer: The RoPE approach in Transformers differs from the traditional additive method of position embedding by being multiplicative instead of additive. While traditional methods add position encoding to context representations, RoPE incorporates relative position information through rotation matrix product. This means that RoPE naturally includes relative position dependency in the self-attention formulation, without altering terms in the expanded formulation like the additive method does. Additionally, RoPE's properties show that it decays as the relative distance between positions increases, providing a clear theoretical interpretation of how position information is encoded.\n",
      "\n",
      "Marketing Prediction: Rotary Position Embedding (RoPE) differs from traditional additive position embedding by using a multiplicative approach to incorporate relative position information through rotation matrix products, offering advantages in capturing long-range dependencies and improving performance in various tasks.\n",
      "Marketing Answer: The RoPE approach in Transformers differs from the traditional additive method of position embedding by incorporating relative position information through rotation matrix product instead of altering terms in the expanded formulation of additive position encoding.\n",
      "------------------------------------------------------------\n",
      "Question: What is the significance of comparing the normalized subspace similarity between ∆Wq, ∆Wv, and random Gaussian matrices when analyzing the adaptation of pre-trained language models?\n",
      " \n",
      "Engineering Prediction: Comparing the normalized subspace similarity between ∆Wq, ∆Wv, and random Gaussian matrices is a crucial aspect of understanding the adaptation mechanism in pre-trained language models. By analyzing the similarity between the column vectors of these matrices, we can gain insights into the \"intrinsic rank\" of ∆Wq and ∆Wv, which refers to the number of significant singular values they possess. This rank provides information about the effective dimensionality of the adaptation space and can help determine the optimal rank for adaptation, which is an open research question. The subspace similarity comparison also sheds light on the correlation between the pre-trained weights (W) and the adapted weights (∆W), helping us understand how the model adapts and adjusts its representations during fine-tuning.\n",
      "Engineering Answer: Comparing the normalized subspace similarity between ∆Wq, ∆Wv, and random Gaussian matrices provides insight into the underlying mechanism for adapting pre-trained language models. It helps determine the intrinsic rank of the adaptation matrix ∆W and sheds light on the connection between ∆W and the original weight matrix W. By analyzing these similarities, we can understand how much of the adaptation is specific to the task at hand and how much is influenced by the pre-trained model. This comparison is crucial for optimizing the adaptation process and maximizing downstream performance in NLP tasks.\n",
      "\n",
      "Marketing Prediction: Comparing the normalized subspace similarity between ∆Wq, ∆Wv, and random Gaussian matrices helps analyze the adaptation of pre-trained language models by providing insights into the \"intrinsic rank\" and correlation of adaptation matrices, which is essential for understanding and optimizing the performance of adapted language models in various applications.\n",
      "Marketing Answer: Comparing the normalized subspace similarity between ∆Wq, ∆Wv, and random Gaussian matrices helps understand the underlying mechanism for adapting pre-trained language models. It reveals the intrinsic rank and common singular value directions learned by different runs, shedding light on the fundamental principles of using pre-trained language models for downstream tasks in NLP.\n",
      "------------------------------------------------------------\n",
      "Question: What issues are associated with the homogeneity of language model training contractors, and how might it affect the behavior of the models?\n",
      " \n",
      "Engineering Prediction: The homogeneity of language model training contractors can lead to a lack of diversity and bias in the training data, which can then be reflected in the behavior of the models. If the contractors share similar backgrounds, perspectives, and linguistic patterns, the models may inadvertently inherit and amplify these biases. This can result in models that generate toxic or harmful outputs, particularly for under-represented groups, as the training data may contain prejudicial correlations. To address this issue, it is important to prioritize diversity and inclusion in the contractor selection process, ensuring a wide range of perspectives and linguistic backgrounds are represented. This can help mitigate biases and improve the fluency and conversational ability of dialogue agents, making them more inclusive and safe for all users.\n",
      "Engineering Answer: The issues associated with the homogeneity of language model training contractors include potential biases in the labeling process, lack of diverse perspectives leading to limited coverage of sensitive content, and reduced robustness in model performance across different tasks. This homogeneity can affect the behavior of the models by reinforcing certain biases, increasing the risk of harmful content generation, and limiting the models' ability to generalize effectively. To address these issues, it is important to ensure diversity among labelers, incorporate varied perspectives in training data, and implement measures to enhance model robustness and performance across a range of tasks.\n",
      "\n",
      "Marketing Prediction: The homogeneity of language model training contractors can lead to biased models that may cause harm in the real world; a diverse and inclusive approach to model training and evaluation can help mitigate these issues and create more beneficial applications.\n",
      "Marketing Answer: The homogeneity of language model training contractors can lead to biased or limited perspectives in the data, which may result in the models producing harmful content, gaming objectives, or lacking sensitivity to diverse viewpoints. This can affect the behavior of the models by reinforcing stereotypes, increasing toxicity, and reducing their ability to accurately represent under-represented groups.\n",
      "------------------------------------------------------------\n",
      "Question: What are common research topics and themes found in recent publications about artificial intelligence and natural language processing?\n",
      " \n",
      "Engineering Prediction: Recent publications in the field of artificial intelligence and natural language processing (NLP) have covered a diverse range of research topics and themes. Common areas of focus include image classification and speech recognition, question answering, knowledge base retrieval, and text processing. Additionally, there is significant interest in applying NLP to specific tasks such as dialogue, translation, semantic parsing, and story generation. The underlying goal of this research is often to align AI systems with human intentions and preferences, requiring an understanding of fields like deep learning, tree search, and machine learning.\n",
      "Engineering Answer: Recent publications in artificial intelligence and natural language processing have covered topics such as transformer models, feature learning in neural networks, attention mechanisms, multi-task benchmark platforms, semantic search using sentence embeddings, cross-task generalization, and question generation for question answering. Themes commonly explored include machine comprehension of text, reinforcement learning algorithms, sentence embeddings, semantic compositionality, reasoning with language models and knowledge graphs, and the gap between neural text and human text. These publications also delve into deep language understanding, retrieval-augmented transformers, image captioning, and open datasets for image-text pairs.\n",
      "\n",
      "Marketing Prediction: Recent publications in artificial intelligence and natural language processing focus on dialogue, translation, semantic parsing, story generation, and question answering, with an underlying emphasis on aligning AI systems with human intentions and improving performance through techniques like deep learning, tree search, and knowledge base retrieval.\n",
      "Marketing Answer: Common research topics and themes in recent publications on artificial intelligence and natural language processing include transformer models, attention mechanisms, semantic search, sentence embeddings, and question answering using language models and knowledge graphs.\n",
      "------------------------------------------------------------\n",
      "Question: Question: When conducting demographic and technical assessments of teams or research subjects, what types of data categories are typically collected and analyzed to ensure a comprehensive understanding of the group's composition and the methods used?\n",
      " \n",
      "Engineering Prediction: When conducting demographic and technical assessments of teams or research subjects, a comprehensive understanding of the group's composition and methods can be achieved by collecting and analyzing a diverse range of data categories. Demographically, this includes but is not limited to gender, ethnicity, age, and cultural background. Technical assessments involve evaluating the team's expertise, skills, and knowledge in relevant fields, as well as understanding the tools, technologies, and methodologies employed in their work. This technical analysis can cover areas such as software development practices, data processing techniques, modeling approaches, and any innovative strategies or processes unique to the team's workflow. By combining demographic and technical data, researchers can gain valuable insights into the team's dynamics, potential biases, and the effectiveness of their methods, enabling informed decisions and strategic planning for future research endeavors.\n",
      "Engineering Answer: When conducting demographic and technical assessments of teams or research subjects, it is important to collect and analyze data categories such as age, gender, education level, professional background, and expertise in specific areas. By gathering information on these categories, you can ensure a comprehensive understanding of the group's composition and the methods used in your assessments. Additionally, it may be helpful to consider factors like cultural background, language proficiency, and geographical location to capture a more nuanced picture of the group being assessed. This detailed approach to data collection and analysis can provide valuable insights for making informed decisions and recommendations based on the gathered information.\n",
      "\n",
      "Marketing Prediction: When assessing teams or research subjects, demographic data typically covers gender and ethnic identities, while technical data can include task categories and associated datasets, with an emphasis on diversity to ensure a comprehensive understanding of the group's composition and methods.\n",
      "Marketing Answer: Answer: Demographic data such as age, gender, education level, and technical data related to skills and experience are typically collected and analyzed for comprehensive understanding.\n",
      "------------------------------------------------------------\n",
      "Question: What kind of tasks can be performed using the datasets described in the provided text, and what are some common features of these datasets?\n",
      " \n",
      "Engineering Prediction: The text describes a diverse set of datasets that enable models to perform several natural language processing tasks. These tasks include query understanding, comprehension, conversational question answering, summarization, fact verification, and fact-checking. A common feature across these datasets is their focus on providing contextual information and synthetic user interactions to comprehensively represent various user personalities and patterns. The datasets are carefully curated and often require post-processing to ensure they accurately reflect the intended task and meet specific requirements, such as in the case of generating negative documents for the query understanding task. This approach allows for instruction-tuning of language models, enabling them to adapt to a wide range of tasks.\n",
      "Engineering Answer: The datasets described in the provided text can be used for tasks such as question answering, duplicate question retrieval, entity retrieval, citation prediction, query understanding, document understanding, passage retrieval, text summarization, fact verification, and code search. Common features of these datasets include diverse task categories, comprehensive instructions, a wide range of synthetic user personalities and interaction patterns, and a focus on enhancing comprehension of documents to deliver accurate results. Additionally, the datasets cover a variety of domains such as public health, scientific exams, climate, and general knowledge.\n",
      "\n",
      "Marketing Prediction: The datasets described in the context are used for various natural language processing tasks, including query understanding, comprehension, conversational QA, summarization, fact verification, and fact-checking. These datasets are carefully curated to represent a diverse range of synthetic user personalities and interaction patterns, ensuring that models trained on this data can handle a wide range of real-world applications and user queries effectively.\n",
      "Marketing Answer: The datasets described in the provided text can be used for tasks such as question answering, document summarization, duplicate question retrieval, code search, sentence simplification, dialogue generation, body retrieval, caption generation, fact verification, and more. Some common features of these datasets include diverse input-output pairs, incorporation of various knowledge-intensive datasets, and a focus on generating high-quality synthetic data points.\n",
      "------------------------------------------------------------\n",
      "Question: What conclusions can be drawn about the relationship between input prompt toxicity and output toxicity when using different language models and prompts?\n",
      " \n",
      "Engineering Prediction: The relationship between input prompt toxicity and output toxicity is complex and depends on various factors, including the language model used and the specific prompts employed. Generally, instruction-following models tend to generate less toxic outputs compared to non-instruction-following models when faced with toxic input prompts. However, the toxicity of the output also depends on how the language model is deployed and used in the real world. Assessing and mitigating the potential harms of language models in generating toxic outputs is an ongoing area of research, with a focus on understanding the impact of model deployment and usage on toxicity levels.\n",
      "Engineering Answer: Based on the findings presented in the results section, it can be concluded that the relationship between input prompt toxicity and output toxicity varies depending on the language model used and the specific prompt given. When instructed to produce a safe and respectful output, InstructGPT models generate less toxic outputs compared to GPT-3, but this advantage disappears when the respectful prompt is removed. On the other hand, when explicitly prompted to produce a toxic output, InstructGPT outputs are much more toxic than GPT-3 outputs. Additionally, the toxicity of the model outputs is highly correlated with the toxicity of the input prompt, as shown in Figure 39.\n",
      "\n",
      "Marketing Prediction: There is a relationship between input prompt toxicity and output toxicity, and this relationship varies depending on the language model and prompts used; however, instruction-following models generally produce less toxic output than non-instruction-following models.\n",
      "Marketing Answer: The study found that when instructed to produce a safe and respectful output, InstructGPT models generate less toxic outputs compared to GPT-3. However, this advantage disappears when the respectful prompt is removed. Interestingly, when explicitly prompted to produce a toxic output, InstructGPT outputs are much more toxic than GPT-3. This suggests that the toxicity of the output is highly correlated with the toxicity of the input prompt.\n",
      "------------------------------------------------------------\n",
      "Question: What are some challenges in training retrieval systems and how are negative samples used to address them?\n",
      " \n",
      "Engineering Prediction: Training retrieval systems face the challenge of returning irrelevant or distracting content, which can negatively impact the performance of generation models that rely on this retrieved information. Negative sampling is a technique used to address this issue by carefully curating negative documents that are irrelevant to the query, which helps the retrieval system learn to discriminate between relevant and irrelevant content. By incorporating these negative samples during training, the system can improve its ability to retrieve relevant documents and enhance the overall effectiveness of the retrieval mechanism, as demonstrated by the ablations in Table 6. This technique plays a crucial role in building robust retrieval systems that can generalize well to both in-domain and out-of-domain queries.\n",
      "Engineering Answer: Training retrieval systems face challenges such as redundancy in retrieved documents and lack of diversity in retrieval. Negative samples, including randomly sampled negatives, denoised hard negatives, and instruction-unfollowing negatives, are crucial for improving system performance. Carefully designed negative samples help the system effectively learn the task, but they can also lead to performance drops in out-of-domain datasets. Combining random samples and challenging negatives during training is key to building a competitive system for both in-domain and out-of-domain retrieval.\n",
      "\n",
      "Marketing Prediction: Training retrieval systems face the challenge of returning irrelevant content, but incorporating negative samples helps guide the model to produce more relevant outputs and improves overall performance.\n",
      "Marketing Answer: Some challenges in training retrieval systems include high cost of annotating datasets for new tasks and improving performance in zero-shot settings. Negative samples, such as denoised hard negative documents and instruction-unfollowing negative documents, are used to train retrieval systems effectively and address performance drops in out-of-domain datasets.\n",
      "------------------------------------------------------------\n",
      "Question: What factors have been found to potentially impact the ability of models to follow instructions, based on the analysis provided?\n",
      " \n",
      "Engineering Prediction: The ability of models to follow instructions can be impacted by several factors, including the presence or absence of instructions during training and testing, the diversity of training data, and the effects of human feedback and value judgments in the labeling process. The analysis suggests that models trained and evaluated without instructions perform poorly, indicating that instructions are crucial for effective performance. Additionally, diverse training data, including duplicated or paraphrased questions, can influence the model's ability to generalize and follow instructions accurately. The human feedback obtained during the labeling process, which involves value judgments, can also shape the behavior of the models and potentially impact their instruction-following capabilities.\n",
      "Engineering Answer: Based on the analysis provided, factors that have been found to potentially impact the ability of models to follow instructions include the human feedback obtained from contractors, which may be influenced by their beliefs, cultural backgrounds, and personal history. Additionally, the model's behavior can be affected by false premises in instructions, tendencies to hedge, and performance degradation with multiple explicit constraints in instructions. The models are also not fully aligned or safe, as they can generate toxic or biased outputs, make up facts, and fail to generate reasonable outputs in some cases.\n",
      "\n",
      "Marketing Prediction: The ability of models to follow instructions can be impacted by the presence or absence of instructions during training and testing, the diversity of training data, and human feedback obtained during the labeling process.\n",
      "Marketing Answer: Factors that may impact the ability of models to follow instructions include false premises in instructions, models hedging unnecessarily, performance degradation with multiple constraints in instructions, generation of toxic or biased outputs, and over-generalization leading to refusal of innocuous instructions.\n",
      "------------------------------------------------------------\n",
      "Question: What are some key factors to consider when building a successful multi-task instruction-following retrieval system as identified in the research?\n",
      " \n",
      "Engineering Prediction: Some key factors for building a successful multi-task instruction-following retrieval system include the use of informative instructions during training and testing, diversity in both the dataset and model scale, and carefully curated negative samples. Additionally, training a single retriever on multiple tasks without task-specific architectures can yield benefits from cross-task interdependence. This approach has proven effective in instruction tuning for large language models. Evaluating the system on benchmarks like BEIR, LOTTE, and the newly introduced X2-Retrieval also contributes to its success.\n",
      "Engineering Answer: Some key factors to consider when building a successful multi-task instruction-following retrieval system include the need for cross-task interdependence for training a single retriever, the flexibility and zero-shot transfer enabled by instructions compared to task identifiers, and the elimination of the need for hosting multiple task-specific retrievers. Additionally, optimizing the mix and volume of instructional data for diverse tasks is crucial, as well as considering the impact of ranking strategy in data construction. Finally, the effectiveness of the dataset scale in retrieval and the importance of carefully designed negative samples should be taken into account for improved efficiency of instruction-following retrievers.\n",
      "\n",
      "Marketing Prediction: Some key factors for building a successful multi-task instruction-following retrieval system include informative instructions, diverse data and models, and carefully designed negative samples, which together enable the system to effectively learn and adapt to various tasks and ultimately improve efficiency in instruction-following.\n",
      "Marketing Answer: Key factors to consider when building a successful multi-task instruction-following retrieval system include the effectiveness of the dataset scale in retrieval, the diversity in data and model scale, carefully designed negative samples, and the ability to adapt to new tasks via instructions.\n",
      "------------------------------------------------------------\n",
      "Question: What are the benefits of using retrieval-augmented techniques in multimodal language modeling, as demonstrated by the performance of the RA-CM3 model in the document?\n",
      " \n",
      "Engineering Prediction: The RA-CM3 model's performance highlights the advantages of employing retrieval-augmented techniques in multimodal language modeling. By leveraging retrieval augmentation, the model can concentrate on understanding how to utilize retrieved documents effectively, leading to enhanced training efficiency. This approach is particularly beneficial for energy conservation as it necessitates significantly less computational power during training compared to existing models. Furthermore, retrieval augmentation proves valuable in capturing long-tail knowledge, including rare entities or minor event types, thereby enhancing the model's ability to handle diverse and complex data. The RA-CM3 model surpasses the retrieval baseline, underscoring the effectiveness of combining retrieval-augmented and non-retrieval-augmented models in multimodal tasks.\n",
      "Engineering Answer: The benefits of using retrieval-augmented techniques in multimodal language modeling, as demonstrated by the performance of the RA-CM3 model, include significantly better training efficiency with less training compute, outperforming existing models by using less training data, compute, and parameters. The retrieval augmentation allows the model to focus on learning how to use retrieved documents in context, leading to improved accuracy in classification tasks. Additionally, the RA-CM3 model achieves strong performance in image and caption generation, surpassing existing models like DALL-E and Flamingo despite using fewer resources.\n",
      "\n",
      "Marketing Prediction: The RA-CM3 model demonstrates that retrieval-augmented techniques improve training efficiency and energy savings, enabling the model to capture long-tail knowledge and generalize to new multimodal data more effectively.\n",
      "Marketing Answer: The benefits of using retrieval-augmented techniques in multimodal language modeling, as demonstrated by the performance of the RA-CM3 model in the document, include outperforming existing models by using less training data, compute, and parameters, achieving significantly better training efficiency, and improving accuracy in k-shot classification tasks. Additionally, retrieval augmentation allows the model to focus on learning how to use retrieved documents in context, leading to stronger performance in tasks such as image and caption generation.\n",
      "------------------------------------------------------------\n",
      "Question: What methods are typically employed to create training data for embedding models that use task-specific instructions?\n",
      " \n",
      "Engineering Prediction: Training data for embedding models that employ task-specific instructions can be generated through a few approaches. One method involves combining large-scale datasets that contain annotated examples with instructions across various tasks and domains. This provides a diverse set of instructions and contexts for the model to learn from. Another approach is to utilize unlabeled corpora and automatically generate training data by training customized retrievers for each task. These retrievers can then be used to find relevant examples for specific tasks, enhancing the model's performance on those tasks. Additionally, human feedback can be incorporated into the training process to improve the model's ability to follow instructions accurately. These methods collectively contribute to the development of effective embedding models that can understand and execute task-specific instructions.\n",
      "Engineering Answer: To create training data for embedding models that use task-specific instructions, a common method is to combine datasets from different sources, such as the SuperNaturalInstructions dataset with existing collections designed for embedding training. The SuperNaturalInstructions dataset provides natural language instructions, which can be paired with positive and negative examples to form training samples. Additionally, for tasks like classification or similarity, training samples can be constructed by selecting text sequences associated with different classes or similarities. This diverse training data is essential for instruction-based finetuning, which enables the embedding model to learn from a wide range of tasks and domains.\n",
      "\n",
      "Marketing Prediction: Training data for embedding models with task-specific instructions can be generated using another model to automatically create training data for a customized retriever, which then encodes annotated examples to find similar instances for in-context learning.\n",
      "Marketing Answer: Training data for embedding models that use task-specific instructions is typically created by formulating a wide variety of tasks as text-to-text problems, distinguishing good/bad candidate outputs given an input text. This is done by combining datasets with natural language instructions and constructing positive and negative pairs for training.\n",
      "------------------------------------------------------------\n",
      "Question: Question: What are some of the challenges and innovations associated with fine-tuning large language models, and how does the approach discussed in the referenced text aim to address them?\n",
      " \n",
      "Engineering Prediction: Fine-tuning large language models is a challenging task due to the significant computational resources required, and it raises concerns about environmental impact and accessibility. The introduction of novel fine-tuning techniques is essential to address these challenges and improve model performance, especially for specific downstream tasks. The referenced text discusses an approach that focuses on learning from human preferences to enhance the model's factuality and citation accuracy for long-form generations. This method aims to align language model behavior with human expectations, reducing undesirable outputs such as toxic and biased responses. By incorporating human feedback and preferences, the fine-tuning process can be optimized to make models more accurate and reliable, ensuring they generate content that meets user needs and expectations.\n",
      "Engineering Answer: Some challenges associated with fine-tuning large language models include limited access to and manipulation of knowledge, lagging performance on knowledge-intensive tasks, and the need for provenance in decision-making and updating world knowledge. The approach discussed in the referenced text aims to address these challenges by utilizing Retrieval Augmented Generation (RAG), which involves retrieving relevant passages from a corpus to feed to the language model for improved performance in tasks such as question-answering and dialogue. This iterative approach focuses on improving alignment with user intent and fine-tuning models to control sentiment and improve response quality in various language tasks.\n",
      "\n",
      "Marketing Prediction: Fine-tuning large language models is resource-intensive, and the text introduces an approach that aims to address the challenges of improving model behavior and performance by leveraging human preferences to enhance language generation quality and factual accuracy.\n",
      "Marketing Answer: The challenges with fine-tuning large language models include aligning them with user intent and controlling the quality of generated outputs. The approach discussed in the referenced text aims to address these challenges by using Retrieval Augmented Generation (RAG) to retrieve relevant passages from a corpus and feed them to the language model, improving alignment and performance.\n",
      "------------------------------------------------------------\n",
      "Question: What is a common technique used to address the outlier issue when applying block-wise k-bit quantization to input tensors, and how does it work?\n",
      " \n",
      "Engineering Prediction: To address the outlier issue in block-wise k-bit quantization of input tensors, a common technique is to divide the input tensor into smaller blocks, with each block being independently quantized with its own quantization constant, 'c'. This method helps manage outliers by adapting the quantization to the local statistics of each block, ensuring that extreme values do not significantly impact the overall quantization process.\n",
      "Engineering Answer: A common technique used to address the outlier issue when applying block-wise k-bit quantization to input tensors is to chunk the input tensor into blocks that are independently quantized, each with their own quantization constant. This approach involves dividing the input tensor into contiguous blocks of size B by flattening the tensor and slicing it into n blocks, where n is determined by the size of the blocks. Each block is then quantized independently using a quantization constant c, which helps prevent outlier values from causing performance degradation.\n",
      "\n",
      "Marketing Prediction: A common approach to addressing the outlier issue in block-wise k-bit quantization is to chunk the input tensor into smaller blocks, allowing independent quantization of each block with its own quantization constant, effectively managing outliers and preserving LLM quality.\n",
      "Marketing Answer: A common technique used to address the outlier issue when applying block-wise k-bit quantization to input tensors is to chunk the input tensor into blocks that are independently quantized, each with their own quantization constant. This helps prevent performance degradation by reducing the impact of outliers on the quantization process.\n",
      "------------------------------------------------------------\n",
      "Question: What considerations or techniques are commonly implemented when setting up finetuning experiments for machine learning models?\n",
      " \n",
      "Engineering Prediction: When setting up fine-tuning experiments for machine learning models, it is common to initialize the model with pre-trained weights and biases, a technique known as \"Fine-Tuning (FT).\" This approach allows for the adaptation of the model to new tasks or datasets by updating all model parameters through gradient descent or similar optimization methods. The number of epochs and the choice of optimization algorithm are crucial considerations, as these factors can impact the model's performance and convergence. Additionally, techniques like mixing pretraining data with fine-tuning data or using larger batch sizes can enhance the fine-tuning process and model generalization. The specific techniques employed may vary based on the model architecture and the nature of the task at hand, showcasing the importance of experimental design and hyperparameter tuning in the fine-tuning process.\n",
      "Engineering Answer: When setting up finetuning experiments for machine learning models, it is common to use a two-stage approach. The initial stage involves setting the initial parameters using a language modeling objective. This is followed by a supervised discriminative 'fine-tuning' stage to adapt these parameters to the target task. Additionally, it is typical to train all models using the Adam optimizer and a triangular learning rate scheduler with 10% warmup. Experimentation with different hyperparameters such as number of epochs, peak learning rate, and batch size is also conducted to optimize model performance. Finally, utilizing a mixture of datasets and balancing the sizes of datasets can help improve the robustness and generalization of the finetuned models.\n",
      "\n",
      "Marketing Prediction: When setting up fine-tuning experiments for machine learning models, it's essential to initialize the model with pre-trained weights and biases, apply supervised fine-tuning with a mix of demonstration and pretraining data, and evaluate performance across various tasks, datasets, and model types to ensure scalability and effectiveness.\n",
      "Marketing Answer: Considerations for setting up finetuning experiments for machine learning models commonly include using a language modeling objective for initial parameter setting and supervised discriminative fine-tuning for adapting parameters to the target task. Techniques such as hyperparameter search, Adam optimizer with triangular learning rate scheduler, and balancing dataset sizes through mixing strategies are also commonly implemented. Additionally, freezing some model layers during fine-tuning and incorporating negative examples for contrastive learning can be effective strategies.\n",
      "------------------------------------------------------------\n",
      "Question: What are the implications of the equivalence relation defined in the theoretical analysis of the DPO model for understanding the relationship between reward functions in reinforcement learning?\n",
      " \n",
      "Engineering Prediction: Theoretical analysis of the DPO model introduces an equivalence relation between reward functions, which has important implications for understanding the relationship between different reward functions in reinforcement learning. This relation partitions reward functions into equivalence classes, where functions within the same class differ only by a constant factor dependent on the state. This means that the DPO reparameterization selects a specific reward function within each class, providing a way to compare and evaluate the performance of different reward functions and their impact on the learned policy. This analysis offers insights into the trade-offs between reward function design choices and their influence on the final policy, allowing researchers to make more informed decisions when designing reward functions for RL tasks.\n",
      "Engineering Answer: The equivalence relation defined in the theoretical analysis of the DPO model implies that two reward functions are considered equivalent if they differ by a constant function. This means that the class of learned reward models is not constrained by this reparameterization, allowing for the exact recovery of the optimal policy. Understanding this relationship between reward functions in reinforcement learning helps in defining a unique reward function within each equivalence class, which is crucial for optimizing policies under existing models of human preferences. It also highlights the generality and flexibility in the reward model due to the proposed reparameterization.\n",
      "\n",
      "Marketing Prediction: The equivalence relation defined in the DPO model's theoretical analysis reveals that reward functions differing only by a value dependent on the state but not the action are essentially interchangeable, offering insights into reward function design and optimization in reinforcement learning.\n",
      "Marketing Answer: The equivalence relation defined in the theoretical analysis of the DPO model shows that two reward functions are considered equivalent if they differ by a fixed function. This implies that different reward functions can lead to the same optimal policy, allowing for flexibility in designing reward models in reinforcement learning.\n"
     ]
    }
   ],
   "source": [
    "val_eval = evaluate(metrics, validation_dict, rag_chains, iterations=0, verbose=False, dept_specific=True, print_results=True)\n",
    "val_eval = composite_evaluation(val_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "n-kiT20xYE1e",
    "outputId": "f55f29ab-f2e8-4e68-d9d4-1cfec9e5ff02"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"val_eval\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"Sample\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 14.38460451364229,\n        \"min\": 12.318568359869648,\n        \"max\": 55.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          35.333333333333336,\n          35.5,\n          30.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eng_bleu\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10.558996878071012,\n        \"min\": 0.0,\n        \"max\": 30.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.10741694996408047,\n          0.09613867528373769,\n          30.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mk_bleu\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10.561851343731615,\n        \"min\": 0.0,\n        \"max\": 30.0,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          30.0,\n          0.1026796858295506,\n          0.1672900173172493\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eng_rouge\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10.509437062706894,\n        \"min\": 0.09176245082022091,\n        \"max\": 30.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.2723352920975987,\n          0.25353694138782845,\n          30.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mk_rouge\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10.498355135495228,\n        \"min\": 0.126984126984127,\n        \"max\": 30.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.30717839307002837,\n          0.27178030303030304,\n          30.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eng_f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10.341511867827263,\n        \"min\": 0.014865160840370307,\n        \"max\": 30.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.8853943725426991,\n          0.8836797177791595,\n          30.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mk_f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10.339624897918359,\n        \"min\": 0.02216327584743035,\n        \"max\": 30.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.8923687915007273,\n          0.8884374797344208,\n          30.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"composite_eng\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10.435214212546452,\n        \"min\": 0.04935779186975457,\n        \"max\": 30.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.5458811638934453,\n          0.5333793114364517,\n          30.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"composite_mk\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10.431118040538701,\n        \"min\": 0.0700039053171269,\n        \"max\": 30.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.5588738508372822,\n          0.5456744458475039,\n          30.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"composite_total\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10.4333743536653,\n        \"min\": 0.054150848837711145,\n        \"max\": 30.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.55107823867098,\n          0.5361975106207773,\n          30.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-80ecb1d1-1d11-40ae-9e56-0927eb4fcd97\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample</th>\n",
       "      <th>eng_bleu</th>\n",
       "      <th>mk_bleu</th>\n",
       "      <th>eng_rouge</th>\n",
       "      <th>mk_rouge</th>\n",
       "      <th>eng_f1</th>\n",
       "      <th>mk_f1</th>\n",
       "      <th>composite_eng</th>\n",
       "      <th>composite_mk</th>\n",
       "      <th>composite_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>35.333333</td>\n",
       "      <td>0.107417</td>\n",
       "      <td>0.102680</td>\n",
       "      <td>0.272335</td>\n",
       "      <td>0.307178</td>\n",
       "      <td>0.885394</td>\n",
       "      <td>0.892369</td>\n",
       "      <td>0.545881</td>\n",
       "      <td>0.558874</td>\n",
       "      <td>0.551078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>12.318568</td>\n",
       "      <td>0.090938</td>\n",
       "      <td>0.109589</td>\n",
       "      <td>0.091762</td>\n",
       "      <td>0.138567</td>\n",
       "      <td>0.014865</td>\n",
       "      <td>0.022163</td>\n",
       "      <td>0.049358</td>\n",
       "      <td>0.070004</td>\n",
       "      <td>0.054151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.164557</td>\n",
       "      <td>0.126984</td>\n",
       "      <td>0.858475</td>\n",
       "      <td>0.850460</td>\n",
       "      <td>0.481927</td>\n",
       "      <td>0.463325</td>\n",
       "      <td>0.482244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>24.250000</td>\n",
       "      <td>0.059236</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.226618</td>\n",
       "      <td>0.220343</td>\n",
       "      <td>0.875913</td>\n",
       "      <td>0.876366</td>\n",
       "      <td>0.519325</td>\n",
       "      <td>0.509367</td>\n",
       "      <td>0.522835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>35.500000</td>\n",
       "      <td>0.096139</td>\n",
       "      <td>0.075702</td>\n",
       "      <td>0.253537</td>\n",
       "      <td>0.271780</td>\n",
       "      <td>0.883680</td>\n",
       "      <td>0.888437</td>\n",
       "      <td>0.533379</td>\n",
       "      <td>0.545674</td>\n",
       "      <td>0.536198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>45.750000</td>\n",
       "      <td>0.130190</td>\n",
       "      <td>0.167290</td>\n",
       "      <td>0.293094</td>\n",
       "      <td>0.356860</td>\n",
       "      <td>0.892692</td>\n",
       "      <td>0.903393</td>\n",
       "      <td>0.557593</td>\n",
       "      <td>0.589656</td>\n",
       "      <td>0.564940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>55.000000</td>\n",
       "      <td>0.478152</td>\n",
       "      <td>0.449700</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.758621</td>\n",
       "      <td>0.927063</td>\n",
       "      <td>0.942075</td>\n",
       "      <td>0.742196</td>\n",
       "      <td>0.774845</td>\n",
       "      <td>0.755255</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-80ecb1d1-1d11-40ae-9e56-0927eb4fcd97')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-80ecb1d1-1d11-40ae-9e56-0927eb4fcd97 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-80ecb1d1-1d11-40ae-9e56-0927eb4fcd97');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-9715c570-f0a3-4b94-9155-47dbf9b52bdf\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9715c570-f0a3-4b94-9155-47dbf9b52bdf')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-9715c570-f0a3-4b94-9155-47dbf9b52bdf button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "          Sample   eng_bleu    mk_bleu  eng_rouge   mk_rouge     eng_f1  \\\n",
       "count  30.000000  30.000000  30.000000  30.000000  30.000000  30.000000   \n",
       "mean   35.333333   0.107417   0.102680   0.272335   0.307178   0.885394   \n",
       "std    12.318568   0.090938   0.109589   0.091762   0.138567   0.014865   \n",
       "min    16.000000   0.000000   0.000000   0.164557   0.126984   0.858475   \n",
       "25%    24.250000   0.059236   0.000000   0.226618   0.220343   0.875913   \n",
       "50%    35.500000   0.096139   0.075702   0.253537   0.271780   0.883680   \n",
       "75%    45.750000   0.130190   0.167290   0.293094   0.356860   0.892692   \n",
       "max    55.000000   0.478152   0.449700   0.647059   0.758621   0.927063   \n",
       "\n",
       "           mk_f1  composite_eng  composite_mk  composite_total  \n",
       "count  30.000000      30.000000     30.000000        30.000000  \n",
       "mean    0.892369       0.545881      0.558874         0.551078  \n",
       "std     0.022163       0.049358      0.070004         0.054151  \n",
       "min     0.850460       0.481927      0.463325         0.482244  \n",
       "25%     0.876366       0.519325      0.509367         0.522835  \n",
       "50%     0.888437       0.533379      0.545674         0.536198  \n",
       "75%     0.903393       0.557593      0.589656         0.564940  \n",
       "max     0.942075       0.742196      0.774845         0.755255  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_eval.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BrZaB5JPfm7W"
   },
   "source": [
    "From a metric standpoint, we are getting even results between Marketing and Engineering.  However, as we highlighted in the previous run, there are some significant concerns around accruacy of the output of the model.  We are going to review those same specific questions to see what we got:\n",
    "\n",
    " - Question: How has the token handling capacity changed between different versions of the Claude model?\n",
    " - The predicted answers now highlight that the capacity as increased over time without explictly stating the levels. This is a step better than what we previously got.\n",
    "\n",
    "And for the second reviewed question:\n",
    "\n",
    " - Question: What benchmark did Chinchilla achieve an average accuracy of 67.5% on?\n",
    " - The predicted answers are now returning a different wrong benchmark, HellaSwag.\n",
    "\n",
    "While slight improvements over the previous answers, there is still plenty of room for concern.  But we will move forward with this embedding for our final evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52AVMJorF7Gh"
   },
   "source": [
    "## 5) Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1MIqToegpFHy"
   },
   "source": [
    "We have now come to the final selection of our model.  With the final selection confirmed, we will now evaluate our model against the hold out set.  We will evaluate it for the orginal baseline model, as well as for our finalized model selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-4pS3oI9Yh9w"
   },
   "source": [
    "#### Baseline Model on Holdout Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BuwOV16fYhkA"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "embedding_model = \"multi-qa-mpnet-base-dot-v1\"\n",
    "chunk_size = 128\n",
    "chunk_overlap = 0\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "llm = \"cohere\"\n",
    "rag_template = \"\"\"[INST]Please answer the question below only based on the context information provided.\\n\\nHere is a context:\\n{context} \\n\\nHere is a question: \\n{question}.[/INST]\"\"\"\n",
    "\n",
    "base_embeddings, text_splitter, qdrant_vectorstore, retriever = build_embedding_splitter_vectorstore(embedding_model, splitter)\n",
    "qdrant_vectorstore = vectorize_documents(text_splitter, qdrant_vectorstore)\n",
    "llm_model = load_llm(llm)\n",
    "rag_chain = build_RAG_prompt_chain(rag_template, llm_model, retriever, format_docs)\n",
    "metrics = ['rouge', 'bleu', 'bertscore']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wVQFjCgxcc1n",
    "outputId": "9ef062b9-f08e-4a81-c8a3-d3d804cff40b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Question: Considering the structure and content of the provided text, what guidelines should be used to evaluate the effectiveness of a summary or chatbot response in this context?\n",
      " \n",
      "Engineering Prediction: To evaluate the effectiveness of a summary or chatbot response in this context, the following guidelines should be considered: \n",
      "\n",
      "- Precision and conciseness: A good summary should capture the most important information accurately and succinctly. Irrelevant details should be omitted to maintain clarity and focus on the key points. \n",
      "\n",
      "- Relevance: The summary or response should directly address the topic or query. It should provide information that is pertinent to the specific issue or question raised. \n",
      "\n",
      "- Coherence: The structure of the summary or response should be logical and easy to follow. Ideas should be presented in a clear and organized manner, ensuring a coherent argument or narrative. \n",
      "\n",
      "- Completeness: While conciseness is important, the summary or response should also provide sufficient information to address the query adequately. Key elements of the source material or conversation should not be overlooked or simplified excessively. \n",
      "\n",
      "- Contextual understanding: The chatbot should demonstrate an understanding of the source material or conversation. This includes grasping the nuances, sentiment, and intent behind the words to generate an appropriate and contextually relevant response. \n",
      "\n",
      "- Specificity: The summary or response should provide specific details or examples to support any claims or statements made. Vague or generalized statements may indicate a lack of understanding or an inability to provide meaningful insights. \n",
      "\n",
      "- Evaluation and critique: The inclusion of a critique token or evaluation step allows for a reflective analysis of the summary or response. This self-assessment ensures that the information presented is accurate, adequate, and aligns with the query's requirements. \n",
      "\n",
      "These guidelines emphasize the importance of accuracy, relevance, clarity, and critical evaluation in summarization tasks and chatbot responses. They aim to ensure that the information presented is reliable, informative, and effectively addresses the user's query or the source material's key points.\n",
      "Engineering Answer: To evaluate the effectiveness of a summary or chatbot response in this context, guidelines should include assessing the faithfulness of the answer to the retrieved context, the relevance of the answer to the question, and the focus of the retrieved context. Additionally, consider using quality metrics such as answer relevancy to rank responses based on how directly they address the question and avoid redundant or incomplete information. Lastly, take into account the performance of different tasks such as summarization, citation prediction, and passage ranking to determine the overall effectiveness of the response.\n",
      "\n",
      "Marketing Prediction: To evaluate the effectiveness of a summary or chatbot response in this context, the following guidelines should be considered: \n",
      "\n",
      "- Precision and conciseness: A good summary should capture the most important information accurately and succinctly. Irrelevant details should be omitted to maintain clarity and focus on the key points. \n",
      "\n",
      "- Relevance: The summary or response should directly address the topic or query. It should provide information that is pertinent to the specific issue or question raised. \n",
      "\n",
      "- Coherence: The structure of the summary or response should be logical and easy to follow. Ideas should be presented in a clear and organized manner, ensuring a coherent argument or explanation. \n",
      "\n",
      "- Completeness: While conciseness is important, the summary or response should also provide sufficient information to address the query adequately. Key elements of the source material or conversation should not be overlooked or simplified excessively. \n",
      "\n",
      "- Contextual understanding: The chatbot should demonstrate an understanding of the source material or conversation. This includes grasping the nuances, sentiment, and intent behind the words to generate an appropriate and contextually relevant response. \n",
      "\n",
      "- Evaluation of generated responses: For chatbot responses, it is important to assess whether the response is meaningful and addresses the user's query effectively. This includes evaluating the language, tone, and accuracy of the information provided by the chatbot. \n",
      "\n",
      "- Adaptability to complexity: The summary or response should be able to handle more complex queries or topics effectively. This includes the ability to summarize or respond to nuanced or multi-faceted issues while still maintaining precision and comprehensiveness. \n",
      "\n",
      "These guidelines can help assess the quality and effectiveness of a summary or chatbot response, ensuring that it meets the desired standards of accuracy, clarity, and relevance.\n",
      "Marketing Answer: Answer: Evaluate based on faithfulness, answer relevance, and context relevance.\n",
      "------------------------------------------------------------\n",
      "Question: What are some recent methods and technologies that have been developed to enhance the capabilities and performance of natural language processing models?\n",
      " \n",
      "Engineering Prediction: Recent methods and technologies that have enhanced natural language processing (NLP) models include: \n",
      "\n",
      "- **Transformer Architecture**: The introduction of the Transformer architecture by Vaswani et al. in 2017 was a breakthrough for NLP. This mechanism allowed models to process words in parallel, making them more efficient and effective at capturing long-range dependencies in text. Models like BERT and GPT are based on this architecture and have achieved state-of-the-art performance on various NLP tasks. \n",
      "\n",
      "- **Pre-training and Fine-tuning**: Pre-training large models on vast amounts of text data, and then fine-tuning them on specific tasks, has been a game-changer. This approach, popularized by models like BERT, RoBERTa, and T5, has led to significant performance improvements across a wide range of NLP tasks. These models learn rich language representations during pre-training, which can then be adapted to specific tasks with relatively little data. \n",
      "\n",
      "- **Contextual Word Embeddings**: Unlike traditional word embeddings that provide a single vector representation for each word, contextual word embeddings, such as those used in BERT, capture the meaning of words based on their surrounding context. This allows models to understand words with multiple meanings more effectively and has improved performance on tasks requiring nuanced language understanding. \n",
      "\n",
      "- **Large-scale Language Models**: Building larger models with billions of parameters, trained on massive amounts of text data, has led to significant performance gains. Examples include GPT-3 from OpenAI and Gopher from DeepMind, which have demonstrated impressive capabilities in generating human-like text and performing few-shot learning on various NLP tasks. \n",
      "\n",
      "- **Transfer Learning**: Transfer learning techniques, where pre-trained models are fine-tuned on specific tasks, have been widely adopted in NLP. This approach leverages the knowledge learned from large-scale pre-training and enables models to quickly adapt to new tasks with less data, improving performance and reducing training time. \n",
      "\n",
      "- **Neural Machine Translation**: Advances in neural machine translation have improved the quality of automated language translation significantly. Models like Transformer-based architectures and sequence-to-sequence models with attention mechanisms have achieved impressive results, making cross-lingual communication more accessible and accurate. \n",
      "\n",
      "- **Natural Language Understanding (NLU)**: There have been advancements in NLU tasks such as sentiment analysis, named entity recognition, and question answering. Models like BERT and its variants have pushed the boundaries of performance on these tasks, enabling better understanding and interpretation of natural language text. \n",
      "\n",
      "- **Dialogue Systems**: Improvements in dialogue systems, often powered by large-scale language models, have led to more human-like and contextually appropriate responses in chatbots and virtual assistants. These systems can maintain coherent and contextually relevant conversations, enhancing user experiences. \n",
      "\n",
      "- **Multilingual NLP**: Methods for training multilingual models that can understand and generate text in multiple languages have improved. Models like mBERT and XLM-R can handle a wide range of languages, enabling more inclusive and globally applicable NLP applications. \n",
      "\n",
      "- **NLP for Low-Resource Languages**: There is a growing focus on developing methods and resources for low-resource languages, where limited annotated data is available. Techniques like transfer learning, zero-shot learning, and data augmentation are being explored to improve NLP performance in these languages. \n",
      "\n",
      "These recent methods and technologies have significantly enhanced the capabilities and performance of NLP models, pushing the field forward and enabling a wide range of practical applications.\n",
      "Engineering Answer: Recent methods and technologies developed to enhance natural language processing models include retrieval-augmented multimodal language modeling, which outperforms existing models with less training data and parameters. Another advancement is the use of feature learning in infinite-width neural networks to improve performance. Additionally, embedding techniques in NLP have been developed to map words or phrases to real number vectors, enhancing the model's understanding of language. These innovations have led to improvements in tasks like query reformulation, document ranking, and fine-tuning larger language models for various applications.\n",
      "\n",
      "Marketing Prediction: Recent methods and technologies that have enhanced natural language processing (NLP) models include: \n",
      "\n",
      "- **Transformer Architecture**: This is a key innovation in NLP, allowing models to process words in parallel, capturing long-range dependencies in the data. Models like BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer) use this architecture and have achieved state-of-the-art performance on various NLP tasks. \n",
      "\n",
      "- **Pre-training and Fine-tuning**: This technique involves pre-training a large language model on a vast amount of text data, and then fine-tuning it on a specific task. This method has proven effective in leveraging large-scale unsupervised data to improve performance on a wide range of NLP tasks. \n",
      "\n",
      "- **Contextual Word Embeddings**: Unlike traditional word embeddings that provide a single vector representation for each word, contextual embeddings, such as those used in BERT, provide dynamic word representations that depend on the context in which the word is used. This allows for a more nuanced understanding of language. \n",
      "\n",
      "- **Transfer Learning**: Transfer learning techniques have been applied to NLP, where pre-trained models are used as a starting point and then adapted for specific tasks. This has been shown to be effective, especially for tasks with limited labeled data. \n",
      "\n",
      "- **Neural Machine Translation**: This approach uses neural networks to automatically translate text from one language to another, and has significantly improved the quality and speed of machine translation. \n",
      "\n",
      "- **Natural Language Understanding (NLU)**: NLU models aim to understand the semantics and context of natural language input. Recent advances include models that can answer questions, generate summaries, and perform sentiment analysis with high accuracy. \n",
      "\n",
      "- **Large Language Models**: Models with a large number of parameters, such as GPT-3, have shown impressive capabilities in generating human-like text and performing few-shot learning, where they can learn new tasks from just a few examples. \n",
      "\n",
      "- **Multi-task Learning**: This involves training a single model on multiple tasks simultaneously, which can improve performance on all tasks by leveraging shared information and representations. \n",
      "\n",
      "These methods and technologies have significantly advanced the field of NLP, leading to models that are more accurate, flexible, and capable of understanding and generating human-like language.\n",
      "Marketing Answer: Recent methods and technologies include retrieval-augmented language models, feature learning in infinite-width neural networks, and word embeddings.\n",
      "------------------------------------------------------------\n",
      "Question: What are some potential directions for future work mentioned in the document related to enhancing question-answering techniques for document-oriented tasks?\n",
      " \n",
      "Engineering Prediction: The document mentions DocVQA as a future direction for enhancing question-answering techniques in document-oriented tasks. This approach focuses on generating structured metadata to capture different document question-answering tasks relevant across various professional fields, improving precision and accuracy in document question-answering.\n",
      "Engineering Answer: One potential direction for future work mentioned in the document is the development of multi-modal approaches that incorporate table and figure information into GPT-4 question-answering for documents. Another direction is to incorporate question type in the PDFTriage approach to improve the efficiency and efficacy of the approach. Additionally, the document suggests further research in document-grounded, information-seeking question answering, which the dataset is designed to facilitate.\n",
      "\n",
      "Marketing Prediction: The document mentions DocVQA as a future direction for enhancing question-answering techniques in document-oriented tasks. This approach focuses on generating structured metadata to capture different document question-answering tasks relevant across various professional fields, aiming for precision and accuracy.\n",
      "Marketing Answer: Some potential future directions mentioned in the document include developing multi-modal approaches that incorporate table and figure information into question-answering for documents, and incorporating question type in the PDFTriage approach to improve efficiency and efficacy.\n",
      "------------------------------------------------------------\n",
      "Question: What information would you expect to find in section 2 of a document, based on the types of questions classified under Summarization?\n",
      " \n",
      "Engineering Prediction: Based on the types of questions classified under summarization, I would expect to find a concise and comprehensive overview of the key points, main ideas, and important information covered in that specific section of the document. This summary should capture the essence of that particular section, highlighting the most relevant and salient details while providing a coherent and concise description. It should help the reader quickly grasp the important insights and understandings conveyed in that part of the text.\n",
      "Engineering Answer: Based on the types of questions classified under Summarization, you would expect to find key takeaways, concise summaries, and specific content extraction related to different sections of the document in section 2. The section likely contains detailed summaries of specific parts of the document, along with structured metadata representation and instructions for summarizing the content effectively. It may also include guidelines for extracting specific information and rewriting text for clarity and conciseness.\n",
      "\n",
      "Marketing Prediction: Based on the types of questions classified under summarization, I would expect to find a concise and comprehensive overview of the key points, main ideas, and important information covered in that specific section of the document. This summary should capture the essence of that particular section, highlighting the most relevant and salient details while providing a coherent and concise description. It should help the reader quickly grasp the important insights and understandings conveyed in that part of the text.\n",
      "Marketing Answer: Based on the types of questions classified under Summarization, you would expect to find key takeaways, concise summaries, and specific content extraction related to the document in section 2.\n",
      "------------------------------------------------------------\n",
      "Question: What are the main advantages and attention mechanisms that contribute to the enhanced performance and efficiency of the newly introduced language model as compared to its predecessors?\n",
      " \n",
      "Engineering Prediction: The main advantages of the newly introduced language model, as compared to its predecessors, include: \n",
      "\n",
      "- Increased effectiveness and performance in language modeling tasks, as indicated by the research paper's results. \n",
      "- Promise of making language models more helpful and efficient, suggesting an improvement in the overall utility of such models. \n",
      "- Carefully designed architecture, allowing for high performance and maintaining efficiency simultaneously. \n",
      "\n",
      "Regarding attention mechanisms, the paper likely employs techniques like self-attention or transformer models, which have been pivotal in the success of modern language models. These mechanisms enable the model to weigh the importance of different input words or tokens when generating a response, contributing to improved context understanding and more accurate predictions. \n",
      "\n",
      "However, without the full paper or additional context, it is challenging to provide specific details about the exact attention mechanisms utilized. The provided information suggests that the model's performance and efficiency enhancements are attributed to a combination of careful design, attention mechanisms, and optimization techniques tailored to language modeling tasks.\n",
      "Engineering Answer: The main advantages of the newly introduced language model include utilizing retrieval-augmentation to incorporate external knowledge, which improves prediction accuracy. Additionally, the model employs attention mechanisms that allow for better understanding of dependencies between source and target sequences, leading to more informed predictions. These attention mechanisms have been extended from machine translation to various other fields, enhancing the model's adaptability and performance across different tasks. Finally, the model's use of self-attention mechanisms enables better contextual representation learning, parallelization, and modeling of longer intra-token relations, improving efficiency and performance compared to previous models.\n",
      "\n",
      "Marketing Prediction: The main advantages of the newly introduced language model, as compared to its predecessors, include: \n",
      "\n",
      "- Increased effectiveness and performance in language modeling tasks, as indicated by the research paper's results. \n",
      "- The ability to maintain efficiency while delivering high performance, suggesting optimizations in the model's design. \n",
      "- The potential to make language models more helpful, as the techniques used seem to promise improved usefulness. \n",
      "\n",
      "Regarding attention mechanisms, the paper likely employs some form of attention mechanism, a key component in many modern language models, to enable the model to focus on relevant parts of the input and improve performance. However, without the full paper, it is challenging to determine the exact attention mechanism used. \n",
      "\n",
      "Overall, the newly introduced language model appears to offer a balance between effectiveness and efficiency, with careful design choices contributing to its improved performance over predecessor models.\n",
      "Marketing Answer: The main advantages of the newly introduced language model include the use of retrieval-augmented mechanisms, attention mechanisms, and context representation learning, which contribute to enhanced performance and efficiency compared to its predecessors.\n",
      "------------------------------------------------------------\n",
      "Question: What criteria are used to assess the quality of recommendations provided by different language models in a comparison study?\n",
      " \n",
      "Engineering Prediction: The criteria used to assess the quality of recommendations provided by different language models in a comparison study can vary, but in the context provided, it seems that the primary evaluation method is human evaluation. \n",
      "\n",
      "Human evaluation, or human rating, is a common approach to assess the quality of language model outputs, especially in natural language processing (NLP) tasks. In this case, \"labelers\" are tasked with rating the model outputs based on certain criteria, which may include: \n",
      "1. **Coherence and Relevance**: This measures how well the model's recommendations stay on topic and provide relevant and coherent responses in the context of the task. \n",
      "2. **Grammatical Correctness**: Assessing if the model's outputs follow the rules of grammar and syntax, contributing to the overall fluency and readability of the text. \n",
      "3. **Contextual Accuracy**: Evaluating if the model's recommendations align with the context and intent of the input data, demonstrating its understanding of the task. \n",
      "4. **Diversity and Creativity**: In some cases, labelers might assess if the model provides diverse and creative responses, especially in generative tasks like story completion or dialogue generation. \n",
      "5. **Factual Accuracy**: For tasks involving factual information, labelers might check if the model's recommendations are factually correct and not misleading. \n",
      "6. **Task-Specific Metrics**: Depending on the specific task, there could be additional metrics. For example, in machine translation tasks, labelers might evaluate the accuracy of translations. \n",
      "\n",
      "These criteria help to provide a holistic evaluation of language models, taking into account not only the correctness of the outputs but also their overall quality and usefulness in real-world applications.\n",
      "Engineering Answer: In a comparison study of language models, criteria such as sentence relevance, lexical accuracy, and contextual understanding are used to assess the quality of recommendations. Different tasks may benefit from different evaluation measures, such as STRINC, LEXICAL, and CXMI. Additionally, template selection plays a vital role in the quality of recommendations, with deliberate template design being important for tasks like query suggestion. The overall quality of recommendations is often judged using a Likert scale, along with metadata collection for each model output.\n",
      "\n",
      "Marketing Prediction: The criteria used to assess the quality of recommendations provided by different language models in a comparison study can vary, but in the context provided, it seems that the primary evaluation method is human evaluation. \n",
      "\n",
      "Human evaluation, or human rating, is a common approach to assessing the quality of language model outputs. In this case, \"labelers\" are given the task of rating the model outputs based on certain criteria, which may include: \n",
      "1. **Coherence and Relevance**: This measures how well the model's response stays on topic and provides relevant and coherent information. \n",
      "2. **Grammatical Correctness**: Assessing if the model's output follows the rules of grammar and syntax, and whether it is free of errors. \n",
      "3. **Context Awareness**: Determining if the model's response takes into account the provided context and generates an appropriate and contextually relevant response. \n",
      "4. **Informativeness and Specificity**: Evaluating if the model provides interesting, informative, and specific responses, rather than generic or vague answers. \n",
      "5. **Fluency and Readability**: Assessing how easy it is for a human to read and understand the model's output, and whether it flows naturally. \n",
      "6. **Task Completion**: If the model is designed for a specific task (e.g., question answering, sentiment analysis), the labelers might evaluate if the model successfully accomplishes that task. \n",
      "\n",
      "These criteria are not exhaustive, and different studies may focus on slightly different aspects depending on the specific language models being compared and the goals of the research. Additionally, automatic evaluation metrics, such as those based on n-gram overlap (e.g., BLEU, ROUGE) or embedding-based similarity measures (e.g., BERTScore), may also be used as supplementary metrics. \n",
      "\n",
      "The specific paper referenced in the context, \"When not to trust language models: Investigating effectiveness of parametric and non-parametric holistic evaluation of language models,\" likely provides further details on the exact evaluation methodology and criteria used in their study.\n",
      "Marketing Answer: The criteria used to assess the quality of recommendations provided by different language models in a comparison study include comparing to human-created benchmarks, examining intrinsic character, comparing two models, investigating rate of learning, and analyzing learning curves.\n",
      "------------------------------------------------------------\n",
      "Question: What approaches have been proposed to enhance the task performance of language models while considering the trade-offs such as runtime efficiency, robustness to irrelevant context, and attribution quality?\n",
      " \n",
      "Engineering Prediction: The context provided suggests that one approach to enhance the task performance of language models while considering trade-offs is careful design and fine-tuning. This includes iterative language feedback and refined task output, which can lead to strong performance across discriminative and generative tasks. However, this approach also mentions a trade-off with inference efficiency, implying that there is a cost to the runtime efficiency of the model. \n",
      "\n",
      "Additionally, the paper referenced in the context, \"Effectiveness of Language Model Fine-Tuning,\" may propose or discuss other approaches and techniques to improve task performance while considering trade-offs. Unfortunately, without the full text of the paper, I cannot provide more specific details or summarize its content.\n",
      "Engineering Answer: Several approaches have been proposed to enhance the task performance of language models while considering trade-offs. These include using compression and selective augmentation methods to decrease the propensity of models to generate toxic or biased outputs. Adversarial setups have been suggested where labelers find worst-case behaviors of the model and add them to the dataset. Additionally, models like BART and T5 leverage bi-directional attention to achieve stronger performance on both discriminative and generative tasks. These methods aim to balance model performance with considerations such as runtime efficiency, robustness to irrelevant context, and attribution quality.\n",
      "\n",
      "Marketing Prediction: The context provided suggests that one approach to enhance the task performance of language models while considering trade-offs is careful design and fine-tuning. This includes iterative language feedback and refined task output, which can lead to strong performance across discriminative and generative tasks. However, this approach also mentions a trade-off with inference efficiency, implying that there is a cost to the runtime efficiency of the model. \n",
      "\n",
      "Additionally, the paper referenced in the context, \"Effectiveness of Language Model Fine-Tuning,\" may propose or discuss other approaches and techniques to improve task performance while considering trade-offs. Unfortunately, without the full text of the paper, I cannot provide more specific details or summarize its content.\n",
      "Marketing Answer: Approaches proposed to enhance language model task performance include compression and selective augmentation, adversarial set-ups for labeling worst-case behaviors, retrieval-augmented models, and extending existing models to enable length extrapolation while maintaining quality.\n",
      "------------------------------------------------------------\n",
      "Question: What metrics are commonly used to compare the performance of language models in various tasks, as outlined in an experimental results table?\n",
      " \n",
      "Engineering Prediction: The most common metrics used to compare the performance of language models across various tasks are accuracy, precision, recall, and F1-score. These metrics provide a quantitative measure of how well a language model performs on a specific task, such as natural language understanding (NLU) or generation. \n",
      "\n",
      "Accuracy measures the proportion of correct predictions made by the model out of all the test samples. Precision calculates the proportion of true positive predictions among all positive predictions, indicating the model's ability to avoid false positives. Recall, on the other hand, measures the proportion of true positive predictions out of all actual positive samples, helping to identify false negatives. \n",
      "\n",
      "The F1-score is the harmonic mean of precision and recall, providing a single metric that balances both concerns. It is often used when there is an imbalance in the class distribution, as it gives more weight to the class with fewer samples. \n",
      "\n",
      "These metrics are typically reported in experimental results tables, allowing for a direct comparison of different language models' performance on the same task or dataset. They provide a quantitative and standardized way to evaluate and improve language models.\n",
      "Engineering Answer: Common metrics used to compare the performance of language models in various tasks, as outlined in an experimental results table, include Exact Match and Unigram F1. These metrics have become standard in evaluating language models. Additionally, other metrics such as BLEU score, FactScore (factuality), precision, and recall are also commonly used to assess the performance of language models across different tasks. It is important to consider a variety of metrics to get a comprehensive understanding of the effectiveness of a language model in different contexts.\n",
      "\n",
      "Marketing Prediction: The most common metrics used to evaluate and compare the performance of language models across various tasks are accuracy, precision, recall, and F1-score. These metrics provide insights into different aspects of a model's performance and are widely adopted in the natural language processing (NLP) community. \n",
      "\n",
      "- Accuracy measures the proportion of correct predictions out of all predictions made by the model. It is useful when the classes are balanced, giving an overall sense of how well the model performs. \n",
      "\n",
      "- Precision calculates the proportion of true positive predictions among all positive predictions. It indicates how often the model is correct when it predicts a positive class. High precision suggests low false-positive rates. \n",
      "\n",
      "- Recall (or sensitivity) measures the proportion of true positive predictions among all actual positive instances. It tells us how well the model identifies positive instances. High recall means the model captures most positive instances, resulting in a low false-negative rate. \n",
      "\n",
      "- F1-score is the harmonic mean of precision and recall, providing a single metric that balances both concerns. It is useful when you want to consider both false positives and false negatives equally. \n",
      "\n",
      "These metrics are typically reported in experimental results tables, allowing for a direct comparison of different language models' performance on specific tasks, such as text classification, named entity recognition, or question answering. Each metric offers a different perspective on the model's performance, and collectively they provide a comprehensive evaluation framework.\n",
      "Marketing Answer: The metrics commonly used to compare the performance of language models in various tasks are Exact Match and Unigram F1.\n",
      "------------------------------------------------------------\n",
      "Question: What is the role of manual assessment in the validation of language model predictions according to the text provided?\n",
      " \n",
      "Engineering Prediction: The text suggests that manual assessment, or human evaluation, plays a crucial role in validating the language model's predictions and capabilities. Specifically, human evaluations are mentioned as a way to test the model's generative language capabilities and its ability to respond to instructions and complete coding tasks in different languages. This manual assessment provides a qualitative layer of analysis that complements the automated evaluations conducted on public NLP datasets. Human feedback, as mentioned in the context, is an essential part of training language models to follow instructions accurately.\n",
      "Engineering Answer: Manual assessment plays a crucial role in the validation of language model predictions. The engineers evaluate the quality of model outputs by having labelers rate them on test sets consisting of prompts from held-out customers. This manual assessment helps ensure that the models are aligned with a broad distribution of language tasks and can identify any behavioral issues that may arise from misalignment. Additionally, human annotators find that certain reflection token predictions are aligned with their assessments, providing valuable insights into the accuracy and effectiveness of the models.\n",
      "\n",
      "Marketing Prediction: The text suggests that manual assessment, or human evaluation, plays a crucial role in validating the language model's predictions and capabilities. Specifically, human evaluations are mentioned as a way to test the model's generative language capabilities and its ability to respond to instructions and complete coding tasks in different languages. This manual assessment provides a qualitative layer of analysis that complements the automatic evaluations conducted on public NLP datasets. By involving human evaluators, the model's performance can be assessed from a user-centric perspective, taking into account factors such as coherence, relevance, and context understanding, which may not be fully captured by automated metrics alone.\n",
      "Marketing Answer: Answer: Manual assessment plays a key role in evaluating the quality of language model predictions by having labelers rate the model outputs and comparing them to prompts from held-out customers.\n",
      "------------------------------------------------------------\n",
      "Question: What are the general steps outlined for training a language model in the document, and how is the training data for the generator language model collected and utilized?\n",
      " \n",
      "Engineering Prediction: The general steps outlined for training a language model in the document are as follows: \n",
      "\n",
      "- The language model is pre-trained on a large corpus of text data to learn the basic patterns and structures of human language. \n",
      "- The model is then fine-tuned using a smaller set of labeled instruction data to teach it to follow specific instructions. \n",
      "- A wide range of instructions is incorporated to make the model versatile and adaptable to different tasks. \n",
      "\n",
      "Regarding the training data for the generator language model: \n",
      "\n",
      "- The training data for the generator language model is collected from a variety of sources, including human-written instructions, existing language models, and other relevant documents. \n",
      "- The data is preprocessed to create a large dataset of input-output examples, where each input is an instruction, and the output is the desired response or action. \n",
      "- This dataset is then used to train the generator model to produce responses or actions based on the given instructions. \n",
      "- The training process involves optimizing the model's parameters to minimize the difference between the generated responses and the desired outputs in the training data. \n",
      "\n",
      "The utilization of the training data is as follows: \n",
      "\n",
      "- The collected training data is used to teach the language model to understand and generate responses to instructions. \n",
      "- During training, the model learns to associate specific instructions with the appropriate responses or actions by minimizing the discrepancy between generated responses and the true outputs in the training data. \n",
      "- The model's performance is evaluated on a separate validation set to ensure it generalizes well to unseen instructions. \n",
      "- Techniques like data augmentation and fine-tuning are applied to enhance the model's ability to understand and generate diverse responses. \n",
      "\n",
      "In summary, the document outlines a pre-training and fine-tuning approach for training a language model to follow instructions. The training data for the generator language model is collected from various sources, preprocessed, and utilized to optimize the model's performance in generating responses to instructions.\n",
      "Engineering Answer: The document outlines the general steps for training a language model, including incorporating retrieved documents into the main input sequence and optimizing the loss function to train the generator. The training data for the generator language model is collected through various techniques such as supervised fine-tuning, critic learning, and custom retrievers for downstream tasks. The collected data is used to train the generator on specific tasks like summarization, machine reading comprehension, and natural language to SQL translation, improving performance on those tasks.\n",
      "\n",
      "Marketing Prediction: The general steps outlined for training a language model in the document are as follows: \n",
      "\n",
      "- The language model is pre-trained on a large corpus of text data to learn the basic patterns and structure of human language. This step is crucial for the language model to acquire a broad understanding of language and generate coherent responses. \n",
      "\n",
      "- The model is then fine-tuned using a smaller set of labeled training data specific to the task at hand, such as following instructions or answering questions. During fine-tuning, the model learns to associate the training examples with the correct output or response. \n",
      "\n",
      "- Various instructions or prompts are fed as input to the language model, and the corresponding desired outputs or responses are provided as training data. This step involves annotating the training data with the correct responses, which helps the model learn the relationship between the input instructions and the expected output. \n",
      "\n",
      "- The training data is used to optimize the language model's parameters through techniques like backpropagation and gradient descent. The model adjusts its internal weights to minimize the difference between the predicted output and the actual training data responses. \n",
      "\n",
      "- Evaluating the language model's performance on a separate validation set helps to fine-tune the model's hyperparameters and prevent overfitting. \n",
      "\n",
      "- Finally, the trained language model is tested on a new set of instructions or deployed in a real-world setting to generate responses based on the provided input. \n",
      "\n",
      "Regarding the training data for the generator language model: \n",
      "\n",
      "- The training data for the generator language model is collected by first identifying a set of instructions or prompts that cover a diverse range of tasks and scenarios. These instructions are designed to test the language model's ability to generalize and follow directions. \n",
      "\n",
      "- Human annotators are then tasked with providing the desired outputs or responses for each of the instructions. These responses serve as the correct answers that the language model should ideally generate given the corresponding input instructions. \n",
      "\n",
      "- The document mentions \"documents for each input,\" implying that the training data is in the form of text documents or examples. These documents likely consist of pairs of input instructions and the corresponding desired output, forming a dataset used for training and testing the language model. \n",
      "\n",
      "- The training data is utilized to optimize the language model's performance. The model learns to predict the correct output given an input instruction by adjusting its internal parameters based on the provided examples. This process involves feeding the training data into the model, calculating the loss or error between the predicted and actual responses, and then updating the model's weights to minimize this error. \n",
      "\n",
      "- Techniques like regularization and early stopping, as mentioned in the document, are employed to prevent overfitting and ensure that the language model generalizes well to unseen data. \n",
      "\n",
      "Overall, the training process for the language model involves pre-training, fine-tuning with task-specific data, optimizing the model's parameters using the provided training data, and employing techniques to improve the model's performance and generalization abilities. The training data for the generator language model is collected through human annotation, providing a diverse set of instructions and corresponding desired outputs, which are then utilized to teach the model how to generate appropriate responses.\n",
      "Marketing Answer: The general steps for training a language model include fine-tuning on specific datasets, filtering pretraining data, and using critic learning. Training data for the generator language model is collected from open-access NLP papers and used for downstream conditional text generation tasks.\n",
      "------------------------------------------------------------\n",
      "Question: What are the three main categories used to refine language model abilities in understanding and executing search tasks according to the given document?\n",
      " \n",
      "Engineering Prediction: The three main categories used to refine language model abilities in understanding and executing search tasks are:\n",
      "\n",
      "1. Query Understanding\n",
      "2. Document Understanding\n",
      "3. Coding Tasks\n",
      "Engineering Answer: The three main categories used to refine language model abilities in understanding and executing search tasks are query understanding, document understanding, and query-document relationship understanding. Tasks within these categories focus on interpreting queries, comprehending documents, and understanding the relationships between queries and documents. This approach aims to enhance the models' performance in interpreting and responding to search-related instructions effectively, improving their utility in complex information retrieval scenarios.\n",
      "\n",
      "Marketing Prediction: The three main categories used to refine language model abilities in understanding and executing search tasks are: \n",
      "\n",
      "1. Query Understanding \n",
      "2. Document Understanding \n",
      "3. Coding Tasks and Instructions \n",
      "\n",
      "These categories help to improve the model's performance and ability to interpret and respond to a diverse range of instructions and queries.\n",
      "Marketing Answer: The three main categories used to refine language model abilities in understanding and executing search tasks are query understanding, document understanding, and query-document relationship understanding.\n",
      "------------------------------------------------------------\n",
      "Question: What are some of the emerging research topics and challenges in the field of natural language processing and information retrieval according to recent academic conferences and publications?\n",
      " \n",
      "Engineering Prediction: Some of the emerging research topics and challenges in the field of Natural Language Processing (NLP) and Information Retrieval (IR) according to recent academic conferences and publications include: \n",
      "\n",
      "- **Neural Information Processing**: This field focuses on the use of neural networks and deep learning techniques for natural language understanding and generation. The Advances in Neural Information Processing Systems conference is a prime example of this, covering topics like language modeling, machine translation, and text generation. \n",
      "\n",
      "- **Language Representation and Embeddings**: Developing effective and context-aware language representations and embeddings is an active area of research. Contextual word embeddings like Word2Vec, GloVe, and more recently, transformer-based models like BERT and GPT have revolutionized NLP tasks.\n",
      "\n",
      "- **Pre-training and Transfer Learning**: Pre-training large language models on massive datasets and then fine-tuning them for specific tasks has led to state-of-the-art results. Transfer learning techniques enable the application of pre-trained models to a wide range of NLP tasks with limited task-specific data. \n",
      "\n",
      "- **Multilingual and Multimodal NLP**: There is a growing interest in developing NLP systems that can understand and generate text in multiple languages, and also process multimodal inputs like text, images, and videos together. \n",
      "\n",
      "- **Question Answering and Dialogue Systems**: Building intelligent systems that can engage in natural language conversations and answer complex questions is a significant challenge. This involves knowledge representation, reasoning, and context maintenance. \n",
      "\n",
      "- **Ethics and Bias**: With the increasing impact of NLP systems on society, there is a focus on addressing ethical concerns and bias in data and models. Fairness, transparency, and accountability are essential aspects of this research direction. \n",
      "\n",
      "- **Efficiency and Interpretability**: As NLP models become more complex, there is a need to improve their efficiency in terms of computational resources and inference speed. Interpretability methods aim to explain the decisions made by these complex models, making them more trustworthy. \n",
      "\n",
      "- **Domain-specific NLP**: Adapting NLP techniques to specific domains like healthcare, finance, or legal requires handling domain-specific language, terminology, and privacy concerns. \n",
      "\n",
      "These topics highlight the dynamic nature of the field, with continuous advancements in both theoretical understanding and practical applications of NLP and IR.\n",
      "Engineering Answer: Recent academic conferences and publications have highlighted emerging research topics and challenges in natural language processing and information retrieval. Some key areas of focus include efficient retrieval augmented generation, unsupervised dense information retrieval with contrastive learning, citation-informed transformers, and knowledge refinement via interaction between search engines and large language models. Additionally, challenges such as zero-shot retrieval, semantic search using GPT sentence embeddings, and prompt-based effective input reformulation for legal case retrieval have been identified as important research directions. These topics reflect the ongoing advancements and complexities in the field, driving innovation and progress in NLP and IR research.\n",
      "\n",
      "Marketing Prediction: Some of the emerging research topics and challenges in the field of Natural Language Processing (NLP) and Information Retrieval (IR) according to recent academic conferences and publications include:\n",
      "\n",
      "- **Neural Information Processing**: This involves the use of machine learning and deep learning techniques to process and understand natural language. The challenge lies in developing models that can effectively learn and represent complex linguistic data.\n",
      "\n",
      "- **Language Understanding**: There is a focus on improving the ability of models to understand and interpret human language, including tasks such as sentiment analysis, named entity recognition, and question-answering. The challenge is to build systems that can comprehend language in a way that is contextually and semantically accurate.\n",
      "\n",
      "- **Multilingual and Multimodal NLP**: With the diverse nature of languages and media, there is a growing interest in developing NLP systems that can process and understand multiple languages and modalities (e.g., text, speech, images). The challenge is to create models that are adaptable and effective across different linguistic and modal variations.\n",
      "\n",
      "- **Information Retrieval and Extraction**: The main focus is on improving the effectiveness and efficiency of information retrieval systems, particularly in large-scale and unstructured datasets. Challenges include developing techniques for accurate entity linking, relation extraction, and knowledge graph construction.\n",
      "\n",
      "- **Ethical and Bias Considerations**: As NLP and IR systems become more prevalent, there is an increasing emphasis on addressing ethical concerns and bias in data and models. The challenge is to develop fair and transparent practices to ensure that these systems do not perpetuate or amplify existing biases and discriminate against certain groups.\n",
      "\n",
      "- **Interpretability and Explainability**: There is a growing demand for interpretable and explainable NLP and IR models, particularly in sensitive applications such as healthcare and legal domains. The challenge is to design models that are not only accurate but also provide transparent and understandable justifications for their predictions.\n",
      "\n",
      "- **Few-shot and Zero-shot Learning**: These techniques aim to improve the ability of models to generalize to new tasks or domains with limited or no training data. The challenge is to develop methods that can effectively adapt and learn from small datasets, especially in specialized or niche areas.\n",
      "\n",
      "These emerging topics and challenges highlight the dynamic nature of the NLP and IR fields, which continuously strive to improve the understanding, processing, and retrieval of human language information.\n",
      "Marketing Answer: Some emerging research topics and challenges in the field of natural language processing and information retrieval include efficient generation from unstructured knowledge, semantic code search evaluation, unsupervised dense information retrieval, context-aware document term weighting, knowledge refinement through interaction with large language models, and investigating the effectiveness of large language models in search re-ranking.\n",
      "------------------------------------------------------------\n",
      "Question: Question: How do models with different fine-tuning strategies compare in terms of accuracy and F1 score for fact verification tasks?\n",
      " \n",
      "Engineering Prediction: The context provided seems to be discussing the fine-tuning of language models and their performance on fact-verification tasks. Let's extract the relevant information to answer the question at hand:\n",
      "\n",
      "- Instruction-tuned models generate plausible answers, but these may not always be factually accurate.\n",
      "- There is a trade-off between efficiency and model quality when comparing fine-tuning strategies.\n",
      "- Direct fine-tuning methods have shown regression compared to fine-tuning baselines.\n",
      "\n",
      "Based on this context, it seems that the fine-tuning strategy significantly impacts the accuracy and F1 score of models on fact-verification tasks. However, the context does not provide specific numerical values or direct comparisons between different fine-tuning strategies regarding their accuracy and F1 scores. \n",
      "\n",
      "To fully answer your question, additional information or data from the experiments conducted would be needed, such as the specific models used, the fine-tuning techniques employed, and the resulting accuracy and F1 scores for each strategy.\n",
      "Engineering Answer: Models with different fine-tuning strategies are compared in terms of accuracy and F1 score for fact verification tasks. The introduction of LLMs has led to notable developments, with some studies leveraging prompting methods to apply LLMs in IR tasks. However, not all LLMs consistently outperform fine-tuned smaller models. For example, RankGPT based on gpt-3.5-turbo underperforms monoBERT in certain scenarios. Fine-tuning is not strictly necessary for models like GPT3, which has been evaluated on closed book question answering tasks without any updates or fine-tuning.\n",
      "\n",
      "Marketing Prediction: The context provided seems to be discussing the fine-tuning of language models and their performance on fact-verification tasks. Let's extract the relevant information to answer the question:\n",
      "\n",
      "- Instruction-tuned models generate plausible answers, but their responses may not be factually grounded.\n",
      "- There is a concern about the trade-off between efficiency and model quality when fine-tuning baselines are used.\n",
      "- Direct fine-tuning methods have shown regression in performance, implying that they may not match the fine-tuning baseline models.\n",
      "\n",
      "Based on this context, it appears that the fine-tuning strategy can significantly impact the accuracy and F1 score of models on fact-verification tasks. However, the specific details of the comparison are not provided in the context. \n",
      "\n",
      "To fully answer the question, more information is needed, such as the specific models used, the fine-tuning strategies employed, and the resulting accuracy and F1 scores. Without these details, we cannot make a comprehensive comparison.\n",
      "Marketing Answer: Models with different fine-tuning strategies have shown mixed results in terms of accuracy and F1 score for fact verification tasks. Some studies have found that large language models (LLMs) outperform smaller fine-tuned models, while others have reported inconsistent performance. Factors such as task complexity and the need for prompt methods to apply LLMs in information retrieval tasks can also impact the comparison.\n",
      "------------------------------------------------------------\n",
      "Question: What components does a fact verification task typically involve in order to assess the accuracy of a given statement?\n",
      " \n",
      "Engineering Prediction: A fact verification task typically involves the following components: \n",
      "\n",
      "- **Claim**: This is the statement or assertion that needs to be verified. It is often a declarative sentence or proposition that needs to be evaluated for its truthfulness. \n",
      "\n",
      "- **Evidence**: This refers to the supporting information or reference materials used to assess the claim. Evidence can come from various sources, such as documents, witness testimonies, statistical data, scientific studies, or other relevant facts and information. \n",
      "\n",
      "- **Reasoning**: This involves the process of evaluating the claim against the evidence. It requires critical thinking skills to analyze, interpret, and synthesize information, identify logical relationships, and draw conclusions. The reasoning component may also involve identifying potential biases, inconsistencies, or fallacies in the claim or evidence. \n",
      "\n",
      "- **Verdict**: Based on the evaluation and reasoning process, a verdict is reached to determine whether the claim is supported, refuted, or partially supported by the evidence. The verdict provides a conclusion about the accuracy and validity of the claim. \n",
      "\n",
      "These components work together to assess the accuracy of a given statement, ensuring that claims are backed by reliable evidence and sound reasoning.\n",
      "Engineering Answer: A fact verification task typically involves assessing the relationship between a claim and the evidence provided, analyzing if there is enough information for a conclusive judgment. This task requires a detailed understanding of the claim and evidence to determine if it is supported or refuted. The use of performance metrics based on including gold answers in model generations instead of exact matching can help search engines deliver accurate and relevant results. Additionally, incorporating lexical measures and verification functions can aid in determining the accuracy of statements.\n",
      "\n",
      "Marketing Prediction: A fact verification task typically involves the following components: \n",
      "\n",
      "- **Claim**: This is the statement or assertion that needs to be verified. It is often a declarative sentence or proposition that needs to be evaluated for its truthfulness. \n",
      "\n",
      "- **Evidence**: This refers to the supporting information or reference material used to assess the claim. Evidence can come from various sources, such as documents, witness testimonies, statistical data, scientific studies, or other relevant facts and information. \n",
      "\n",
      "- **Reasoning**: This involves the process of evaluating the claim against the evidence. It requires critical thinking skills to analyze, interpret, and synthesize information, identify logical relationships, and draw conclusions. The reasoning component may also involve identifying potential biases, inconsistencies, or fallacies in the claim or evidence. \n",
      "\n",
      "- **Verdict**: Based on the evaluation and reasoning process, a verdict is reached to determine the accuracy of the claim. Typical verdicts include \"Supported,\" \"Refuted,\" \"Partially Supported,\" \"Unverified,\" or \"False.\" The verdict provides a conclusion about the validity or truth value of the claim based on the available evidence. \n",
      "\n",
      "These components work together to assess the accuracy of a given statement, ensuring that claims are supported by reliable evidence and sound reasoning.\n",
      "Marketing Answer: A fact verification task typically involves assessing the relationship between a claim and supporting evidence to determine accuracy.\n",
      "------------------------------------------------------------\n",
      "Question: What are the key factors that determine the performance of HALO-aligned models compared to non-HALO models, according to the results presented in the analysis?\n",
      " \n",
      "Engineering Prediction: The key factor contributing to the superior performance of HALO-aligned models compared to their non-HALO counterparts is their ability to handle diverse function classes effectively. The HALO-aligned models, specifically DPO and the offline PPO variant, have demonstrated their prowess by either matching or surpassing the performance of non-HALO models across all scales. This consistency in performance highlights their adaptability and robustness in handling various tasks and datasets.\n",
      "Engineering Answer: According to the analysis presented, the key factors that determine the performance of HALO-aligned models compared to non-HALO models include the specific alignment method used (such as DPO and PPO variant), the model size (significant gap at 13B+ model sizes), and the ability to match or exceed the generation quality of SFT target sequences. Additionally, the study suggests that the cost of increasing model alignment is modest relative to pretraining, and that the modeling of human biases in HALOs may have practical benefits in improving overall performance.\n",
      "\n",
      "Marketing Prediction: The key factor contributing to the superior performance of HALO-aligned models compared to their non-HALO counterparts is their ability to handle diverse function classes effectively. The HALO-aligned models, specifically DPO and the offline PPO variant, have demonstrated consistent matching or outperformance when compared to non-HALO models across all scales. This suggests that the HALO-alignment technique plays a crucial role in enhancing the models' capabilities and adaptability to different tasks and datasets.\n",
      "Marketing Answer: The key factor that determines the performance of HALO-aligned models compared to non-HALO models is the model size, with HALO-aligned models generally outperforming non-HALO models at larger sizes (13B+ model sizes).\n",
      "------------------------------------------------------------\n",
      "Question: How does the performance of KTO compare to DPO in model alignment, and what are the potential implications for data usage and training efficiency?\n",
      " \n",
      "Engineering Prediction: Answer: KTO matches or exceeds the performance of DPO in model alignment, which is surprising given that it learns from a weaker signal. This suggests that KTO is more data-efficient and can achieve better results with less data. This has potential implications for reducing the computational resources and costs associated with training large language models.\n",
      "Engineering Answer: Based on the provided data and experiments, KTO consistently outperforms DPO in model alignment, even with restrictions such as using only one output per input. This suggests that KTO can achieve higher win rates and improve performance across various benchmarks compared to DPO. The implications of this performance difference include the ability to achieve quality generation results with significantly fewer desirable examples, potentially leading to more efficient data usage and training processes. This indicates that KTO may offer a more efficient and effective approach to model alignment compared to DPO.\n",
      "\n",
      "Marketing Prediction: Answer: KTO outperforms DPO in model alignment, which is surprising given that it learns from a weaker signal. This suggests that KTO is more data-efficient and can achieve better results with less data. This has potential implications for reducing the computational resources and costs associated with training large language models.\n",
      "Marketing Answer: KTO outperforms DPO in model alignment with up to 90% fewer examples. This suggests that KTO can achieve high performance even with imbalanced data, potentially leading to more efficient training processes.\n",
      "------------------------------------------------------------\n",
      "Question: What are some common approaches to building an open-domain question answering system?\n",
      " \n",
      "Engineering Prediction: There are several approaches to building an open-domain question-answering system. One common method is to first retrieve relevant information from a large knowledge base, such as a corpus of text documents or a structured database. This retrieval step can be done using keyword searches, natural language processing techniques, or more advanced methods like semantic search or latent vector models. Once relevant information is retrieved, the system then needs to process and understand the content to extract the answer. This could involve techniques such as named entity recognition, relation extraction, or more complex methods like machine reading comprehension models. The system then generates a response, which could be in the form of a text answer, a summary, or even a generated language response. \n",
      "\n",
      "Some other approaches include: \n",
      "1. Closed-book QA: This method does not rely on external knowledge sources but instead uses pre-trained language models that have learned knowledge from large text corpora. During inference, the model generates answers based on its internal knowledge without additional retrieval steps. \n",
      "2. Hybrid approaches: These combine the above methods, using both external knowledge bases and pre-trained language models to enhance the accuracy and robustness of the system. \n",
      "3. Multi-hop reasoning: This approach involves multiple retrieval and inference steps, where the system iteratively gathers evidence and reasons over multiple pieces of information to arrive at an answer. \n",
      "\n",
      "Each approach has its own advantages and limitations, and the choice of method depends on factors such as the availability and size of knowledge bases, the complexity of questions to be answered, and the desired level of accuracy and explainability.\n",
      "Engineering Answer: Some common approaches to building an open-domain question answering system include using the RAG model, which minimizes the negative log-likelihood of answers, and comparing it to extractive QA paradigms that rely on non-parametric knowledge retrieval. Another approach is to incorporate question rewriting techniques to make open-domain QA more conversational. Additionally, utilizing datasets like QASPER, which contain questions requiring complex reasoning, can improve the performance of the system. References to papers by Anantha et al. and Asai et al. provide further insights into building ODQA systems.\n",
      "\n",
      "Marketing Prediction: There are several approaches to building an open-domain question-answering system. One common method is to employ a retrieve-and-read mechanism. This involves first retrieving relevant documents or passages from a large corpus or knowledge base using keyword searches or more advanced techniques like dense passage retrieval. Once relevant information is retrieved, a machine reading component processes the text to extract the answer. This can be done through rule-based approaches or utilizing more advanced techniques like machine learning and natural language processing. \n",
      "\n",
      "Another approach is to use generative models, such as large language models (LLMs), that can provide answers based on their vast knowledge trained from large text corpora. These models can generate responses to questions without the need for explicit retrieval, as the knowledge is implicitly stored in their parameters. However, these models often require fine-tuning or prompt engineering to perform well on specific question-answering tasks. \n",
      "\n",
      "Additionally, hybrid systems that combine retrieval and generative models have also been proposed. These systems aim to leverage the strengths of both approaches by using retrieval to provide supporting evidence and generative models to synthesize and formulate the final answer. \n",
      "\n",
      "It's important to note that building an effective open-domain question-answering system often involves a combination of techniques, including information retrieval, natural language understanding, and knowledge representation, depending on the specific requirements and constraints of the task at hand.\n",
      "Marketing Answer: Common approaches to building an open-domain question answering system include using retrieval over a knowledge base and incorporating the retrieved content as part of the prompt. Other methods involve pretraining models on large amounts of text data and fine-tuning them for question answering tasks.\n",
      "------------------------------------------------------------\n",
      "Question: What is the difference between open-book and closed-book question answering?\n",
      " \n",
      "Engineering Prediction: In the context of question-answering systems, \"open-book\" and \"closed-book\" refer to whether or not the system has access to additional context or information to answer a question.\n",
      "\n",
      "- Open-book question answering: The system has access to external sources of information, such as documents or databases, to retrieve and provide answers. It can leverage non-parametric knowledge, meaning it can draw on a vast amount of information to answer questions. This is similar to an open-book exam, where students are allowed to refer to their notes or textbooks during the exam.\n",
      "\n",
      "- Closed-book question answering: The system, on the other hand, relies solely on its inherent knowledge to generate answers. It does not have access to external context or information. This is akin to a closed-book exam, where students are expected to answer questions based on their memory and understanding without referring to any external sources.\n",
      "\n",
      "In summary, the key difference lies in the availability of external information sources to provide answers. Open-book question answering leverages external knowledge sources, while closed-book question answering relies exclusively on the system's internal knowledge.\n",
      "Engineering Answer: Open-book question answering involves the use of external sources of knowledge, such as Wikipedia, to retrieve information and generate a response. In contrast, closed-book question answering relies on pre-trained language models that have memorized factual knowledge within their parameters to generate responses without explicit context. Closed-book QA can be seen as analogous to a closed-book exam where no external resources are allowed. The key distinction lies in the reliance on external knowledge sources for open-book QA versus internal memorized knowledge for closed-book QA.\n",
      "\n",
      "Marketing Prediction: In the context of question-answering systems, \"open-book\" and \"closed-book\" refer to whether or not the system has access to additional context or information to answer a question.\n",
      "\n",
      "- Open-book question answering: The system has access to external sources of information, such as documents or databases, to retrieve and provide answers. It can leverage non-parametric knowledge, meaning it can draw on a vast amount of information to answer questions. This is similar to an open-book exam, where students are allowed to refer to their notes or textbooks during the exam.\n",
      "\n",
      "- Closed-book question answering: The system, on the other hand, relies solely on its inherent knowledge to generate answers. It does not have access to external context or information. This is akin to a closed-book exam, where students are expected to answer questions based on their memorized knowledge without referring to any external sources.\n",
      "\n",
      "In summary, the key difference lies in the availability of external information sources to provide answers. Open-book question answering leverages external knowledge sources, while closed-book question answering relies exclusively on the inherent knowledge of the system.\n",
      "Marketing Answer: Open-book question answering involves using external sources of knowledge to answer questions, while closed-book question answering relies on pre-trained language models to provide answers without explicit context.\n",
      "------------------------------------------------------------\n",
      "Question: What are the basic components of the Retriever-Reader framework in open-domain QA?\n",
      " \n",
      "Engineering Prediction: The basic components of the Retriever-Reader framework in open-domain QA are the retriever and the reader. The retriever is responsible for retrieving relevant documents or passages from a large corpus, while the reader is a machine reading comprehension model that extracts the answer to the question from the retrieved context.\n",
      "Engineering Answer: The basic components of the Retriever-Reader framework in open-domain QA include a retriever model, which fetches relevant information based on input prompts efficiently using FAISS. The retriever component is responsible for retrieving contextually relevant documents or evidence blocks based on the input question. The reader component then processes this retrieved information to generate answers to the questions posed. This framework combines information retrieval and machine reading comprehension to achieve state-of-the-art results in open-domain question answering tasks.\n",
      "\n",
      "Marketing Prediction: The basic components of the Retriever-Reader framework in open-domain QA are the retriever and the reader. The retriever is responsible for retrieving relevant documents or passages from a large corpus, while the reader is a machine reading comprehension model that extracts the answer to the question from the retrieved context.\n",
      "Marketing Answer: The basic components of the Retriever-Reader framework in open-domain QA are the retriever and the reader components, which can be set up and trained independently or jointly trained end-to-end. The retriever component automatically fetches relevant information based on input prompts, while the reader component processes and comprehends the retrieved information to answer questions.\n",
      "------------------------------------------------------------\n",
      "Question: How is the TF-IDF model used in question answering retrieval systems?\n",
      " \n",
      "Engineering Prediction: The TF-IDF model is a classic and effective approach used in question-answering retrieval systems to rank and retrieve relevant text segments. In the given context, TF-IDF, specifically the BM25 variant, is employed as a scoring function to measure the similarity between the question and potential answer options. This helps identify the most relevant text segments or references that can aid in generating a response.\n",
      "Engineering Answer: In question answering retrieval systems, the TF-IDF model is used to represent queries and documents as bag-of-word vectors with terms weighted by term frequency multiplied by inverse document frequency. This allows for efficient non-learning-based search engine operations based on the vector space model. The TF-IDF model helps in calculating the relevance of documents to queries by measuring the importance of terms in the context of the entire document collection. This classic information retrieval approach aids in retrieving relevant information to answer questions accurately and efficiently.\n",
      "\n",
      "Marketing Prediction: The TF-IDF model is a classic and effective approach used in question-answering retrieval systems to rank and retrieve relevant text segments. In the given context, TF-IDF, specifically the BM25 variant, is employed as a retrieval scoring function. The system treats the question as a \"bag of words\" and uses TF-IDF to match and retrieve relevant text segments from a reference source, such as Wikipedia, to augment the generation of answers. This helps in providing relevant and informative responses to user queries.\n",
      "Marketing Answer: The TF-IDF model is used in question answering retrieval systems to weight terms in queries and documents based on their importance in determining relevance.\n",
      "------------------------------------------------------------\n",
      "Question: Can neural networks enhance the process of information retrieval in QA systems?\n",
      " \n",
      "Engineering Prediction: Yes, neural networks can significantly enhance the process of information retrieval in QA systems. The introduction of neural retrievers has been a key factor in the performance improvements of QA tasks. These neural models are trained specifically for retrieval and can effectively augment generation processes in knowledge-intensive NLP tasks.\n",
      "Engineering Answer: Neural networks, such as MLP, LSTM, and bidirectional LSTM, can be used to learn dense representations of text for information retrieval in QA systems. These approaches, known as 'Neural IR', are a new category of methods that can improve performance in retrieval problems. The introduction of neural retrievers in recent QA literature has shown to outperform traditional word-similarity-based architectures, such as BM25, and can scale to handle knowledge-grounded dialogue tasks effectively. Additionally, incorporating pre-trained retrievers in QA systems has been shown to enhance the performance of generative language models.\n",
      "\n",
      "Marketing Prediction: Yes, neural networks can significantly enhance the process of information retrieval in QA systems. The introduction of neural retrievers has been a key factor in the performance improvements of QA tasks. These neural models are trained specifically for retrieval and can effectively augment generation processes in knowledge-intensive NLP tasks.\n",
      "Marketing Answer: Yes, neural networks can enhance the process of information retrieval in QA systems by improving performance in open-domain QA tasks and enabling the generation of more accurate answers.\n",
      "------------------------------------------------------------\n",
      "Question: What is the importance of fine-tuning in the context of QA data for open-domain question answering models?\n",
      " \n",
      "Engineering Prediction: Fine-tuning is an important step in the training process of open-domain question-answering (ODQA) models. In the context of QA data, fine-tuning helps improve the model's alignment with human intention and reduces the cost of communication. \n",
      "\n",
      "By fine-tuning on QA datasets, the ODQA model can better understand the nuances of human language and provide more accurate and relevant answers to user questions. This step helps the model to improve its performance and effectiveness in open-domain QA tasks, where the questions can be more varied and complex compared to closed-domain or constrained QA. \n",
      "\n",
      "Additionally, fine-tuning can reduce the cost of communication by improving the model's ability to understand and generate language. This leads to more efficient and effective interactions between the model and human users, potentially reducing the need for clarification questions or incorrect responses. \n",
      "\n",
      "Overall, fine-tuning on QA data is a crucial step in developing robust and user-friendly ODQA models, ensuring they can better understand and respond to human queries.\n",
      "Engineering Answer: Fine-tuning is important in the context of QA data for open-domain question answering models because it allows the model to adapt and improve its performance on specific QA datasets. By fine-tuning the model with common QA datasets, engineers can optimize the model's ability to answer questions accurately. However, there is a concern about the significant overlap between questions in the train and test sets of public QA datasets, which could affect the generalization ability of the fine-tuned models. Engineers should carefully consider this overlap and potentially explore ways to mitigate its impact during the fine-tuning process to ensure the model's effectiveness in real-world applications.\n",
      "\n",
      "Marketing Prediction: Fine-tuning is an important step in the training process of open-domain question-answering (ODQA) models. In the context of QA data, fine-tuning helps improve the model's alignment with human intention and reduces the cost of communication. \n",
      "\n",
      "By fine-tuning on QA datasets, the ODQA model can better understand the nuances of human language and provide more accurate and relevant answers to user questions. This step helps the model to improve its performance and effectiveness in open-domain QA tasks, where the questions can be more varied and complex compared to closed-domain or constrained QA. \n",
      "\n",
      "Additionally, fine-tuning can reduce the communication cost by improving the model's ability to understand and generate contextually appropriate responses. This leads to more efficient and effective interactions between the model and human users, which is a key goal of ODQA systems.\n",
      "Marketing Answer: Fine-tuning is important in the context of QA data for open-domain question answering models to improve search task performance and the ability to generalize to unseen datasets.\n",
      "------------------------------------------------------------\n",
      "Question: How does pre-training with tasks like the Inverse Cloze Task benefit open-domain question answering models?\n",
      " \n",
      "Engineering Prediction: Pre-training with tasks like the Inverse Cloze Task (ICT) benefits open-domain question-answering models by enhancing their ability to generalize to unseen domains and tasks. \n",
      "\n",
      "The ICT is a language modeling task where the model is trained to predict a sentence given a context paragraph. This helps the model understand context and semantic relationships between sentences, improving its ability to comprehend and generate coherent text. \n",
      "\n",
      "By pre-training on knowledge-intensive tasks like ICT, the model acquires a broad understanding of language and can better capture underlying linguistic patterns. This enables the model to generate more relevant and contextually appropriate responses, even for questions from domains it hasn't encountered before. \n",
      "\n",
      "The benefits of pre-training with ICT and similar tasks extend to open-ended generation tasks, where the model generates free-form text. This improves the diversity and quality of the generated responses, making the model more versatile and adaptable to a wide range of question-answering scenarios.\n",
      "Engineering Answer: Pre-training with tasks like the Inverse Cloze Task benefits open-domain question answering models by improving the retrieval process over a knowledge base. By predicting the context given a sentence, the model can better understand the relationship between the question and the evidence. This approach helps in incorporating retrieved content effectively into the prompt, leading to higher accuracy in the question answering task. Additionally, using models pretrained with ICT can enhance the overall performance of the QA system by providing a better understanding of the context.\n",
      "\n",
      "Marketing Prediction: Pre-training with tasks like the Inverse Cloze Task (ICT) benefits open-domain question-answering models by enhancing their ability to generalize to unseen domains and tasks. \n",
      "\n",
      "The ICT is a language modeling task where the model is trained to predict a sentence given a context paragraph. This helps the model understand context and semantic relationships between sentences, improving its ability to comprehend and generate coherent text. \n",
      "\n",
      "By pre-training on knowledge-intensive tasks like ICT, the model acquires a broad understanding of language and can better capture underlying patterns and semantic representations. This improves its performance on open-domain question answering, where it needs to provide accurate and contextually relevant responses to a diverse range of questions, even in unseen domains. \n",
      "\n",
      "The benefits of pre-training with ICT include improved context understanding, better generalization to new tasks, and enhanced language generation capabilities, all of which contribute to more effective open-domain question-answering systems.\n",
      "Marketing Answer: Pre-training with tasks like the Inverse Cloze Task benefits open-domain question answering models by improving retrieval and generation steps, ultimately enhancing the accuracy of the process.\n",
      "------------------------------------------------------------\n",
      "Question: What is the main goal of prompt engineering in language models?\n",
      " \n",
      "Engineering Prediction: The main goal of prompt engineering in language models is to align and steer the model's behavior to perform a desired task. It is a method of communicating with LLMs to guide their predictions and responses in a specific direction. Prompt engineering techniques are used to improve the model's performance and adaptability to various tasks by providing context and instructions in a format that the model can understand.\n",
      "Engineering Answer: The main goal of prompt engineering in language models is to effectively steer the behavior of the model towards desired outcomes without updating the model weights. This is achieved by composing and formatting prompts in a way that maximizes the model's performance on a specific task. Prompt engineering involves treating prompts as trainable parameters and optimizing them directly on the embedding space through methods like AutoPrompt, Prefix-Tuning, P-tuning, and Prompt-Tuning. The ultimate aim is to enhance the model's performance and alignment with user-defined tasks.\n",
      "\n",
      "Marketing Prediction: The main goal of prompt engineering in language models is to align and steer the model's behavior to perform a desired task.\n",
      "Marketing Answer: The main goal of prompt engineering in language models is to steer the behavior of the model for desired outcomes without updating the model weights.\n",
      "------------------------------------------------------------\n",
      "Question: What are some known biases that can affect the performance of few-shot classification in LLMs?\n",
      " \n",
      "Engineering Prediction: Some known biases that can affect the performance of few-shot classification in LLMs include the following: \n",
      "\n",
      "- **Context Bias**: LLMs may rely too heavily on the provided context, ignoring new information in the few-shot examples. This can lead to incorrect predictions if the context is misleading or incomplete. \n",
      "\n",
      "- **Cognitive Bias**: Similar to humans, LLMs can develop cognitive biases, such as confirmation bias (seeking information that confirms their existing predictions) or anchoring bias (relying too heavily on the first few examples provided). \n",
      "\n",
      "- **Data Bias**: The few-shot examples used to train LLMs may not be representative of the broader data distribution, leading to biased predictions. This is especially true if the examples are selected in a non-random manner or come from a biased source. \n",
      "\n",
      "- **Shortcut Learning Bias**: LLMs might learn shortcuts or spurious correlations in the training data, ignoring more complex patterns. For instance, in image classification, the LLM might focus on simple features (e.g. background color) that are easy to learn from few examples, but fail to generalize to new data. \n",
      "\n",
      "- **Prompt Design Bias**: The way a few-shot task is presented to the LLM (the prompt) can significantly influence its performance. Different prompt designs might introduce biases, making the LLM more or less effective at certain tasks. \n",
      "\n",
      "- **Catastrophic Forgetting**: While not exactly a bias, this phenomenon can impact few-shot learning. LLMs might forget previously learned tasks when trained on new few-shot examples, especially if the new task is significantly different. \n",
      "\n",
      "These biases can impact the effectiveness of few-shot learning in LLMs and are important considerations when designing and evaluating these models. Researchers are actively working on mitigating these biases to improve the performance and reliability of LLMs in few-shot classification tasks.\n",
      "Engineering Answer: Some known biases that can affect the performance of few-shot classification in LLMs include majority label bias, recency bias, and common token bias. Majority label bias occurs when the distribution of labels among examples is unbalanced, recency bias refers to the tendency for the model to repeat the label at the end, and common token bias indicates that LLM tends to produce common tokens more often than rare tokens. These biases can contribute to high variance in few-shot classification tasks and may impact the model's ability to generalize effectively.\n",
      "\n",
      "Marketing Prediction: Some known biases that can affect the performance of few-shot classification in LLMs include the following: \n",
      "\n",
      "- **Context Bias**: LLMs, due to their inherent design, tend to rely heavily on the context provided in the prompt. In few-shot classification, if the context examples are not diverse or are biased towards a specific class, the LLM might struggle to generalize to new, unseen examples that fall outside the provided context. \n",
      "\n",
      "- **Cognitive Bias**: This refers to the tendency of LLMs to exhibit similar cognitive biases as the data they are trained on. For example, if the training data contains stereotypes or biased associations, the LLM might pick up on these biases and exhibit them in its few-shot classification performance. \n",
      "\n",
      "- **Frequency Bias**: LLMs often favor frequent patterns in the training data, which can lead to a bias towards common or majority classes in few-shot classification tasks. This bias can cause the model to ignore or misclassify rare or minority examples, affecting overall performance. \n",
      "\n",
      "- **Token Bias**: Tokenization, the process of converting text into numerical representations, can introduce bias. Certain tokens or sequences might be given higher weights or importance, which could influence the LLM's predictions, especially in few-shot settings where token frequency is limited. \n",
      "\n",
      "- **Prompt Engineering Bias**: The way few-shot examples are presented to the LLM (prompt engineering) can introduce bias. The choice of words, phrasing, and structure of the prompt can influence the LLM's performance, and if not carefully designed, might lead to biased predictions or limited generalization. \n",
      "\n",
      "- **Dataset Bias**: The quality and representativeness of the dataset used for few-shot learning are crucial. If the dataset contains biases, such as class imbalance or biased labeling, the LLM's performance will be affected, as it learns from the provided data. \n",
      "\n",
      "These are some key biases that can impact the effectiveness of few-shot classification in LLMs. Identifying and mitigating these biases is an active area of research in the field of LLMs and machine learning in general.\n",
      "Marketing Answer: Some known biases that can affect the performance of few-shot classification in LLMs are majority label bias, recency bias, and common token bias.\n",
      "------------------------------------------------------------\n",
      "Question: Why might increasing model size not reduce variance in model performance with varying prompts?\n",
      " \n",
      "Engineering Prediction: Increasing model size does not always reduce variance in performance with varying prompts because the relationship between model size and performance is complex and task-dependent. While larger models can have higher capacity and thus perform better on some tasks, they also introduce additional parameters that need to be optimized during training. This can lead to overfitting on the training data and higher variance when evaluated on new, unseen prompts.\n",
      "\n",
      "The context provided suggests that the task at hand is understanding-related, and larger models without fine-tuning can sometimes outperform smaller models. This indicates that the task may not require extensive parameter optimization, and the increased capacity of larger models is sufficient to improve performance.\n",
      "\n",
      "However, in other tasks where fine-tuning is necessary, increasing model size may not yield the expected improvements. The Knowledge F1 metric, for example, seems to decrease with larger models, suggesting that the additional parameters introduce noise or interfere with the model's ability to generalize.\n",
      "\n",
      "In summary, the relationship between model size and performance is nuanced, and the effectiveness of increasing model size depends on the specific task and the trade-offs between capacity and optimization.\n",
      "Engineering Answer: Increasing model size may not necessarily reduce variance in model performance with varying prompts because the model's ability to generalize and adapt to different prompts is not solely dependent on its size. Factors such as the quality and relevance of the training examples, the learning rate or schedule, and the model's sensitivity to different hyperparameters can also play a significant role in determining performance variability. Additionally, the complexity of the task or dataset being used for training can impact how effectively the model scales with size. It is essential to consider these factors holistically when optimizing model performance rather than relying solely on increasing model size.\n",
      "\n",
      "Marketing Prediction: Increasing model size does not always reduce variance in performance with varying prompts because the relationship between model size and performance is complex and task-dependent. While larger models can have higher capacity and thus perform better on some tasks, they also introduce additional parameters that need to be optimized during training. This can lead to overfitting on the training data and higher variance when evaluated on new, unseen prompts.\n",
      "\n",
      "In the context provided, it is mentioned that increasing model size leads to decreasing performance in the Knowledge F1 metric. This suggests that while larger models may perform better on some metrics or tasks, they can also underperform in other aspects. The Knowledge F1 metric, for example, evaluates the model's ability to retain and retrieve factual knowledge, which may be impacted by larger models that focus more on pattern recognition than memorization.\n",
      "\n",
      "Additionally, the context mentions that larger models without fine-tuning can sometimes outperform smaller models on understanding tasks. This indicates that the pre-training process of larger models may already capture a broad range of linguistic patterns and knowledge, making them effective even without task-specific fine-tuning. However, this advantage may not consistently reduce variance across different prompts, as the model's performance still depends on the specific wording and context of each prompt.\n",
      "\n",
      "In summary, the relationship between model size and performance is nuanced. While larger models can provide benefits in certain scenarios, they do not guarantee reduced variance across all tasks and evaluation metrics. The optimal model size depends on the specific requirements and characteristics of the task at hand.\n",
      "Marketing Answer: Increasing model size may not reduce variance in model performance with varying prompts because the same order of prompts may work well for one model but poorly for another. Additionally, when the validation set is limited, choosing the order of prompts that prevents the model from producing extremely unbalanced predictions or being overconfident can also affect performance.\n",
      "------------------------------------------------------------\n",
      "Question: What is the benefit of instruction-based finetuning in language models?\n",
      " \n",
      "Engineering Prediction: The benefit of instruction-based fine-tuning in language models is that it can help to mitigate domain shifts.\n",
      "Engineering Answer: Instruction-based finetuning improves models' ability to generalize to unseen domains and tasks by providing task-specific representations that can be used for many downstream language tasks without additional training. This method also allows pretrained language models to follow instructions provided in prompts, enabling them to generate the desired output given specific inputs. Additionally, instruction finetuning helps transform raw pretrained LLMs into chatbot-like models, making finetuning more accessible and common, particularly for researchers with limited resources. Overall, the benefit of instruction-based finetuning is improved model performance, enhanced generalizability, and reduced communication costs in aligning with human intentions.\n",
      "\n",
      "Marketing Prediction: The benefit of instruction-based fine-tuning in language models is that it can help to mitigate domain shifts.\n",
      "Marketing Answer: The benefit of instruction-based finetuning in language models is improved ability to generalize to unseen domains and tasks, without the need for additional training.\n",
      "------------------------------------------------------------\n",
      "Question: Can you describe a situation where retrieval-based methods would be necessary to enhance language model performance?\n",
      " \n",
      "Engineering Prediction: Retrieval-based methods are necessary when the language model needs to access information beyond its training data or when faced with tasks requiring factual knowledge. This is especially true for open-domain question-answering or complex reasoning tasks, where the model needs to retrieve relevant information from a large corpus to provide accurate responses or reasoning chains. \n",
      "\n",
      "For example, consider a language model trained to answer questions about books. While it may have knowledge about classic literature, it might lack specific information about newly released books or niche genres. In this case, a retrieval-based method could be used to fetch additional context or relevant passages from a database, enabling the model to provide up-to-date and informed responses. \n",
      "\n",
      "Similarly, for complex reasoning tasks like multi-hop question answering, the model might need to retrieve and combine information from multiple sources or documents to arrive at the correct answer. Here, retrieval-based methods ensure that the model can access and utilize the required information effectively, enhancing its performance and the accuracy of its responses.\n",
      "Engineering Answer: Retrieval-based methods are necessary to enhance language model performance in scenarios where the model needs to generate accurate and informative responses for entity-rich queries, such as 'George Washington standing in front of the Eiffel Tower.' In such cases, incorporating a retrieval module can provide additional context and relevant information to improve the model's understanding and generation of the desired output. Additionally, retrieval-based methods are crucial for question answering tasks, where the model needs to access external knowledge sources to provide accurate and comprehensive answers. By utilizing retrieval mechanisms, the language model can benefit from a wider range of information and improve its performance in handling complex and ambiguous queries effectively.\n",
      "\n",
      "Marketing Prediction: Retrieval-based methods are necessary when the language model needs to access information beyond its training data or when faced with tasks requiring factual knowledge. This is especially true for open-domain question-answering or complex reasoning tasks, where the model needs to retrieve relevant information from a large corpus to provide accurate responses or reasoning chains. \n",
      "\n",
      "For example, consider a language model trained to answer questions about books. While it may have knowledge about classic literature, it might lack specific information about newly released books or niche genres. In this case, a retrieval-based method could be used to fetch additional context or relevant passages from a database, allowing the model to provide more informed and up-to-date responses. \n",
      "\n",
      "Similarly, for complex reasoning tasks like multi-hop question answering, the model might need to retrieve and combine information from multiple sources or documents to arrive at the correct answer. Here, retrieval-based methods ensure that the model can access and utilize the required information effectively, enhancing its performance and the accuracy of its responses.\n",
      "Marketing Answer: Retrieval-based methods are necessary to enhance language model performance in tasks like question answering, where incorporating additional information from external sources can improve the model's ability to generate accurate and relevant responses.\n",
      "------------------------------------------------------------\n",
      "Question: What is the Chain-of-Thought prompting technique and for which types of tasks is it particularly beneficial?\n",
      " \n",
      "Engineering Prediction: Chain-of-Thought (CoT) prompting is a technique used in Large Language Models (LLMs) to improve their reasoning capabilities and provide more transparent and interpretable predictions. \n",
      "\n",
      "With CoT prompting, the LLM is instructed to generate a step-by-step reasoning process, or a chain of thought, that leads to the final answer. This is done by providing a prompt that asks the model to explain its reasoning, often by decomposing a complex problem into a series of simpler steps. \n",
      "\n",
      "For example, when asked a math word problem, a LLM with CoT prompting might first define the variables, then lay out the equations, solve them step-by-step, and finally provide the answer along with the solution path. \n",
      "\n",
      "This technique is particularly beneficial for tasks that require multi-step reasoning, complex problem solving, or logical inference. This includes mathematical problem solving, commonsense reasoning, and complex question answering. CoT prompting helps improve the accuracy and interpretability of LLM responses in these domains. \n",
      "\n",
      "Additionally, CoT prompting can be useful for debugging and understanding the decision-making process of LLMs, as it provides a window into the \"thought process\" of the model. This can help identify errors or biases in the model's reasoning and potentially improve its performance.\n",
      "Engineering Answer: Chain-of-Thought (CoT) prompting is a technique that generates reasoning chains or rationales step by step to lead to a final answer, benefiting complicated reasoning tasks using large models with more than 50B parameters. It can be implemented through iterative Monte Carlo search methods or through a three-step process called augment-prune-select. CoT is particularly beneficial for enhancing model performance on complex tasks by decomposing them into smaller and simpler steps, shedding light on the model's thinking process. Task decomposition in CoT can be done with simple prompting, task-specific instructions, or human inputs.\n",
      "\n",
      "Marketing Prediction: Chain-of-Thought (CoT) prompting is a technique used in Large Language Models (LLMs) to improve their reasoning capabilities and provide more transparent and interpretable predictions. \n",
      "\n",
      "With CoT prompting, the LLM is instructed to generate a step-by-step reasoning process, or a chain of thought, that leads to the final answer. This is done by providing a prompt that asks the model to explain its reasoning, often by decomposing a complex problem into a series of simpler steps. \n",
      "\n",
      "This technique is particularly beneficial for tasks that require multi-step reasoning, logical inference, or mathematical calculations. Some examples include: \n",
      "1. **Math and Science Problems**: CoT prompting can be used to solve multi-step math problems or explain scientific concepts. For instance, a prompt might ask the LLM to calculate the trajectory of a projectile, and the model would need to explain the steps of its calculation. \n",
      "2. **Common Sense Reasoning**: CoT can help improve the common sense reasoning abilities of LLMs. For example, when answering a question like \"What would happen if you put a lit match in a glass of water?\", the LLM might explain the steps of its reasoning, including the properties of fire and water. \n",
      "3. **Question Answering and Trivia**: For complex questions that require synthesis of multiple facts or multi-hop reasoning, CoT prompting can be beneficial. An example would be a question that requires understanding the context of a story and then making logical inferences based on that context. \n",
      "4. **Complex Instruction Following**: Tasks that involve following a set of instructions, especially when those instructions are intricate and have multiple conditions, can be improved with CoT prompting. \n",
      "5. **Diagnostic Reasoning**: In medical diagnosis or machine troubleshooting scenarios, CoT prompting can be used to explain the steps taken to arrive at a diagnosis or solution. \n",
      "\n",
      "In all these cases, CoT prompting helps to make the LLM's decision-making process more transparent and interpretable, which can improve user trust and also help identify any errors in the model's reasoning.\n",
      "Marketing Answer: Chain-of-Thought (CoT) prompting is a technique that generates reasoning chains or rationales step by step to lead to a final answer. It is particularly beneficial for complicated reasoning tasks when using large models with more than 50B parameters. Simple tasks only benefit slightly from CoT prompting.\n",
      "------------------------------------------------------------\n",
      "Question: How do augmented language models with external tools differ from regular models in functionality?\n",
      " \n",
      "Engineering Prediction: Augmented Language Models (ALMs) that utilize external tools offer enhanced functionality compared to regular language models by incorporating additional features and capabilities. \n",
      "\n",
      "Here are some key differences in functionality between ALMs with external tools and regular language models: \n",
      "\n",
      "1. **Data Processing and Feature Extraction**: ALMs can utilize external tools to process and extract features from data more effectively. For example, they can employ tools for advanced data preprocessing, such as named entity recognition, sentiment analysis, or computer vision models for image understanding. These additional tools enable ALMs to capture richer representations of the input data. \n",
      "\n",
      "2. **Knowledge Integration**: External tools can provide ALMs with access to external knowledge bases or ontologies. This integration allows ALMs to incorporate world knowledge or domain-specific information into their language understanding and generation processes. For example, an ALM could utilize a knowledge graph to enhance its ability to answer complex questions or generate more informed responses. \n",
      "\n",
      "3. **Model Architecture Enhancements**: Some external tools offer new model architectures or extensions that improve the capabilities of ALMs. These tools might introduce attention mechanisms, memory modules, or graph neural networks that enhance context understanding, long-term dependency modeling, or relational reasoning. These architectural enhancements can improve the overall performance and generalization abilities of ALMs. \n",
      "\n",
      "4. **Model Adaptation and Personalization**: ALMs with external tools can provide better adaptation and personalization. These tools might include user-specific data, such as user profiles or interaction histories, to tailor the language model's responses or recommendations to individual users. This functionality enables more personalized and contextually relevant language understanding and generation. \n",
      "\n",
      "5. **Multimodal Understanding**: External tools can extend the capabilities of ALMs to process and understand multimodal data, including text, images, audio, and video. By incorporating tools for multimodal fusion or alignment, ALMs can capture complex relationships between different modalities, enabling a more holistic understanding of the input data. \n",
      "\n",
      "6. **Interactive Learning and Reinforcement**: Some external tools facilitate interactive learning or reinforcement learning paradigms for ALMs. These tools can provide feedback loops, reward functions, or user interaction mechanisms that enable ALMs to learn from user interactions or optimize their responses based on specific objectives. \n",
      "\n",
      "\n",
      "These are just a few examples of how augmented language models with external tools can differ from regular models in terms of functionality. The specific enhancements depend on the nature of the external tools integrated with the ALM.\n",
      "Engineering Answer: Augmented language models with external tools, such as TALM and Toolformer, are fine-tuned to learn how to use external tool APIs, expanding their capabilities beyond traditional language processing tasks. These models are trained to incorporate external tool API calls in order to improve the quality of their outputs, allowing them to perform tasks like speech recognition, machine translation, and information retrieval more effectively. By leveraging external tools, these models have the ability to access and utilize a wider range of resources and functionalities, enhancing their overall performance and versatility compared to regular language models.\n",
      "\n",
      "Marketing Prediction: Augmented Language Models (ALMs) that utilize external tools differ from regular language models in their ability to incorporate additional functionality and knowledge from these tools. \n",
      "\n",
      "ALMs are an extension of traditional language models, and by incorporating external tools, they can enhance their language understanding and generation capabilities. These external tools can provide ALMs with specific functionalities, such as: \n",
      "1. **Knowledge Base**: External tools can act as a knowledge base, providing the ALM with additional factual information or domain-specific knowledge. This can enhance the model's ability to provide accurate and informative responses, especially in specific subject areas. \n",
      "2. **Semantic Analysis**: Tools can contribute to a deeper semantic analysis of the text. For example, they might provide sentiment analysis, named entity recognition, or relationship extraction, enabling the ALM to understand the context and nuances of the language more effectively. \n",
      "3. **Data Augmentation**: Some tools generate additional training data, which can be used to improve the ALM's performance and reduce the need for large-scale human-labeled data. \n",
      "4. **Specialized Functions**: External tools can introduce specialized functions, such as summarization, question-answering, or machine translation capabilities, making the ALM more versatile. \n",
      "5. **User Interaction**: Certain tools can facilitate user interaction, such as dialogue management systems, which enable more natural and intuitive conversations with the ALM. \n",
      "\n",
      "By incorporating these external tools, ALMs can go beyond the limitations of regular language models, offering enhanced functionality and improved performance in specific tasks or domains. This makes them more flexible and adaptable to a wider range of applications.\n",
      "Marketing Answer: Augmented language models with external tools differ from regular models by fine-tuning a LM to use external tool APIs, expanding the dataset to improve model outputs and enhancing tasks like speech recognition, machine translation, and natural language generation.\n",
      "------------------------------------------------------------\n",
      "Question: What can be inferred about the utilization of attention in neural networks?\n",
      " \n",
      "Engineering Prediction: It can be inferred that attention in neural networks, particularly in the field of deep learning and natural language processing (NLP), is a mechanism that assigns importance weights to input data. These weights help the network focus on the most relevant information, improving prediction accuracy and understanding of the data. The reference provided suggests that attention mechanisms can enhance memory and information processing capabilities in deep learning models, especially when dealing with large and complex datasets.\n",
      "Engineering Answer: Attention mechanisms in neural networks play a crucial role in allowing models to focus on specific parts of input data when making predictions or generating outputs. By assigning importance weights to different elements, such as pixels in an image or words in a sentence, attention helps the model to attend to relevant information and make more accurate predictions. The use of attention can improve the interpretability of neural networks by showing which parts of the input data are being focused on during the prediction process. Additionally, attention mechanisms, like multi-head attention, can enhance model performance by allowing the model to jointly attend to information from different representation subspaces at different positions.\n",
      "\n",
      "Marketing Prediction: It can be inferred that attention in neural networks, particularly in the field of deep learning and natural language processing (NLP), is a mechanism that assigns importance weights to input data. These weights help the network focus on the most relevant information, improving prediction accuracy and understanding of the data. The reference provided suggests that attention mechanisms have been a topic of interest and research since at least 2016, with advancements being made in efficient and exact attention methods.\n",
      "Marketing Answer: Attention in neural networks allows the model to focus on specific parts of input data, such as images or text, in order to make predictions or generate output. It helps the model to learn relationships and correlations between different elements and improve performance in tasks like image captioning or language translation.\n",
      "------------------------------------------------------------\n",
      "Question: Can the use of attention mechanisms in deep learning models be applied to both machine translation and computer vision?\n",
      " \n",
      "Engineering Prediction: Yes, attention mechanisms in deep learning models can be applied to both machine translation and computer vision tasks. \n",
      "\n",
      "In the context provided, it is mentioned that the attention mechanism was initially developed to assist in memorizing long source sentences in neural machine translation. This technique helps the model focus on relevant parts of the input sequence, which is especially useful for translating long and complex sentences. \n",
      "\n",
      "However, the concept of attention can also be applied to computer vision tasks. Visual attention can be directed to different regions of an image, highlighting important or relevant areas that the model should focus on. This can be useful for tasks such as object detection, image captioning, or even image generation, where the model needs to understand and interpret visual input. \n",
      "\n",
      "So, attention mechanisms are versatile tools in deep learning that can enhance performance in both machine translation and computer vision applications by allowing models to focus on the most relevant information.\n",
      "Engineering Answer: Yes, attention mechanisms in deep learning models have shown success in both machine translation and computer vision tasks. In machine translation, attention allows the model to capture dependencies between source and target sequences regardless of distance, leading to improved translation quality. Similarly, in computer vision, attention mechanisms have been used to focus on relevant parts of an image during caption generation, showcasing the ability to handle details and global dependencies effectively. Therefore, utilizing attention in both domains can enhance the performance of deep learning models significantly.\n",
      "\n",
      "Marketing Prediction: Yes, attention mechanisms in deep learning models can be applied to both machine translation and computer vision tasks. \n",
      "\n",
      "In the context provided, it is mentioned that the attention mechanism was initially developed to assist in memorizing long source sentences in neural machine translation. This technique helps the model focus on relevant parts of the input sequence, which is especially useful for translating long and complex sentences. \n",
      "\n",
      "However, the concept of attention can also be applied to computer vision tasks. Just as attention helps in focusing on relevant words or phrases in machine translation, it can also be used to highlight important regions or objects in an image. By assigning importance weights to different parts of an image, attention mechanisms can help models better understand and interpret visual data. \n",
      "\n",
      "So, attention mechanisms are versatile tools in deep learning that can enhance performance in both machine translation and computer vision applications.\n",
      "Marketing Answer: Yes, attention mechanisms in deep learning models can be applied to both machine translation and computer vision.\n",
      "------------------------------------------------------------\n",
      "Question: What are the potential benefits of incorporating self-attention mechanisms into Generative Adversarial Networks (GANs)?\n",
      " \n",
      "Engineering Prediction: The potential benefit of incorporating self-attention mechanisms into Generative Adversarial Networks (GANs) is that it enables the models to capture long-range dependencies and attend to relevant features or regions in the input data. This mechanism allows the GAN to focus on specific parts of the input and weigh their importance dynamically, resulting in improved generation quality and stability. \n",
      "\n",
      "By adding self-attention layers, GANs can model global and local context more effectively, especially for tasks involving complex data such as images or text. This can lead to more coherent and contextually appropriate generations, as the model can consider the relationships between distant or disparate parts of the input. \n",
      "\n",
      "Furthermore, self-attention can help with stabilizing GAN training, as it provides a form of inductive bias that guides the model to focus on relevant features. This can result in faster convergence and reduced mode collapse, where the generator produces limited varieties of samples. \n",
      "\n",
      "Overall, incorporating self-attention mechanisms into GANs enhances their ability to model complex relationships within data and improves the quality and diversity of generated content.\n",
      "Engineering Answer: Incorporating self-attention mechanisms into GANs can help the generator and discriminator better model relationships between spatial regions, leading to improved generation of detailed and realistic images. This is particularly useful for capturing global dependencies and enhancing the performance of transformer architectures. Additionally, self-attention can enable the model to assess its own predictions after each generated segment, allowing for customizable decoding algorithms to meet specific constraints or user preferences. Overall, self-attention in GANs can enhance detail handling and overall performance.\n",
      "\n",
      "Marketing Prediction: By incorporating self-attention mechanisms into Generative Adversarial Networks (GANs), the model gains the ability to capture long-range dependencies and attend to relevant features or regions in the input data. This allows the GAN to focus on important details and generate more coherent and globally consistent outputs. Self-attention enables the generator and discriminator to consider the relationships between different parts of the data, improving the overall quality and realism of the generated content. This is especially beneficial for tasks such as image generation, where understanding the relationships between distant pixels or regions is crucial for creating realistic and contextually appropriate images.\n",
      "Marketing Answer: Incorporating self-attention mechanisms into GANs can help the generator and discriminator better model relationships between spatial regions, leading to improved performance in handling details and capturing global dependencies.\n",
      "------------------------------------------------------------\n",
      "Question: How does the transformer model variate from traditional sequence-aligned recurrent architectures?\n",
      " \n",
      "Engineering Prediction: The Transformer model differs from traditional sequence-aligned recurrent architectures in that it is entirely built on self-attention mechanisms, abandoning the need for sequence alignment. This unique architecture enables the Transformer to handle significantly longer sequences beyond its training data and extrapolate effectively to different sequence lengths. While traditional recurrent architectures process sequences step by step, the Transformer's self-attention mechanism allows it to attend to all input elements in parallel, making it highly efficient and capable of capturing global dependencies in the data.\n",
      "Engineering Answer: The transformer model differs from traditional sequence-aligned recurrent architectures by not having a recurrent or convolutional structure. Instead, it heavily relies on self-attention mechanisms for processing sequences. This lack of recurrence and convolution, even with positional encoding, weakly incorporates sequential order, which can be a drawback for tasks sensitive to positional dependencies. Additionally, the transformer's architecture includes embedding layers, sinusoid-wave-based positional encoding, and softmax and linear layers in the final decoder output to maintain position information and facilitate processing of long sequences efficiently.\n",
      "\n",
      "Marketing Prediction: The Transformer model differs from traditional sequence-aligned recurrent architectures in that it is entirely built on self-attention mechanisms, abandoning the need for sequence alignment. This unique architecture allows the Transformer to handle significantly longer sequences beyond its training data and extrapolate effectively to different sequence lengths. By doing so, the Transformer model showcases a remarkable ability to process and understand sequential data in a way that traditional recurrent architectures struggle to achieve.\n",
      "Marketing Answer: The transformer model differs from traditional sequence-aligned recurrent architectures by not having a recurrent or convolutional structure, and instead making heavy use of self-attention. This allows for handling very long sequences efficiently and achieving better performance on tasks involving long texts.\n",
      "------------------------------------------------------------\n",
      "Question: What implications does the concept of a Neural Turing Machine have for the theoretical power of neural networks?\n",
      " \n",
      "Engineering Prediction: The Neural Turing Machine (NTM) has significant implications for the theoretical power of neural networks. By combining the neural network's ability to learn and generalize with the Turing machine's computational power, the NTM creates a model with enhanced capabilities. \n",
      "\n",
      "Firstly, the NTM can process and store large amounts of data more efficiently. The neural network component can learn to prioritize and store relevant information in an external memory matrix, similar to a Turing machine's tape. This allows the NTM to handle complex sequences and tasks that traditional neural networks might struggle with due to their limited memory capacity. \n",
      "\n",
      "Secondly, the NTM has the potential for more advanced reasoning and problem-solving capabilities. The neural network can learn to manipulate symbols and perform complex operations on the data stored in the external memory, much like a Turing machine's read/write head. This combination of neural network flexibility and Turing machine computational power enables the NTM to tackle more intricate decision-making processes and potentially solve more complex problems. \n",
      "\n",
      "Additionally, the NTM architecture opens up new avenues for research in areas like natural language processing, machine learning, and artificial intelligence. The model's ability to store and manipulate data in a more human-like fashion could lead to significant advancements in these fields, improving the overall capabilities and applications of neural networks. \n",
      "\n",
      "In conclusion, the Neural Turing Machine expands the theoretical power of neural networks by providing enhanced memory, improved data processing, and more advanced computational capabilities. This hybrid model architecture showcases the potential for further innovation in the field, pushing the boundaries of what neural networks can achieve.\n",
      "Engineering Answer: The concept of a Neural Turing Machine (NTM) expands the theoretical power of neural networks by incorporating external memory storage, allowing for more complex computations and tasks. This mimics the Turing machine tape, enabling the neural network to control operation heads for reading and writing to the tape. However, the finite memory in NTM suggests it may resemble more of a 'Neural von Neumann Machine,' limiting its mathematical limitlessness seen in traditional Turing machines. Overall, the addition of external memory in NTM enhances the capabilities and potential applications of neural networks in solving more advanced problems.\n",
      "\n",
      "Marketing Prediction: The Neural Turing Machine (NTM) has significant implications for the theoretical power of neural networks. By combining the principles of Turing machines with neural networks, NTM architecture enhances the capabilities of traditional neural networks in several ways:\n",
      "\n",
      "1. **Enhanced Memory and Recall**: One of the key strengths of NTMs is their ability to provide neural networks with a form of external memory. This additional memory can be read, written to, and addressed in a content-based manner, much like a conventional Turing machine. This feature enables neural networks to store and retrieve large amounts of information, potentially over extended periods, and improves their ability to handle complex sequential data and tasks that require long-term memory.\n",
      "\n",
      "2. **Improved Generalization and Learning**: The integration of Turing machine principles allows neural networks to generalize better and learn more efficiently. NTMs can learn to manipulate and reason with complex data structures, making them particularly useful for tasks involving symbolic reasoning, natural language processing, and program execution. This capability enhances the network's ability to generalize rules and patterns, leading to improved performance on a wide range of cognitive tasks.\n",
      "\n",
      "3. **Flexible Computation**: NTMs offer a more flexible computational model compared to traditional neural networks. They can dynamically allocate and manipulate memory resources as needed, enabling them to adapt to varying computational demands. This flexibility allows NTMs to handle a broader range of problems and makes them more versatile in solving tasks that require dynamic memory management.\n",
      "\n",
      "4. **Increased Capacity for Algorithm Execution**: The Turing machine component of NTMs provides them with the ability to execute algorithms and perform complex computations. This capability expands the scope of problems that neural networks can solve and enables them to tackle tasks that require step-by-step procedural reasoning, iterative calculations, and dynamic programming.\n",
      "\n",
      "5. **Bridging Symbolic and Sub-symbolic Processing**: NTMs offer a bridge between symbolic AI (which is based on logical rules and manipulations of symbols) and sub-symbolic AI (which is based on connectionist models and statistical learning). By combining the strengths of both paradigms, NTMs have the potential to address some of the limitations of traditional neural networks, such as their lack of interpretability and difficulty in incorporating prior knowledge.\n",
      "\n",
      "In summary, the concept of Neural Turing Machines expands the theoretical power of neural networks by providing them with enhanced memory, improved generalization capabilities, flexible computation, and the ability to execute complex algorithms. NTMs represent a significant step forward in the pursuit of more powerful and versatile machine learning models, with potential applications in a wide range of domains.\n",
      "Marketing Answer: The concept of a Neural Turing Machine suggests that neural networks can be equipped with external memory storage for more complex operations, potentially increasing their theoretical power.\n"
     ]
    }
   ],
   "source": [
    "baseline_test_eval = evaluate(metrics, test_dict, rag_chain, iterations=0, verbose=False, dept_specific=False, print_results=True)\n",
    "baseline_test_eval = composite_evaluation(baseline_test_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "9Z3Dn3YUhrPX",
    "outputId": "e0481888-5474-4e25-f925-36c38142e7f0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"baseline_test_eval\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"Sample\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 30.17806634975593,\n        \"min\": 13.785694692370804,\n        \"max\": 104.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          81.31428571428572,\n          82.0,\n          35.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eng_bleu\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12.347493277446944,\n        \"min\": 0.0,\n        \"max\": 35.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.07682629209133746,\n          0.07732436262017761,\n          35.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mk_bleu\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12.33577797398606,\n        \"min\": 0.0,\n        \"max\": 35.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.08043192327798498,\n          0.0357249401562083,\n          35.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eng_rouge\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12.293344862403936,\n        \"min\": 0.07113470567856986,\n        \"max\": 35.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.25446076053446026,\n          0.2442244224422442,\n          35.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mk_rouge\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12.292413596178944,\n        \"min\": 0.04195804195804196,\n        \"max\": 35.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.23260488916065425,\n          0.20512820512820515,\n          35.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eng_f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12.111611752032191,\n        \"min\": 0.019283202188783842,\n        \"max\": 35.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.8749718376568385,\n          0.8770012259483337,\n          35.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mk_f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12.110050098906514,\n        \"min\": 0.032525777439182446,\n        \"max\": 35.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.8763814432280405,\n          0.8721993565559387,\n          35.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"composite_eng\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12.211656040751823,\n        \"min\": 0.03712122657473845,\n        \"max\": 35.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.529189405407025,\n          0.5321124653618581,\n          35.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"composite_mk\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12.209451281558078,\n        \"min\": 0.07723187936923617,\n        \"max\": 35.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.5240585730178134,\n          0.5029464459138779,\n          35.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"composite_total\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12.211710902049363,\n        \"min\": 0.04824977219631681,\n        \"max\": 35.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.5271370724513403,\n          0.515046102825842,\n          35.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-8c0b90cf-455c-4e52-8825-619ac6c08b4d\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample</th>\n",
       "      <th>eng_bleu</th>\n",
       "      <th>mk_bleu</th>\n",
       "      <th>eng_rouge</th>\n",
       "      <th>mk_rouge</th>\n",
       "      <th>eng_f1</th>\n",
       "      <th>mk_f1</th>\n",
       "      <th>composite_eng</th>\n",
       "      <th>composite_mk</th>\n",
       "      <th>composite_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>81.314286</td>\n",
       "      <td>0.076826</td>\n",
       "      <td>0.080432</td>\n",
       "      <td>0.254461</td>\n",
       "      <td>0.232605</td>\n",
       "      <td>0.874972</td>\n",
       "      <td>0.876381</td>\n",
       "      <td>0.529189</td>\n",
       "      <td>0.524059</td>\n",
       "      <td>0.527137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>13.785695</td>\n",
       "      <td>0.051092</td>\n",
       "      <td>0.105101</td>\n",
       "      <td>0.071135</td>\n",
       "      <td>0.142133</td>\n",
       "      <td>0.019283</td>\n",
       "      <td>0.032526</td>\n",
       "      <td>0.037121</td>\n",
       "      <td>0.077232</td>\n",
       "      <td>0.048250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>59.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.101695</td>\n",
       "      <td>0.041958</td>\n",
       "      <td>0.844042</td>\n",
       "      <td>0.818897</td>\n",
       "      <td>0.466631</td>\n",
       "      <td>0.422036</td>\n",
       "      <td>0.458420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>69.500000</td>\n",
       "      <td>0.042885</td>\n",
       "      <td>0.023644</td>\n",
       "      <td>0.214925</td>\n",
       "      <td>0.127077</td>\n",
       "      <td>0.854528</td>\n",
       "      <td>0.849923</td>\n",
       "      <td>0.505706</td>\n",
       "      <td>0.468872</td>\n",
       "      <td>0.495613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>82.000000</td>\n",
       "      <td>0.077324</td>\n",
       "      <td>0.035725</td>\n",
       "      <td>0.244224</td>\n",
       "      <td>0.205128</td>\n",
       "      <td>0.877001</td>\n",
       "      <td>0.872199</td>\n",
       "      <td>0.532112</td>\n",
       "      <td>0.502946</td>\n",
       "      <td>0.515046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>92.500000</td>\n",
       "      <td>0.095641</td>\n",
       "      <td>0.100431</td>\n",
       "      <td>0.280654</td>\n",
       "      <td>0.273116</td>\n",
       "      <td>0.887044</td>\n",
       "      <td>0.896754</td>\n",
       "      <td>0.545990</td>\n",
       "      <td>0.544595</td>\n",
       "      <td>0.542411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>104.000000</td>\n",
       "      <td>0.190747</td>\n",
       "      <td>0.433725</td>\n",
       "      <td>0.447761</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.919355</td>\n",
       "      <td>0.959609</td>\n",
       "      <td>0.631146</td>\n",
       "      <td>0.754049</td>\n",
       "      <td>0.659628</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8c0b90cf-455c-4e52-8825-619ac6c08b4d')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-8c0b90cf-455c-4e52-8825-619ac6c08b4d button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-8c0b90cf-455c-4e52-8825-619ac6c08b4d');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-305b328b-9161-4fe4-8f82-c03a4f95afa4\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-305b328b-9161-4fe4-8f82-c03a4f95afa4')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-305b328b-9161-4fe4-8f82-c03a4f95afa4 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "           Sample   eng_bleu    mk_bleu  eng_rouge   mk_rouge     eng_f1  \\\n",
       "count   35.000000  35.000000  35.000000  35.000000  35.000000  35.000000   \n",
       "mean    81.314286   0.076826   0.080432   0.254461   0.232605   0.874972   \n",
       "std     13.785695   0.051092   0.105101   0.071135   0.142133   0.019283   \n",
       "min     59.000000   0.000000   0.000000   0.101695   0.041958   0.844042   \n",
       "25%     69.500000   0.042885   0.023644   0.214925   0.127077   0.854528   \n",
       "50%     82.000000   0.077324   0.035725   0.244224   0.205128   0.877001   \n",
       "75%     92.500000   0.095641   0.100431   0.280654   0.273116   0.887044   \n",
       "max    104.000000   0.190747   0.433725   0.447761   0.625000   0.919355   \n",
       "\n",
       "           mk_f1  composite_eng  composite_mk  composite_total  \n",
       "count  35.000000      35.000000     35.000000        35.000000  \n",
       "mean    0.876381       0.529189      0.524059         0.527137  \n",
       "std     0.032526       0.037121      0.077232         0.048250  \n",
       "min     0.818897       0.466631      0.422036         0.458420  \n",
       "25%     0.849923       0.505706      0.468872         0.495613  \n",
       "50%     0.872199       0.532112      0.502946         0.515046  \n",
       "75%     0.896754       0.545990      0.544595         0.542411  \n",
       "max     0.959609       0.631146      0.754049         0.659628  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_test_eval.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5X63KptZYpVo"
   },
   "source": [
    "#### Final Selected Model on Holdout Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3yqNjUlDYY6A"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "embedding_model = 'multi-qa-mpnet-base-dot-v1'\n",
    "chunk_size = 256\n",
    "chunk_overlap = 16\n",
    "llm = \"cohere\"\n",
    "retreiver_search_type = \"mmr\"\n",
    "retreiver_k = 6\n",
    "eng_rag_template = \"\"\"[INST]\n",
    "              Please provide an precise and concise answer to the engineer's question below based on the context information provided.\\n\\n\n",
    "              Below is a context:\\n{context}\\n\n",
    "              Below is a question:\\n{question}\\n\n",
    "              Below are answer instructions in order of importance:\n",
    "- Formatting: Provide a succint, single-paragraph answer. Do not use bullet points. Do not explicitly reference papers in your answer.\n",
    "- Technical Detail: Include technical details and terminologies that relate to the question.\n",
    "- Research Focus: Orient answers towards the research aspects of the questions.\n",
    "- Objective Tone: Maintain an objective and informative tone, aiming to educate the reader without persuasive language.\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "mk_rag_template = \"\"\"[INST]\n",
    "              Please provide a precise and concise answer to the marketer's question below based on the context provided.\\n\\n\n",
    "              Below is a context:\\n{context}\\n\n",
    "              Below is a question:\\n{question}\\n\n",
    "              Below are answer instructions in order of importance:\n",
    "- Formatting: Provide a concise, single-paragraph answer that uses the fewest words necessary to fully address the question. Answer in a single sentence or phrase if you can. Do not use bullet points. Do not explicitly reference papers in your answer.\n",
    "- Succinctness: Make sure your answer is concise and to the point. Provide only the essential information without delving into the technical depth.\n",
    "- Broad Overview: Give a broad overview of the topic. Provide only the essential information without delving into the technical depth.\n",
    "- Focus on Applications: EFocus on real-world uses and benefits and highlight how technology can solve problems or create opportunities.\n",
    "[/INST]\n",
    "\"\"\"\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "base_embeddings, text_splitter, qdrant_vectorstore, retriever = build_embedding_splitter_vectorstore(embedding_model, splitter, retr_search_type=retreiver_search_type, retr_k=retreiver_k)\n",
    "qdrant_vectorstore = vectorize_documents(text_splitter, qdrant_vectorstore)\n",
    "llm_model = load_llm(llm)\n",
    "eng_rag_prompt = ChatPromptTemplate.from_template(eng_rag_template)\n",
    "mk_rag_prompt = ChatPromptTemplate.from_template(mk_rag_template)\n",
    "eng_rag_chain = build_RAG_prompt_chain(eng_rag_template, llm_model, retriever, format_docs)\n",
    "mk_rag_chain = build_RAG_prompt_chain(mk_rag_template, llm_model, retriever, format_docs)\n",
    "rag_chains = {\"engineering\": eng_rag_chain, \"marketing\": mk_rag_chain}\n",
    "metrics = ['rouge', 'bleu', 'bertscore']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DWswRMTXhPy8",
    "outputId": "536ad0c7-63aa-4681-f06f-240fee0a0116"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Question: Considering the structure and content of the provided text, what guidelines should be used to evaluate the effectiveness of a summary or chatbot response in this context?\n",
      " \n",
      "Engineering Prediction: To evaluate the effectiveness of a summary or chatbot response in this context, one should consider the following guidelines: a concise and precise comparison of the two responses should be made, highlighting the differences in structure, content, and effectiveness in addressing the user query. The evaluation should also consider the relevance and faithfulness of the responses to the original text, ensuring that important points are not omitted or distorted. Finally, the evaluation should take into account the research focus, assessing whether the responses provide insightful and accurate summaries that reflect an understanding of the technical aspects discussed in the text, such as fine-tuning LMs using PPO on human preferences and the comparison of different methods through sampling completions.\n",
      "Engineering Answer: To evaluate the effectiveness of a summary or chatbot response in this context, guidelines should include assessing the faithfulness of the answer to the retrieved context, the relevance of the answer to the question, and the focus of the retrieved context. Additionally, consider using quality metrics such as answer relevancy to rank responses based on how directly they address the question and avoid redundant or incomplete information. Lastly, take into account the performance of different tasks such as summarization, citation prediction, and passage ranking to determine the overall effectiveness of the response.\n",
      "\n",
      "Marketing Prediction: To evaluate the effectiveness of a summary or chatbot response in this context, the following guidelines should be considered: The response should be concise, relevant, and faithful to the source material, reflecting the most important points accurately while also considering user preferences and providing helpful information.\n",
      "Marketing Answer: Answer: Evaluate based on faithfulness, answer relevance, and context relevance.\n",
      "------------------------------------------------------------\n",
      "Question: What are some recent methods and technologies that have been developed to enhance the capabilities and performance of natural language processing models?\n",
      " \n",
      "Engineering Prediction: Recent advancements in natural language processing (NLP) have led to the development of innovative methods and technologies to enhance the capabilities and performance of NLP models. One notable trend is the emergence of techniques such as prompt engineering, which includes methods like AutoPrompt, Prefix-Tuning, P-tuning, and Prompt-Tuning. These techniques aim to improve the performance of NLP models by providing additional context or guidance in the form of prompts, enabling models to better understand and generate responses. Additionally, there is a growing interest in exploring language models' potential to utilize tools and perform autonomous scientific research, as highlighted in recent studies. The field is also witnessing a focus on efficient vector search algorithms and the intriguing human-computer interaction aspect of designing interfaces for labelers to provide feedback to language models. Furthermore, research is being conducted on data augmentation techniques, not only for language models but also for computer vision models, underscoring the interdisciplinary nature of current NLP advancements.\n",
      "Engineering Answer: Recent methods and technologies developed to enhance natural language processing models include retrieval-augmented multimodal language modeling, which outperforms existing models with less training data and parameters. Another advancement is the use of feature learning in infinite-width neural networks to improve performance. Additionally, embedding techniques in NLP have been developed to map words or phrases to real number vectors, enhancing the model's understanding of language. These innovations have led to improvements in tasks like query reformulation, document ranking, and fine-tuning larger language models for various applications.\n",
      "\n",
      "Marketing Prediction: Recent methods to enhance natural language processing (NLP) include advances in context representation learning, machine translation, and language modeling, with a trend towards developing techniques like prompt engineering and retrieval-augmented methods to improve performance and controllability in text generation tasks.\n",
      "Marketing Answer: Recent methods and technologies include retrieval-augmented language models, feature learning in infinite-width neural networks, and word embeddings.\n",
      "------------------------------------------------------------\n",
      "Question: What are some potential directions for future work mentioned in the document related to enhancing question-answering techniques for document-oriented tasks?\n",
      " \n",
      "Engineering Prediction: Some potential directions for future work in enhancing question-answering techniques for document-oriented tasks include developing multi-modal approaches that can incorporate table and figure information, improving question type incorporation, and enhancing alignment with human preferences for document question-answering. The goal is to develop techniques that can effectively aggregate content from multiple sources and improve latent retrieval and evidence document selection.\n",
      "Engineering Answer: One potential direction for future work mentioned in the document is the development of multi-modal approaches that incorporate table and figure information into GPT-4 question-answering for documents. Another direction is to incorporate question type in the PDFTriage approach to improve the efficiency and efficacy of the approach. Additionally, the document suggests further research in document-grounded, information-seeking question answering, which the dataset is designed to facilitate.\n",
      "\n",
      "Marketing Prediction: Developing multi-modal approaches that incorporate table and figure information, as well as improving alignment with human preferences for document question-answering, are suggested as future directions to enhance question-answering techniques for document-oriented tasks.\n",
      "Marketing Answer: Some potential future directions mentioned in the document include developing multi-modal approaches that incorporate table and figure information into question-answering for documents, and incorporating question type in the PDFTriage approach to improve efficiency and efficacy.\n",
      "------------------------------------------------------------\n",
      "Question: What information would you expect to find in section 2 of a document, based on the types of questions classified under Summarization?\n",
      " \n",
      "Engineering Prediction: Section 2 of the document, as indicated by the table of contents, is likely to provide an overview of related works and existing research in the field. This section typically serves as a literature review, summarizing and analyzing previous studies and their contributions to the specific research area. It helps set the foundation and provide context for the subsequent sections, including the introduction of new methodologies, experiments, or proposed solutions. The types of questions classified under summarization in the provided excerpt suggest a focus on understanding the content of the document, identifying key aspects, and distilling the information into a concise form. Therefore, in section 2, one could expect to find a comprehensive review of relevant research literature, including an analysis of the datasets used, evaluation metrics employed, and any notable findings or advancements in the field of document question-answering and summarization techniques.\n",
      "Engineering Answer: Based on the types of questions classified under Summarization, you would expect to find key takeaways, concise summaries, and specific content extraction related to different sections of the document in section 2. The section likely contains detailed summaries of specific parts of the document, along with structured metadata representation and instructions for summarizing the content effectively. It may also include guidelines for extracting specific information and rewriting text for clarity and conciseness.\n",
      "\n",
      "Marketing Prediction: Section 2 of the document is likely to provide an overview of related work, including a discussion of tool and retrieval-augmented LLMs, and may also include examples and features of datasets used for summarization tasks, such as news article summarization with ROUGE-L scores as evaluation metrics.\n",
      "Marketing Answer: Based on the types of questions classified under Summarization, you would expect to find key takeaways, concise summaries, and specific content extraction related to the document in section 2.\n",
      "------------------------------------------------------------\n",
      "Question: What are the main advantages and attention mechanisms that contribute to the enhanced performance and efficiency of the newly introduced language model as compared to its predecessors?\n",
      " \n",
      "Engineering Prediction: The newly introduced language model exhibits enhanced performance and efficiency through its attention mechanisms and the incorporation of augmented language models. The model's attention techniques, such as query and sliding window attention, enable it to focus on specific parts of the input sequence, improving its ability to understand and generate context-aware responses. Additionally, the integration of augmented language models enhances its reasoning skills and external tool usage, contributing to its improved performance. The combination of these attention mechanisms and augmented language models allows the newly introduced model to outperform its predecessors in various benchmarks, showcasing its effectiveness and efficiency in language processing tasks.\n",
      "Engineering Answer: The main advantages of the newly introduced language model include utilizing retrieval-augmentation to incorporate external knowledge, which improves prediction accuracy. Additionally, the model employs attention mechanisms that allow for better understanding of dependencies between source and target sequences, leading to more informed predictions. These attention mechanisms have been extended from machine translation to various other fields, enhancing the model's adaptability and performance across different tasks. Finally, the model's use of self-attention mechanisms enables better contextual representation learning, parallelization, and modeling of longer intra-token relations, improving efficiency and performance compared to previous models.\n",
      "\n",
      "Marketing Prediction: The newly introduced language model enhances performance and efficiency through improved attention mechanisms, such as query and sliding window attention, which allow the model to focus on specific parts of the text and outperform other language models in various benchmarks.\n",
      "Marketing Answer: The main advantages of the newly introduced language model include the use of retrieval-augmented mechanisms, attention mechanisms, and context representation learning, which contribute to enhanced performance and efficiency compared to its predecessors.\n",
      "------------------------------------------------------------\n",
      "Question: What criteria are used to assess the quality of recommendations provided by different language models in a comparison study?\n",
      " \n",
      "Engineering Prediction: The quality of recommendations from language models is assessed using a range of criteria, including helpfulness, honesty, and overall quality. Helpfulness evaluates how well a model assists users in completing their tasks, while honesty considers whether the model fabricates information or misleads the user. Overall quality takes into account factors such as grammar, coherence, and relevance to the user's query. Additionally, labelers are employed to rank model completions based on their preferences, providing a subjective evaluation of the recommendations. This process considers various stakeholders, such as the organization training the model and the end users, to ensure the language model's recommendations are effective and beneficial for all parties involved.\n",
      "Engineering Answer: In a comparison study of language models, criteria such as sentence relevance, lexical accuracy, and contextual understanding are used to assess the quality of recommendations. Different tasks may benefit from different evaluation measures, such as STRINC, LEXICAL, and CXMI. Additionally, template selection plays a vital role in the quality of recommendations, with deliberate template design being important for tasks like query suggestion. The overall quality of recommendations is often judged using a Likert scale, along with metadata collection for each model output.\n",
      "\n",
      "Marketing Prediction: The quality of recommendations from language models is assessed through rankings based on helpfulness, honesty, and overall quality, with an emphasis on real-world applicability and problem-solving.\n",
      "Marketing Answer: The criteria used to assess the quality of recommendations provided by different language models in a comparison study include comparing to human-created benchmarks, examining intrinsic character, comparing two models, investigating rate of learning, and analyzing learning curves.\n",
      "------------------------------------------------------------\n",
      "Question: What approaches have been proposed to enhance the task performance of language models while considering the trade-offs such as runtime efficiency, robustness to irrelevant context, and attribution quality?\n",
      " \n",
      "Engineering Prediction: Customized templates and diverse task descriptions have been shown to improve the performance of language models, with careful design achieving efficient inference and enhanced generalizability. This approach, known as instruction tuning, has been applied to GPT-4, demonstrating the effectiveness of tailored guidance in enhancing language model capabilities while considering trade-offs like runtime efficiency and robustness to irrelevant context. Additionally, research has explored the use of ResNet, going beyond kernels, to further improve the efficiency and performance of language models, particularly in generative language model inference. These techniques collectively contribute to the ongoing efforts in optimizing the performance of language models while navigating the inherent trade-offs in the field.\n",
      "Engineering Answer: Several approaches have been proposed to enhance the task performance of language models while considering trade-offs. These include using compression and selective augmentation methods to decrease the propensity of models to generate toxic or biased outputs. Adversarial setups have been suggested where labelers find worst-case behaviors of the model and add them to the dataset. Additionally, models like BART and T5 leverage bi-directional attention to achieve stronger performance on both discriminative and generative tasks. These methods aim to balance model performance with considerations such as runtime efficiency, robustness to irrelevant context, and attribution quality.\n",
      "\n",
      "Marketing Prediction: Customized templates, diverse task descriptions, and instruction tuning improve performance and generalizability while maintaining efficient inference in language models.\n",
      "Marketing Answer: Approaches proposed to enhance language model task performance include compression and selective augmentation, adversarial set-ups for labeling worst-case behaviors, retrieval-augmented models, and extending existing models to enable length extrapolation while maintaining quality.\n",
      "------------------------------------------------------------\n",
      "Question: What metrics are commonly used to compare the performance of language models in various tasks, as outlined in an experimental results table?\n",
      " \n",
      "Engineering Prediction: A range of metrics are used to evaluate language models across various tasks, as seen in Table 1 and Table 2, including EM (Exact Match), F1 score, Accuracy, and ROUGE-1/ROUGE-2/ROUGE-L scores. These metrics provide a quantitative measure of the model's performance and are essential for comparing different models and approaches in natural language processing research.\n",
      "Engineering Answer: Common metrics used to compare the performance of language models in various tasks, as outlined in an experimental results table, include Exact Match and Unigram F1. These metrics have become standard in evaluating language models. Additionally, other metrics such as BLEU score, FactScore (factuality), precision, and recall are also commonly used to assess the performance of language models across different tasks. It is important to consider a variety of metrics to get a comprehensive understanding of the effectiveness of a language model in different contexts.\n",
      "\n",
      "Marketing Prediction: ROUGE-1, ROUGE-2, and ROUGE-L scores are commonly used metrics for evaluating and comparing the performance of language models across various tasks, as seen in Table 6 of the provided context.\n",
      "Marketing Answer: The metrics commonly used to compare the performance of language models in various tasks are Exact Match and Unigram F1.\n",
      "------------------------------------------------------------\n",
      "Question: What is the role of manual assessment in the validation of language model predictions according to the text provided?\n",
      " \n",
      "Engineering Prediction: Manual assessment plays a crucial role in validating the predictions of language models by providing a qualitative evaluation of the model's performance. It involves human annotators reviewing the model's responses and making subjective judgments about their quality, including factors such as toxicity, bias, and hallucination. This process is essential for understanding the limitations of automatic evaluations and gaining insights into the model's alignment with human intent and natural language capabilities.\n",
      "Engineering Answer: Manual assessment plays a crucial role in the validation of language model predictions. The engineers evaluate the quality of model outputs by having labelers rate them on test sets consisting of prompts from held-out customers. This manual assessment helps ensure that the models are aligned with a broad distribution of language tasks and can identify any behavioral issues that may arise from misalignment. Additionally, human annotators find that certain reflection token predictions are aligned with their assessments, providing valuable insights into the accuracy and effectiveness of the models.\n",
      "\n",
      "Marketing Prediction: Manual assessment is crucial for validating language model predictions by evaluating the model's alignment with user intent, checking for toxic or biased responses, and ensuring the information provided is factual and not hallucinated.\n",
      "Marketing Answer: Answer: Manual assessment plays a key role in evaluating the quality of language model predictions by having labelers rate the model outputs and comparing them to prompts from held-out customers.\n",
      "------------------------------------------------------------\n",
      "Question: What are the general steps outlined for training a language model in the document, and how is the training data for the generator language model collected and utilized?\n",
      " \n",
      "Engineering Prediction: The training process for the language model involves optimizing a loss function that comprises two components: the main loss (*Lmain*) and the retrieval loss (*Lretr*). The main loss focuses on the generator module's likelihood of generating the target sequence, while the retrieval loss pertains to the likelihood of retrieving the relevant documents. The training data for the generator language model is collected through supervised data collection, specifically for English-to-German translation with a vocabulary of 37k based on a joint source and target byte pair encoding. This training data is utilized to optimize the model's performance during its learning process.\n",
      "Engineering Answer: The document outlines the general steps for training a language model, including incorporating retrieved documents into the main input sequence and optimizing the loss function to train the generator. The training data for the generator language model is collected through various techniques such as supervised fine-tuning, critic learning, and custom retrievers for downstream tasks. The collected data is used to train the generator on specific tasks like summarization, machine reading comprehension, and natural language to SQL translation, improving performance on those tasks.\n",
      "\n",
      "Marketing Prediction: The training process involves optimizing a loss function that combines the main task loss and a retrieval loss, with the training data for the generator collected via supervised data collection methods and utilized to fine-tune the model.\n",
      "Marketing Answer: The general steps for training a language model include fine-tuning on specific datasets, filtering pretraining data, and using critic learning. Training data for the generator language model is collected from open-access NLP papers and used for downstream conditional text generation tasks.\n",
      "------------------------------------------------------------\n",
      "Question: What are the three main categories used to refine language model abilities in understanding and executing search tasks according to the given document?\n",
      " \n",
      "Engineering Prediction: The three main categories employed to enhance language models' capabilities in understanding and executing search tasks are: enhancing document understanding, utilizing concurrent methods trained with retrieved text, and employing text-generation models to produce task outputs and self-critique.\n",
      "Engineering Answer: The three main categories used to refine language model abilities in understanding and executing search tasks are query understanding, document understanding, and query-document relationship understanding. Tasks within these categories focus on interpreting queries, comprehending documents, and understanding the relationships between queries and documents. This approach aims to enhance the models' performance in interpreting and responding to search-related instructions effectively, improving their utility in complex information retrieval scenarios.\n",
      "\n",
      "Marketing Prediction: The three main categories used to enhance language models' search capabilities are: enhancing document understanding, utilizing concurrent methods with retrieved text, and applying critique and selection processes.\n",
      "Marketing Answer: The three main categories used to refine language model abilities in understanding and executing search tasks are query understanding, document understanding, and query-document relationship understanding.\n",
      "------------------------------------------------------------\n",
      "Question: What are some of the emerging research topics and challenges in the field of natural language processing and information retrieval according to recent academic conferences and publications?\n",
      " \n",
      "Engineering Prediction: Recent academic conferences and publications highlight several emerging research topics and challenges in natural language processing (NLP) and information retrieval (IR). There is a growing interest in knowledge-intensive NLP tasks, leveraging large language models, and improving question-answering systems. A key challenge is addressing semantic drift in question generation, which can be tackled through iterative retrieval from knowledge bases and text corpora. Additionally, there is a focus on advancing image classification and speech recognition through NLP techniques. The field also explores ethical considerations and the responsible deployment of NLP and IR technologies, emphasizing the need for explainability and fairness in these systems.\n",
      "Engineering Answer: Recent academic conferences and publications have highlighted emerging research topics and challenges in natural language processing and information retrieval. Some key areas of focus include efficient retrieval augmented generation, unsupervised dense information retrieval with contrastive learning, citation-informed transformers, and knowledge refinement via interaction between search engines and large language models. Additionally, challenges such as zero-shot retrieval, semantic search using GPT sentence embeddings, and prompt-based effective input reformulation for legal case retrieval have been identified as important research directions. These topics reflect the ongoing advancements and complexities in the field, driving innovation and progress in NLP and IR research.\n",
      "\n",
      "Marketing Prediction: Recent academic conferences and publications highlight that addressing semantic drift, leveraging large language models for open-domain question answering, and combining knowledge bases with text for question generation are among the emerging research topics in NLP and information retrieval, with a focus on improving accuracy and efficiency in real-world applications.\n",
      "Marketing Answer: Some emerging research topics and challenges in the field of natural language processing and information retrieval include efficient generation from unstructured knowledge, semantic code search evaluation, unsupervised dense information retrieval, context-aware document term weighting, knowledge refinement through interaction with large language models, and investigating the effectiveness of large language models in search re-ranking.\n",
      "------------------------------------------------------------\n",
      "Question: Question: How do models with different fine-tuning strategies compare in terms of accuracy and F1 score for fact verification tasks?\n",
      " \n",
      "Engineering Prediction: The performance of models with different fine-tuning strategies varies across fact-verification tasks. While the RAG-Sequence model excels in incorporating knowledge, it falls short in retaining factual information, as indicated by lower regular F1 scores compared to RAG-Token. The models demonstrate good generalizability among datasets, with superior performance on the FEVER dataset for accuracy and F1 score. However, there is a significant gap when compared to human performance, with models lagging by 27 F1 points for answering questions and 32 F1 points for selecting evidentiary paragraphs. This highlights the ongoing challenge of fact verification for language models and the need for continued research and improvement.\n",
      "Engineering Answer: Models with different fine-tuning strategies are compared in terms of accuracy and F1 score for fact verification tasks. The introduction of LLMs has led to notable developments, with some studies leveraging prompting methods to apply LLMs in IR tasks. However, not all LLMs consistently outperform fine-tuned smaller models. For example, RankGPT based on gpt-3.5-turbo underperforms monoBERT in certain scenarios. Fine-tuning is not strictly necessary for models like GPT3, which has been evaluated on closed book question answering tasks without any updates or fine-tuning.\n",
      "\n",
      "Marketing Prediction: The models with different fine-tuning strategies vary in accuracy and F1 scores for fact-verification tasks, with the best model still lagging 27-32 F1 points behind human performance.\n",
      "Marketing Answer: Models with different fine-tuning strategies have shown mixed results in terms of accuracy and F1 score for fact verification tasks. Some studies have found that large language models (LLMs) outperform smaller fine-tuned models, while others have reported inconsistent performance. Factors such as task complexity and the need for prompt methods to apply LLMs in information retrieval tasks can also impact the comparison.\n",
      "------------------------------------------------------------\n",
      "Question: What components does a fact verification task typically involve in order to assess the accuracy of a given statement?\n",
      " \n",
      "Engineering Prediction: A fact verification task typically involves assessing the relationship between a given statement and relevant materials to determine its accuracy. This entails a deep understanding of the context and semantics of the statement, as well as the ability to reason and evaluate the evidence. The goal is to verify the correctness and factuality of the statement, ensuring it is supported by the provided information. This process often requires a holistic evaluation, taking into account the nuances and potential challenges of the specific task and dataset.\n",
      "Engineering Answer: A fact verification task typically involves assessing the relationship between a claim and the evidence provided, analyzing if there is enough information for a conclusive judgment. This task requires a detailed understanding of the claim and evidence to determine if it is supported or refuted. The use of performance metrics based on including gold answers in model generations instead of exact matching can help search engines deliver accurate and relevant results. Additionally, incorporating lexical measures and verification functions can aid in determining the accuracy of statements.\n",
      "\n",
      "Marketing Prediction: A fact verification task typically involves examining the relationship between a claim and relevant evidence to determine its accuracy, requiring an understanding of the interplay between these components to reach a conclusion.\n",
      "Marketing Answer: A fact verification task typically involves assessing the relationship between a claim and supporting evidence to determine accuracy.\n",
      "------------------------------------------------------------\n",
      "Question: What are the key factors that determine the performance of HALO-aligned models compared to non-HALO models, according to the results presented in the analysis?\n",
      " \n",
      "Engineering Prediction: The performance of HALO-aligned models compared to non-HALO models is influenced by several key factors. Firstly, the results show that HALO-aligned models can match or even outperform non-HALO models at certain scales, particularly at larger model sizes of 13 billion parameters and above, where the performance gap becomes statistically significant. Secondly, effective fine-tuning techniques are crucial to mitigate performance degradation, as any alignment method with a high \"alignment tax\" would be impractical for real-world applications. Thirdly, the choice of the alignment method matters; for instance, CSFT, a simple alignment method, underperforms compared to HALO-aligned models at larger scales. Finally, the evaluation metric and dataset can impact the performance; in this case, the ISUSE dataset proved challenging for both HALO and non-HALO models, indicating that further improvements are needed to enhance their performance on this specific task.\n",
      "Engineering Answer: According to the analysis presented, the key factors that determine the performance of HALO-aligned models compared to non-HALO models include the specific alignment method used (such as DPO and PPO variant), the model size (significant gap at 13B+ model sizes), and the ability to match or exceed the generation quality of SFT target sequences. Additionally, the study suggests that the cost of increasing model alignment is modest relative to pretraining, and that the modeling of human biases in HALOs may have practical benefits in improving overall performance.\n",
      "\n",
      "Marketing Prediction: The performance of HALO-aligned models is influenced by model size and alignment technique, with larger models and certain alignment methods resulting in improved performance over non-HALO models, particularly at scales above 13 billion parameters.\n",
      "Marketing Answer: The key factor that determines the performance of HALO-aligned models compared to non-HALO models is the model size, with HALO-aligned models generally outperforming non-HALO models at larger sizes (13B+ model sizes).\n",
      "------------------------------------------------------------\n",
      "Question: How does the performance of KTO compare to DPO in model alignment, and what are the potential implications for data usage and training efficiency?\n",
      " \n",
      "Engineering Prediction: KTO exhibits superior performance compared to DPO in model alignment tasks, as evident in experiments with the Mistral-7B model on the OpenAssistant dataset. KTO, even with restricted training data, outperforms DPO-aligned models, suggesting enhanced data efficiency. This advantage may be attributed to the preference likelihood aspect of KTO, which optimizes for prospect theory and improves alignment. The implications of these findings are significant for training efficiency and data usage, as KTO can achieve comparable or better results without the need for supervised fine-tuning, thus streamlining the alignment process and reducing the reliance on extensive labeled data.\n",
      "Engineering Answer: Based on the provided data and experiments, KTO consistently outperforms DPO in model alignment, even with restrictions such as using only one output per input. This suggests that KTO can achieve higher win rates and improve performance across various benchmarks compared to DPO. The implications of this performance difference include the ability to achieve quality generation results with significantly fewer desirable examples, potentially leading to more efficient data usage and training processes. This indicates that KTO may offer a more efficient and effective approach to model alignment compared to DPO.\n",
      "\n",
      "Marketing Prediction: KTO matches or surpasses DPO in model alignment tasks, offering improved performance and potential data efficiency benefits, which may reduce the data requirements for effective model training.\n",
      "Marketing Answer: KTO outperforms DPO in model alignment with up to 90% fewer examples. This suggests that KTO can achieve high performance even with imbalanced data, potentially leading to more efficient training processes.\n",
      "------------------------------------------------------------\n",
      "Question: What are some common approaches to building an open-domain question answering system?\n",
      " \n",
      "Engineering Prediction: Common approaches to building an open-domain question-answering system involve retrieval and generation methods. The system first retrieves relevant information from a large knowledge base, such as Wikipedia, and then generates answers based on the retrieved content. The accuracy of the system depends on the quality of both the retrieval and generation processes. This type of approach is particularly useful for open-domain knowledge-grounded dialogue, where the system needs to understand and generate responses within the context of an entire dialogue, and not just answer standalone questions.\n",
      "Engineering Answer: Some common approaches to building an open-domain question answering system include using the RAG model, which minimizes the negative log-likelihood of answers, and comparing it to extractive QA paradigms that rely on non-parametric knowledge retrieval. Another approach is to incorporate question rewriting techniques to make open-domain QA more conversational. Additionally, utilizing datasets like QASPER, which contain questions requiring complex reasoning, can improve the performance of the system. References to papers by Anantha et al. and Asai et al. provide further insights into building ODQA systems.\n",
      "\n",
      "Marketing Prediction: Common approaches to building an open-domain question-answering system involve utilizing large language models, trained on vast datasets, to retrieve and generate answers from a knowledge base, with an emphasis on both accurate retrieval and generation for effective responses.\n",
      "Marketing Answer: Common approaches to building an open-domain question answering system include using retrieval over a knowledge base and incorporating the retrieved content as part of the prompt. Other methods involve pretraining models on large amounts of text data and fine-tuning them for question answering tasks.\n",
      "------------------------------------------------------------\n",
      "Question: What is the difference between open-book and closed-book question answering?\n",
      " \n",
      "Engineering Prediction: Open-book question answering utilizes external sources of knowledge, such as Wikipedia, to generate answers, whereas closed-book question answering does not rely on external sources and instead generates responses based solely on the internal knowledge of the model.\n",
      "Engineering Answer: Open-book question answering involves the use of external sources of knowledge, such as Wikipedia, to retrieve information and generate a response. In contrast, closed-book question answering relies on pre-trained language models that have memorized factual knowledge within their parameters to generate responses without explicit context. Closed-book QA can be seen as analogous to a closed-book exam where no external resources are allowed. The key distinction lies in the reliance on external knowledge sources for open-book QA versus internal memorized knowledge for closed-book QA.\n",
      "\n",
      "Marketing Prediction: Open-book question answering utilizes external knowledge sources, like Wikipedia, to provide answers, whereas closed-book question answering does not rely on external sources and instead generates responses based solely on the information provided in the question or prompt.\n",
      "Marketing Answer: Open-book question answering involves using external sources of knowledge to answer questions, while closed-book question answering relies on pre-trained language models to provide answers without explicit context.\n",
      "------------------------------------------------------------\n",
      "Question: What are the basic components of the Retriever-Reader framework in open-domain QA?\n",
      " \n",
      "Engineering Prediction: The Retriever-Reader framework in open-domain QA consists of two main components: the retriever and the reader. The retriever is responsible for information retrieval, often utilizing BERT-based dense vectors to find relevant passages or contexts. The reader then applies machine reading comprehension techniques to extract the answer from the retrieved context, focusing on answer and context relevance. The joint training of these components maximizes the marginal likelihood of correct answers, with an emphasis on answer grounding and relevance to the original question.\n",
      "Engineering Answer: The basic components of the Retriever-Reader framework in open-domain QA include a retriever model, which fetches relevant information based on input prompts efficiently using FAISS. The retriever component is responsible for retrieving contextually relevant documents or evidence blocks based on the input question. The reader component then processes this retrieved information to generate answers to the questions posed. This framework combines information retrieval and machine reading comprehension to achieve state-of-the-art results in open-domain question answering tasks.\n",
      "\n",
      "Marketing Prediction: The Retriever-Reader framework in open-domain QA consists of two stages: retrieval, where relevant information is gathered, and reading, where machine reading comprehension techniques are applied to extract the answer from the retrieved context.\n",
      "Marketing Answer: The basic components of the Retriever-Reader framework in open-domain QA are the retriever and the reader components, which can be set up and trained independently or jointly trained end-to-end. The retriever component automatically fetches relevant information based on input prompts, while the reader component processes and comprehends the retrieved information to answer questions.\n",
      "------------------------------------------------------------\n",
      "Question: How is the TF-IDF model used in question answering retrieval systems?\n",
      " \n",
      "Engineering Prediction: The TF-IDF model is a vital component in question-answering retrieval systems, specifically in the context of open-domain QA datasets like TriviaQA and PopQA. In these systems, the TF-IDF-based cosine similarity metric is employed to rank paragraphs within documents, with each document split into paragraphs of 6 sentences. The model calculates the similarity between evidence paragraphs and the query, selecting the most relevant paragraph to generate an answer. This process addresses the challenges of answer grounding, answer relevance, and context relevance, contributing to the development of effective evaluation frameworks for question-answering retrieval models.\n",
      "Engineering Answer: In question answering retrieval systems, the TF-IDF model is used to represent queries and documents as bag-of-word vectors with terms weighted by term frequency multiplied by inverse document frequency. This allows for efficient non-learning-based search engine operations based on the vector space model. The TF-IDF model helps in calculating the relevance of documents to queries by measuring the importance of terms in the context of the entire document collection. This classic information retrieval approach aids in retrieving relevant information to answer questions accurately and efficiently.\n",
      "\n",
      "Marketing Prediction: The TF-IDF model is used in question-answering retrieval systems to rank paragraphs based on their relevance to a given query, with the most relevant paragraph then used to generate an answer.\n",
      "Marketing Answer: The TF-IDF model is used in question answering retrieval systems to weight terms in queries and documents based on their importance in determining relevance.\n",
      "------------------------------------------------------------\n",
      "Question: Can neural networks enhance the process of information retrieval in QA systems?\n",
      " \n",
      "Engineering Prediction: Yes, neural networks can significantly enhance the information retrieval process in QA systems, as evidenced by the performance gains achieved by models such as RAG and FiD. This enhancement is primarily due to the introduction of neural retrieval, also known as \"neural IR,\" which has been shown to dramatically improve results in open-domain QA tasks when compared to non-neural retrievers like BM25. By conditioning retrieval on the output of a generative model, relevant facts and information can be surfaced to improve the overall effectiveness of the QA system, especially in more challenging tasks like knowledge-grounded dialogue.\n",
      "Engineering Answer: Neural networks, such as MLP, LSTM, and bidirectional LSTM, can be used to learn dense representations of text for information retrieval in QA systems. These approaches, known as 'Neural IR', are a new category of methods that can improve performance in retrieval problems. The introduction of neural retrievers in recent QA literature has shown to outperform traditional word-similarity-based architectures, such as BM25, and can scale to handle knowledge-grounded dialogue tasks effectively. Additionally, incorporating pre-trained retrievers in QA systems has been shown to enhance the performance of generative language models.\n",
      "\n",
      "Marketing Prediction: Yes, neural networks can enhance information retrieval in QA systems by improving the accuracy and efficiency of fact-finding, which is essential for generating knowledgeable and factual responses.\n",
      "Marketing Answer: Yes, neural networks can enhance the process of information retrieval in QA systems by improving performance in open-domain QA tasks and enabling the generation of more accurate answers.\n",
      "------------------------------------------------------------\n",
      "Question: What is the importance of fine-tuning in the context of QA data for open-domain question answering models?\n",
      " \n",
      "Engineering Prediction: Fine-tuning is an important technique for open-domain question-answering models as it enables the adaptation of pre-trained language models to specific QA datasets and improves performance on target tasks. It is particularly beneficial when the target datasets differ significantly from the pre-training data, ensuring the model aligns with the specific QA task and enhancing its ability to provide accurate and relevant responses. While not always necessary, fine-tuning can improve performance on public NLP datasets and is a key method to improve model alignment and domain diversity.\n",
      "Engineering Answer: Fine-tuning is important in the context of QA data for open-domain question answering models because it allows the model to adapt and improve its performance on specific QA datasets. By fine-tuning the model with common QA datasets, engineers can optimize the model's ability to answer questions accurately. However, there is a concern about the significant overlap between questions in the train and test sets of public QA datasets, which could affect the generalization ability of the fine-tuned models. Engineers should carefully consider this overlap and potentially explore ways to mitigate its impact during the fine-tuning process to ensure the model's effectiveness in real-world applications.\n",
      "\n",
      "Marketing Prediction: Fine-tuning is not always necessary for open-domain question answering models, but it can improve performance and adaptability to specific tasks and domains, enhancing accuracy and diversity in model outputs.\n",
      "Marketing Answer: Fine-tuning is important in the context of QA data for open-domain question answering models to improve search task performance and the ability to generalize to unseen datasets.\n",
      "------------------------------------------------------------\n",
      "Question: How does pre-training with tasks like the Inverse Cloze Task benefit open-domain question answering models?\n",
      " \n",
      "Engineering Prediction: Pre-training with tasks like the Inverse Cloze Task (ICT) benefits open-domain question-answering models by enhancing their ability to retrieve and generate relevant information. ICT helps the model to learn to predict a given context sentence based on its surrounding sentences, improving its understanding of language semantics and context. This pre-training task enables the model to better align and steer its responses, resulting in more accurate and contextually relevant answers. The effectiveness of ICT lies in its ability to teach the model to capture semantic relationships and understand context, which are crucial for open-domain question answering, where the model must generate answers without explicit context.\n",
      "Engineering Answer: Pre-training with tasks like the Inverse Cloze Task benefits open-domain question answering models by improving the retrieval process over a knowledge base. By predicting the context given a sentence, the model can better understand the relationship between the question and the evidence. This approach helps in incorporating retrieved content effectively into the prompt, leading to higher accuracy in the question answering task. Additionally, using models pretrained with ICT can enhance the overall performance of the QA system by providing a better understanding of the context.\n",
      "\n",
      "Marketing Prediction: Pre-training with tasks like the Inverse Cloze Task benefits open-domain question-answering models by improving their ability to retrieve and generate accurate responses, enhancing their performance on both in-domain and out-of-domain tasks.\n",
      "Marketing Answer: Pre-training with tasks like the Inverse Cloze Task benefits open-domain question answering models by improving retrieval and generation steps, ultimately enhancing the accuracy of the process.\n",
      "------------------------------------------------------------\n",
      "Question: What is the main goal of prompt engineering in language models?\n",
      " \n",
      "Engineering Prediction: The primary goal of prompt engineering in language models is to enhance alignment and steerability. Prompt engineering techniques aim to align the model's predictions with the user's intentions and objectives by providing carefully crafted prompts or input sequences. This involves optimizing the model's performance and ensuring it generates responses that accurately reflect the desired task or context. Steerability, or the ability to guide the model's behavior, is also improved through prompt engineering, allowing for more precise control over the model's output and enabling it to adapt to different tasks and user requirements.\n",
      "Engineering Answer: The main goal of prompt engineering in language models is to effectively steer the behavior of the model towards desired outcomes without updating the model weights. This is achieved by composing and formatting prompts in a way that maximizes the model's performance on a specific task. Prompt engineering involves treating prompts as trainable parameters and optimizing them directly on the embedding space through methods like AutoPrompt, Prefix-Tuning, P-tuning, and Prompt-Tuning. The ultimate aim is to enhance the model's performance and alignment with user-defined tasks.\n",
      "\n",
      "Marketing Prediction: The primary goal of prompt engineering in language models is to enhance alignment and steerability, enabling the models to better understand and respond to user needs and preferences, and ultimately improving their performance and applicability in various real-world contexts and applications.\n",
      "Marketing Answer: The main goal of prompt engineering in language models is to steer the behavior of the model for desired outcomes without updating the model weights.\n",
      "------------------------------------------------------------\n",
      "Question: What are some known biases that can affect the performance of few-shot classification in LLMs?\n",
      " \n",
      "Engineering Prediction: Known biases that can impact the performance of few-shot classification in large language models include the majority label bias, which occurs when the distribution of labels among examples is uneven, and the length bias, where longer contexts or prompts are favored for learning. These biases contribute to high variance in few-shot classification performance and can be mitigated through techniques like instruction tuning and the use of specialized models, such as the mentioned PPO-ptx model. The research community is actively investigating the impact of these and other biases, aiming to improve the reliability and accuracy of few-shot learning in LLMs.\n",
      "Engineering Answer: Some known biases that can affect the performance of few-shot classification in LLMs include majority label bias, recency bias, and common token bias. Majority label bias occurs when the distribution of labels among examples is unbalanced, recency bias refers to the tendency for the model to repeat the label at the end, and common token bias indicates that LLM tends to produce common tokens more often than rare tokens. These biases can contribute to high variance in few-shot classification tasks and may impact the model's ability to generalize effectively.\n",
      "\n",
      "Marketing Prediction: Known biases that can impact the performance of few-shot classification in large language models include the majority label bias, where the distribution of labels among examples is uneven, and various human biases, such as loss aversion, which are implicitly incorporated into objectives for aligning LLMs with human feedback.\n",
      "Marketing Answer: Some known biases that can affect the performance of few-shot classification in LLMs are majority label bias, recency bias, and common token bias.\n",
      "------------------------------------------------------------\n",
      "Question: Why might increasing model size not reduce variance in model performance with varying prompts?\n",
      " \n",
      "Engineering Prediction: Larger language models have more parameters and are thus expected to benefit more from additional training data and instructions. Smaller models, due to their limited capacity, may not be able to utilize the increased dataset size and instructions as effectively, leading to higher variance in performance when compared to larger models. This issue is further exacerbated in the online inference setting, where batch sizes are typically small, and model parallelism is not employed, requiring heavy experimentation and heuristics to optimize performance.\n",
      "Engineering Answer: Increasing model size may not necessarily reduce variance in model performance with varying prompts because the model's ability to generalize and adapt to different prompts is not solely dependent on its size. Factors such as the quality and relevance of the training examples, the learning rate or schedule, and the model's sensitivity to different hyperparameters can also play a significant role in determining performance variability. Additionally, the complexity of the task or dataset being used for training can impact how effectively the model scales with size. It is essential to consider these factors holistically when optimizing model performance rather than relying solely on increasing model size.\n",
      "\n",
      "Marketing Prediction: Increasing model size does not reduce variance in performance with varying prompts because larger models have greater capacities and are thus less sensitive to changes in prompt engineering methods, which can vary significantly between models and require extensive experimentation.\n",
      "Marketing Answer: Increasing model size may not reduce variance in model performance with varying prompts because the same order of prompts may work well for one model but poorly for another. Additionally, when the validation set is limited, choosing the order of prompts that prevents the model from producing extremely unbalanced predictions or being overconfident can also affect performance.\n",
      "------------------------------------------------------------\n",
      "Question: What is the benefit of instruction-based finetuning in language models?\n",
      " \n",
      "Engineering Prediction: Instruction-based fine-tuning in language models offers the benefit of improved alignment with human intentions and a significant reduction in communication costs. This technique, often achieved through Reinforcement Learning from Human Feedback (RLHF), enhances the model's ability to generalize to unseen domains and tasks by providing instructions and feedback. As a result, the model can better understand and follow instructions, even those that fall outside the distribution of the fine-tuning data. This method has been shown to be effective in improving the model's performance and adaptability, particularly in diverse domains such as geography, biology, and civil comments.\n",
      "Engineering Answer: Instruction-based finetuning improves models' ability to generalize to unseen domains and tasks by providing task-specific representations that can be used for many downstream language tasks without additional training. This method also allows pretrained language models to follow instructions provided in prompts, enabling them to generate the desired output given specific inputs. Additionally, instruction finetuning helps transform raw pretrained LLMs into chatbot-like models, making finetuning more accessible and common, particularly for researchers with limited resources. Overall, the benefit of instruction-based finetuning is improved model performance, enhanced generalizability, and reduced communication costs in aligning with human intentions.\n",
      "\n",
      "Marketing Prediction: Instruction-based fine-tuning improves a language model's ability to understand and follow instructions, enhancing its alignment with human intentions and reducing the cost and complexity of communication.\n",
      "Marketing Answer: The benefit of instruction-based finetuning in language models is improved ability to generalize to unseen domains and tasks, without the need for additional training.\n",
      "------------------------------------------------------------\n",
      "Question: Can you describe a situation where retrieval-based methods would be necessary to enhance language model performance?\n",
      " \n",
      "Engineering Prediction: Retrieval-based methods are necessary when enhancing language model performance in scenarios requiring factual accuracy and longer target sequences. These methods are particularly useful for knowledge-grounded conversational tasks, where the model needs to generate responses based on specific factual knowledge. By incorporating retrieval mechanisms, the model can attend to relevant knowledge sources and improve its performance by providing more accurate and informative responses. This approach also enhances interpretability and explainability, as the provenance of the knowledge used by the model is transparent. However, the implementation of retrieval-augmented strategies requires careful tuning to ensure optimal performance, considering factors such as the retrieval model, corpus, language model, and prompt format.\n",
      "Engineering Answer: Retrieval-based methods are necessary to enhance language model performance in scenarios where the model needs to generate accurate and informative responses for entity-rich queries, such as 'George Washington standing in front of the Eiffel Tower.' In such cases, incorporating a retrieval module can provide additional context and relevant information to improve the model's understanding and generation of the desired output. Additionally, retrieval-based methods are crucial for question answering tasks, where the model needs to access external knowledge sources to provide accurate and comprehensive answers. By utilizing retrieval mechanisms, the language model can benefit from a wider range of information and improve its performance in handling complex and ambiguous queries effectively.\n",
      "\n",
      "Marketing Prediction: Retrieval-based methods are necessary when enhancing language model performance in generating longer sequences or when specific factual knowledge is required, as they provide relevant knowledge and improve interpretability and explainability.\n",
      "Marketing Answer: Retrieval-based methods are necessary to enhance language model performance in tasks like question answering, where incorporating additional information from external sources can improve the model's ability to generate accurate and relevant responses.\n",
      "------------------------------------------------------------\n",
      "Question: What is the Chain-of-Thought prompting technique and for which types of tasks is it particularly beneficial?\n",
      " \n",
      "Engineering Prediction: Chain-of-Thought (CoT) prompting is a technique used to enhance the performance of Large Language Models (LLMs) by generating a sequence of short sentences that describe the step-by-step reasoning logic, also known as reasoning chains or rationales, leading to the final answer. CoT prompting is particularly beneficial for tasks that require higher-order reasoning and complex logic, such as multi-step problem-solving, decision-making, and abstractive question-answering. By explicitly breaking down the reasoning process into a chain of steps, CoT prompting improves the transparency and interpretability of LLMs' outputs, making it easier to identify and correct errors and biases in their decision-making. This technique has been shown to enhance the models' ability to follow instructions and perform human-like tasks accurately.\n",
      "Engineering Answer: Chain-of-Thought (CoT) prompting is a technique that generates reasoning chains or rationales step by step to lead to a final answer, benefiting complicated reasoning tasks using large models with more than 50B parameters. It can be implemented through iterative Monte Carlo search methods or through a three-step process called augment-prune-select. CoT is particularly beneficial for enhancing model performance on complex tasks by decomposing them into smaller and simpler steps, shedding light on the model's thinking process. Task decomposition in CoT can be done with simple prompting, task-specific instructions, or human inputs.\n",
      "\n",
      "Marketing Prediction: Chain-of-Thought (CoT) prompting is a technique that generates a sequence of short, reasoned sentences to explain the logic behind each step of a task, ultimately leading to a final answer. This method is particularly beneficial for tasks requiring complex reasoning, such as problem-solving, decision-making, and creative brainstorming, as it enhances language models' performance and understanding of user intent.\n",
      "Marketing Answer: Chain-of-Thought (CoT) prompting is a technique that generates reasoning chains or rationales step by step to lead to a final answer. It is particularly beneficial for complicated reasoning tasks when using large models with more than 50B parameters. Simple tasks only benefit slightly from CoT prompting.\n",
      "------------------------------------------------------------\n",
      "Question: How do augmented language models with external tools differ from regular models in functionality?\n",
      " \n",
      "Engineering Prediction: Augmented language models that incorporate external tools, such as retrieval-augmented models, aim to enhance the reasoning capabilities and flexibility of regular language models. They achieve this by integrating additional mechanisms or modules that provide extra functionality. In the case of retrieval-augmented models, a retrieval step is added, allowing the model to access and utilize external knowledge sources efficiently. This augmentation equips the model with the ability to reason and make decisions based on a broader context, potentially improving performance on tasks that require external knowledge or understanding longer inputs. The integration of external tools offers a way to extend the capabilities of language models beyond their original design, making them more adaptable to diverse user needs and input lengths.\n",
      "Engineering Answer: Augmented language models with external tools, such as TALM and Toolformer, are fine-tuned to learn how to use external tool APIs, expanding their capabilities beyond traditional language processing tasks. These models are trained to incorporate external tool API calls in order to improve the quality of their outputs, allowing them to perform tasks like speech recognition, machine translation, and information retrieval more effectively. By leveraging external tools, these models have the ability to access and utilize a wider range of resources and functionalities, enhancing their overall performance and versatility compared to regular language models.\n",
      "\n",
      "Marketing Prediction: Augmented language models that incorporate external tools offer enhanced reasoning capabilities and flexibility, allowing users to execute specific tasks and improve question-answering through external tool integration.\n",
      "Marketing Answer: Augmented language models with external tools differ from regular models by fine-tuning a LM to use external tool APIs, expanding the dataset to improve model outputs and enhancing tasks like speech recognition, machine translation, and natural language generation.\n",
      "------------------------------------------------------------\n",
      "Question: What can be inferred about the utilization of attention in neural networks?\n",
      " \n",
      "Engineering Prediction: Attention in neural networks can be interpreted as a vector of importance weights, where each element, such as a pixel or a word, is assigned a weight that signifies its relevance in the prediction or inference process. This is mathematically represented by the attention equation provided, where the softmax function normalizes the product of the query and key vectors, resulting in a probability distribution over the values. The Neural Turing Machine (NTM) is an example of a model that utilizes attention, specifically through content-based and location-based addressing mechanisms to generate attention distributions. The research focus on attention in neural networks aims to enhance the understanding and performance of these models by analyzing aspects such as architecture, theoretical foundations, and specific applications, ultimately improving their ability to process and interpret data.\n",
      "Engineering Answer: Attention mechanisms in neural networks play a crucial role in allowing models to focus on specific parts of input data when making predictions or generating outputs. By assigning importance weights to different elements, such as pixels in an image or words in a sentence, attention helps the model to attend to relevant information and make more accurate predictions. The use of attention can improve the interpretability of neural networks by showing which parts of the input data are being focused on during the prediction process. Additionally, attention mechanisms, like multi-head attention, can enhance model performance by allowing the model to jointly attend to information from different representation subspaces at different positions.\n",
      "\n",
      "Marketing Prediction: Attention in neural networks is a powerful mechanism that assigns importance weights to elements, enabling the network to focus on relevant features for prediction and inference tasks, such as image recognition or natural language processing.\n",
      "Marketing Answer: Attention in neural networks allows the model to focus on specific parts of input data, such as images or text, in order to make predictions or generate output. It helps the model to learn relationships and correlations between different elements and improve performance in tasks like image captioning or language translation.\n",
      "------------------------------------------------------------\n",
      "Question: Can the use of attention mechanisms in deep learning models be applied to both machine translation and computer vision?\n",
      " \n",
      "Engineering Prediction: Yes, attention mechanisms in deep learning models have proven effective in both machine translation and computer vision tasks. By assigning importance weights to elements, attention methods enable models to capture dependencies between source and target sequences, enhancing performance in machine translation. This success prompted its extension into computer vision, where it helps mitigate the impact of context window extensions on attention score computations, allowing models to adapt more easily to visual data. Thus, attention mechanisms offer flexibility and improvements across these diverse domains.\n",
      "Engineering Answer: Yes, attention mechanisms in deep learning models have shown success in both machine translation and computer vision tasks. In machine translation, attention allows the model to capture dependencies between source and target sequences regardless of distance, leading to improved translation quality. Similarly, in computer vision, attention mechanisms have been used to focus on relevant parts of an image during caption generation, showcasing the ability to handle details and global dependencies effectively. Therefore, utilizing attention in both domains can enhance the performance of deep learning models significantly.\n",
      "\n",
      "Marketing Prediction: Yes, attention mechanisms in deep learning models have been successfully applied to both machine translation and computer vision tasks, improving performance by capturing dependencies and enabling models to focus on relevant elements.\n",
      "Marketing Answer: Yes, attention mechanisms in deep learning models can be applied to both machine translation and computer vision.\n",
      "------------------------------------------------------------\n",
      "Question: What are the potential benefits of incorporating self-attention mechanisms into Generative Adversarial Networks (GANs)?\n",
      " \n",
      "Engineering Prediction: By incorporating self-attention mechanisms, GANs can capture global dependencies and explicitly learn the relationship between pixels, even those far apart. This enhancement equips GANs with the ability to attend to both near and distant pixel relationships, enabling a better understanding of the content and context within an image. The self-attention technique, originally designed for neural machine translation, creates shortcuts to relevant source information, improving the model's ability to generate or discriminate content. This combination of parametric and non-parametric memory in GANs results in more effective performance on both generative and discriminative tasks, showcasing the potential for a unified model capable of handling a diverse range of challenges.\n",
      "Engineering Answer: Incorporating self-attention mechanisms into GANs can help the generator and discriminator better model relationships between spatial regions, leading to improved generation of detailed and realistic images. This is particularly useful for capturing global dependencies and enhancing the performance of transformer architectures. Additionally, self-attention can enable the model to assess its own predictions after each generated segment, allowing for customizable decoding algorithms to meet specific constraints or user preferences. Overall, self-attention in GANs can enhance detail handling and overall performance.\n",
      "\n",
      "Marketing Prediction: Self-attention mechanisms in GANs can capture global dependencies and contextual relationships, enabling the model to generate more coherent and contextually-aware content, which is beneficial for a range of real-world applications, from creating more realistic images to improving language translation.\n",
      "Marketing Answer: Incorporating self-attention mechanisms into GANs can help the generator and discriminator better model relationships between spatial regions, leading to improved performance in handling details and capturing global dependencies.\n",
      "------------------------------------------------------------\n",
      "Question: How does the transformer model variate from traditional sequence-aligned recurrent architectures?\n",
      " \n",
      "Engineering Prediction: The transformer model is a novel architecture that diverges from traditional sequence-aligned recurrent networks by exclusively employing self-attention mechanisms. Unlike recurrent models, transformers lack sequential structure, instead processing input sequences in parallel, capturing positional information through positional encodings. This unique design enables efficient training and effective handling of long-range dependencies, making transformers particularly useful for tasks requiring attention to positional relationships, such as reinforcement learning and natural language processing.\n",
      "Engineering Answer: The transformer model differs from traditional sequence-aligned recurrent architectures by not having a recurrent or convolutional structure. Instead, it heavily relies on self-attention mechanisms for processing sequences. This lack of recurrence and convolution, even with positional encoding, weakly incorporates sequential order, which can be a drawback for tasks sensitive to positional dependencies. Additionally, the transformer's architecture includes embedding layers, sinusoid-wave-based positional encoding, and softmax and linear layers in the final decoder output to maintain position information and facilitate processing of long sequences efficiently.\n",
      "\n",
      "Marketing Prediction: The transformer model differs from traditional sequence-aligned recurrent architectures by relying solely on self-attention mechanisms, abandoning recurrent or convolutional structures, and only weakly incorporating sequential order.\n",
      "Marketing Answer: The transformer model differs from traditional sequence-aligned recurrent architectures by not having a recurrent or convolutional structure, and instead making heavy use of self-attention. This allows for handling very long sequences efficiently and achieving better performance on tasks involving long texts.\n",
      "------------------------------------------------------------\n",
      "Question: What implications does the concept of a Neural Turing Machine have for the theoretical power of neural networks?\n",
      " \n",
      "Engineering Prediction: The Neural Turing Machine (NTM) architecture combines a neural network with external memory storage, mimicking the design of a Turing machine. This integration enhances the theoretical capabilities of neural networks by providing them with the potential for infinite memory capacity and the ability to generalize to new scenarios more effectively. The NTM's external memory, controlled by the neural network, offers a flexible and expansive storage option, addressing the limitations of finite memory in traditional neural network designs. This architecture enables neural networks to handle more complex tasks and potentially achieve human-like performance in certain areas, particularly in generating responses free from knowledge hallucination.\n",
      "Engineering Answer: The concept of a Neural Turing Machine (NTM) expands the theoretical power of neural networks by incorporating external memory storage, allowing for more complex computations and tasks. This mimics the Turing machine tape, enabling the neural network to control operation heads for reading and writing to the tape. However, the finite memory in NTM suggests it may resemble more of a 'Neural von Neumann Machine,' limiting its mathematical limitlessness seen in traditional Turing machines. Overall, the addition of external memory in NTM enhances the capabilities and potential applications of neural networks in solving more advanced problems.\n",
      "\n",
      "Marketing Prediction: The Neural Turing Machine (NTM) architecture combines neural networks with external memory, mimicking the Turing machine's infinite storage capacity. This enhances the theoretical power of neural networks by enabling them to access and process vast amounts of data, improving their ability to generalize to new scenarios and reducing knowledge hallucination issues in chatbots.\n",
      "Marketing Answer: The concept of a Neural Turing Machine suggests that neural networks can be equipped with external memory storage for more complex operations, potentially increasing their theoretical power.\n"
     ]
    }
   ],
   "source": [
    "final_test_eval = evaluate(metrics, test_dict, rag_chains, iterations=0, verbose=False, dept_specific=True, print_results=True)\n",
    "final_test_eval = composite_evaluation(final_test_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "B7gitRZliEOy",
    "outputId": "f4289cc5-7d3d-4bae-aa75-d9a4abb8c5be"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"final_test_eval\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"Sample\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 30.17806634975593,\n        \"min\": 13.785694692370804,\n        \"max\": 104.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          81.31428571428572,\n          82.0,\n          35.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eng_bleu\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12.335208899075093,\n        \"min\": 0.0,\n        \"max\": 35.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.10243612083909204,\n          0.10212210470714853,\n          35.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mk_bleu\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12.32758782313836,\n        \"min\": 0.0,\n        \"max\": 35.0,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          35.0,\n          0.12235769307714589,\n          0.20338874220857642\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eng_rouge\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12.289939004112744,\n        \"min\": 0.06382732302483582,\n        \"max\": 35.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.2669276778639225,\n          0.25925925925925924,\n          35.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mk_rouge\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12.26522941961702,\n        \"min\": 0.07142857142857142,\n        \"max\": 35.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.3372752098580401,\n          0.3191489361702128,\n          35.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eng_f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12.10815118761157,\n        \"min\": 0.015008229613098934,\n        \"max\": 35.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.8879783153533936,\n          0.8896036148071289,\n          35.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mk_f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12.103667140603505,\n        \"min\": 0.026056508338195394,\n        \"max\": 35.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.9039849860327585,\n          0.9003185629844666,\n          35.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"composite_eng\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12.207777093482143,\n        \"min\": 0.03659619869208779,\n        \"max\": 35.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.544554685203692,\n          0.5437254368315811,\n          35.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"composite_mk\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12.196908715615033,\n        \"min\": 0.07399499222803585,\n        \"max\": 35.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.5776465945892204,\n          0.5669473784167383,\n          35.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"composite_total\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12.20300709753298,\n        \"min\": 0.04373956097411213,\n        \"max\": 35.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.5577914489579033,\n          0.552785446691788,\n          35.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-9af3a080-59d9-4748-9b5c-158fbef2753d\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample</th>\n",
       "      <th>eng_bleu</th>\n",
       "      <th>mk_bleu</th>\n",
       "      <th>eng_rouge</th>\n",
       "      <th>mk_rouge</th>\n",
       "      <th>eng_f1</th>\n",
       "      <th>mk_f1</th>\n",
       "      <th>composite_eng</th>\n",
       "      <th>composite_mk</th>\n",
       "      <th>composite_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>81.314286</td>\n",
       "      <td>0.102436</td>\n",
       "      <td>0.122358</td>\n",
       "      <td>0.266928</td>\n",
       "      <td>0.337275</td>\n",
       "      <td>0.887978</td>\n",
       "      <td>0.903985</td>\n",
       "      <td>0.544555</td>\n",
       "      <td>0.577647</td>\n",
       "      <td>0.557791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>13.785695</td>\n",
       "      <td>0.069939</td>\n",
       "      <td>0.115380</td>\n",
       "      <td>0.063827</td>\n",
       "      <td>0.135472</td>\n",
       "      <td>0.015008</td>\n",
       "      <td>0.026057</td>\n",
       "      <td>0.036596</td>\n",
       "      <td>0.073995</td>\n",
       "      <td>0.043740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>59.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.858473</td>\n",
       "      <td>0.844432</td>\n",
       "      <td>0.473681</td>\n",
       "      <td>0.445593</td>\n",
       "      <td>0.480705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>69.500000</td>\n",
       "      <td>0.070237</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.223808</td>\n",
       "      <td>0.265046</td>\n",
       "      <td>0.876900</td>\n",
       "      <td>0.887983</td>\n",
       "      <td>0.521151</td>\n",
       "      <td>0.527932</td>\n",
       "      <td>0.530832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>82.000000</td>\n",
       "      <td>0.102122</td>\n",
       "      <td>0.109031</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.319149</td>\n",
       "      <td>0.889604</td>\n",
       "      <td>0.900319</td>\n",
       "      <td>0.543725</td>\n",
       "      <td>0.566947</td>\n",
       "      <td>0.552785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>92.500000</td>\n",
       "      <td>0.146574</td>\n",
       "      <td>0.203389</td>\n",
       "      <td>0.307961</td>\n",
       "      <td>0.416747</td>\n",
       "      <td>0.898214</td>\n",
       "      <td>0.918959</td>\n",
       "      <td>0.570543</td>\n",
       "      <td>0.614744</td>\n",
       "      <td>0.579747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>104.000000</td>\n",
       "      <td>0.289633</td>\n",
       "      <td>0.388273</td>\n",
       "      <td>0.410256</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.921361</td>\n",
       "      <td>0.955494</td>\n",
       "      <td>0.635767</td>\n",
       "      <td>0.737396</td>\n",
       "      <td>0.676418</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9af3a080-59d9-4748-9b5c-158fbef2753d')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-9af3a080-59d9-4748-9b5c-158fbef2753d button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-9af3a080-59d9-4748-9b5c-158fbef2753d');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-d0efc27d-e3f8-475c-a432-a4f726323dad\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d0efc27d-e3f8-475c-a432-a4f726323dad')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-d0efc27d-e3f8-475c-a432-a4f726323dad button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "           Sample   eng_bleu    mk_bleu  eng_rouge   mk_rouge     eng_f1  \\\n",
       "count   35.000000  35.000000  35.000000  35.000000  35.000000  35.000000   \n",
       "mean    81.314286   0.102436   0.122358   0.266928   0.337275   0.887978   \n",
       "std     13.785695   0.069939   0.115380   0.063827   0.135472   0.015008   \n",
       "min     59.000000   0.000000   0.000000   0.148148   0.071429   0.858473   \n",
       "25%     69.500000   0.070237   0.000000   0.223808   0.265046   0.876900   \n",
       "50%     82.000000   0.102122   0.109031   0.259259   0.319149   0.889604   \n",
       "75%     92.500000   0.146574   0.203389   0.307961   0.416747   0.898214   \n",
       "max    104.000000   0.289633   0.388273   0.410256   0.640000   0.921361   \n",
       "\n",
       "           mk_f1  composite_eng  composite_mk  composite_total  \n",
       "count  35.000000      35.000000     35.000000        35.000000  \n",
       "mean    0.903985       0.544555      0.577647         0.557791  \n",
       "std     0.026057       0.036596      0.073995         0.043740  \n",
       "min     0.844432       0.473681      0.445593         0.480705  \n",
       "25%     0.887983       0.521151      0.527932         0.530832  \n",
       "50%     0.900319       0.543725      0.566947         0.552785  \n",
       "75%     0.918959       0.570543      0.614744         0.579747  \n",
       "max     0.955494       0.635767      0.737396         0.676418  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_test_eval.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JdOhlN23AYiA"
   },
   "source": [
    "\n",
    "\n",
    "### 5.1) Model Specifications\n",
    "\n",
    "Document the detailed specs of your choices. Also comment on how you valued the needs of the marketing tean vs the needs of the researchers, in case you had to make a trade-off.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m76VTy3_iIg6"
   },
   "source": [
    "To re-iterate from the beginning of this notebook, below are the parameters that were explored and the values that were selected for the final model:\n",
    "\n",
    "**Embedding Models**\n",
    " - 'multi-qa-mpnet-base-dot-v1'\n",
    "\n",
    "**Splitter Chunk Parameters**\n",
    " - 'CHUNK_SIZE' = 256\n",
    " - 'OVERLAP' = 16\n",
    "\n",
    "**Retriever Parameters**\n",
    " - 'k' = 6 (Num of chunks)\n",
    " - search_type = 'mmr' (Type of retriever search)\n",
    "\n",
    "**LLM Model**\n",
    " - Cohere\n",
    "\n",
    "**RAG Prompt Template(s)**\n",
    "\n",
    "    **For Engineering**\n",
    "<blockquote>\n",
    "\n",
    "\n",
    "              Please provide an precise and concise answer to the engineer's question below based on the context information provided.\n",
    "              \n",
    "              Below is a context:\n",
    "        {context}\n",
    "              Below is a question:\n",
    "        {question}\n",
    "              Below are answer instructions in order of importance:\n",
    "        - Formatting: Provide a succint, single-paragraph answer. Do not use bullet points. Do not explicitly reference papers in your answer.\n",
    "        - Technical Detail: Include technical details and terminologies that relate to the question.\n",
    "        - Research Focus: Orient answers towards the research aspects of the questions.\n",
    "        - Objective Tone: Maintain an objective and informative tone, aiming to educate the reader without persuasive language.\n",
    "\n",
    "</blockquote>\n",
    "\n",
    "    **For Marketing**\n",
    "<blockquote>\n",
    "\n",
    "              Please provide a precise and concise answer to the marketer's question below based on the context provided.\n",
    "              \n",
    "              Below is a context:\n",
    "        {context}\n",
    "              Below is a question:\n",
    "        {question}\n",
    "              Below are answer instructions in order of importance:\n",
    "        - Formatting: Provide a concise, single-paragraph answer that uses the fewest words necessary to fully address the question. Answer in a single sentence or phrase if you can. Do not use bullet points. Do not explicitly reference papers in your answer.\n",
    "        - Succinctness: Make sure your answer is concise and to the point. Provide only the essential information without delving into the technical depth.\n",
    "        - Broad Overview: Give a broad overview of the topic. Provide only the essential information without delving into the technical depth.\n",
    "        - Focus on Applications: EFocus on real-world uses and benefits and highlight how technology can solve problems or create opportunities.\n",
    "\n",
    "</blockquote>\n",
    "\n",
    "**Evaluation Metric**\n",
    "\n",
    "Copying evaluation metric from section 3.D in this notebook:\n",
    "\n",
    "After reviewing the training set scores that we are getting for the three separate metrics, we have determined that we want to create a 'composite' metric that is a weighted combination of the three. Given the individual normalized scores for BLEU, ROUGE-Lsum, and BERTScore F1 for both Engineering (eng) and Marketing (mk), the composite score can be calculated as follows:\n",
    "\n",
    "<br>\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Composite Score}_{\\text{eng}} &= w_{\\text{BLEU}} \\cdot N_{\\text{BLEU}_{\\text{eng}}} + w_{\\text{ROUGE-Lsum}} \\cdot N_{\\text{ROUGE-Lsum}_{\\text{eng}}} + w_{\\text{BERTScoreF1}} \\cdot N_{\\text{BERTScoreF1}_{\\text{eng}}} \\\\\n",
    "\\text{Composite Score}_{\\text{mk}} &= w_{\\text{BLEU}} \\cdot N_{\\text{BLEU}_{\\text{mk}}} + w_{\\text{ROUGE-Lsum}} \\cdot N_{\\text{ROUGE-Lsum}_{\\text{mk}}} + w_{\\text{BERTScoreF1}} \\cdot N_{\\text{BERTScoreF1}_{\\text{mk}}}\n",
    "\\end{align*}\n",
    "\n",
    "<br>\n",
    "\n",
    "where $w_{\\text{BLEU}}$, $w_{\\text{ROUGE-Lsum}}$, and $w_{\\text{BERTScoreF1}}$ are the weights for the BLEU, ROUGE-Lsum, and BERTScore F1 scores respectively, and $N_{\\text{Metric}_{\\text{dept}}}$ represents the normalized score for each metric in each department (Engineering or Marketing).\n",
    "\n",
    "We are choosing the following weights based off the importance that each metric brings to evaluating our generated answers. With BERT being best at measuring the contextual similarity between the summaries, it is given the most weight. Then ROUGE, with its measurement of phrase-based similarities, and lastly BLEU providing specific word choice similiarities. Below are the weights:\n",
    "\\begin{align*}\n",
    "    w_{\\text{BLEU}} &= 0.2 \\\\\n",
    "    w_{\\text{ROUGE-Lsum}} &= 0.3 \\\\\n",
    "    w_{\\text{BERTScoreF1}} &= 0.5\n",
    "\\end{align*}\n",
    "\n",
    "We also decided to weight the importance of the responses between the departments.  We understand that this is just a POC, and that should a production system be implemented, it would likely contain much more content in the document store, and with that a significant increase in the technical jargon and complexity within those documents. With the company having 300 engineers and 40 marketers, this system will likely see more usage from engineers.  Also in addition to that, the importance of accuractly containing specific (technical) details is more heavily weighted towards engineering. Marketing requirements will be higher-level and more generalized, and with such will not be as detrimented by missing specifics.  This this we are choosing the following departmental weighting:\n",
    "\n",
    "\\begin{align*}\n",
    "    w_{\\text{eng}} &= 0.6 \\\\\n",
    "    w_{\\text{mk}} &= 0.4\n",
    "\\end{align*}\n",
    "\n",
    "<br>\n",
    "The final composite score, adjusted for departmental impact, is calculated as:\n",
    "\n",
    "<br>\n",
    "\n",
    "\\begin{equation*}\n",
    "\\text{Final Composite Score} = w_{\\text{eng}} \\cdot \\text{Composite Score}_{\\text{eng}} + w_{\\text{mk}} \\cdot \\text{Composite Score}_{\\text{mk}}\n",
    "\\end{equation*}\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L7J41pWiyh06"
   },
   "source": [
    "\n",
    "###5.2) Some Test Questions\n",
    "\n",
    "**QUESTIONS:**\n",
    "\n",
    "\n",
    "Please study the answers generated by your chosen setup for these specific test questions:\n",
    "\n",
    "1. \"What purpose do large language models serve in the field of natural language processing?\" (Question 0)\n",
    "\n",
    "2. \"What methods are typically employed to create training data for embedding models that use task-specific instructions?\" (Question 50)\n",
    "\n",
    "3. \"How does a model's ability to answer questions relate to its exposure to specific types of questions during training?\" (Question 83, no labeled answers)\n",
    "\n",
    "For each of the three questions above please provide:\n",
    "\n",
    "a) The RAG results (research and marketing response)  \n",
    "b) The context provided  \n",
    "c) The document sources for the context  \n",
    "d) Also discuss your metric(s) for the first two examples (for both responses) compared to the gold responses\n",
    "\n",
    "Then, for questions 1 and 2, comment on how well you feel your metrics captured the differences and similarities between your answer and the gold answer?\n",
    "\n",
    "Put your answers to these questions into the answers file as you have done on previous assignments. Please consult the answer file for further details.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t_sf-i_HpTSj"
   },
   "source": [
    "####5.2.1 Test Question 1\n",
    "\n",
    "Please run the query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "7eXbVhP1qkmU",
    "outputId": "aaa1ff77-a49c-435e-c5d5-20a35450844a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Retriever: [Document(page_content='limitations, and societal impact of large language models. arXiv preprint arXiv:2102.02503.\\nThoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng, H.-T., Jin, A., Bos,', metadata={'source': 'https://arxiv.org/pdf/2203.02155.pdf', 'file_path': 'https://arxiv.org/pdf/2203.02155.pdf', 'page': 24, 'total_pages': 68, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20220307013712Z', 'modDate': 'D:20220307013712Z', 'trapped': '', 'page_num': 24, 'doc_num': 6, 'doc_source': 'ArXiv', 'split_id': 2014, '_id': '5b052b48b0654036bd513dfc447dcdbc', '_collection_name': 'rag_tech_db'}), Document(page_content='them to do what a given set of humans want them to do. By default, language models optimize\\nthe next word prediction objective, which is only a proxy for what we want these models to do.', metadata={'source': 'https://arxiv.org/pdf/2203.02155.pdf', 'file_path': 'https://arxiv.org/pdf/2203.02155.pdf', 'page': 19, 'total_pages': 68, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20220307013712Z', 'modDate': 'D:20220307013712Z', 'trapped': '', 'page_num': 19, 'doc_num': 6, 'doc_source': 'ArXiv', 'split_id': 1916, '_id': '57ef2c7e5e9c4b6ea434c0e698e6cc1c', '_collection_name': 'rag_tech_db'}), Document(page_content='Big language models have been pre-trained on a large collection of unsupervised textual corpus. Given enough parameters, these models are able to memorize some factual knowledge within parameter weights. Therefore, we can use these models to do', metadata={'source': 'https://lilianweng.github.io/posts/2020-10-29-odqa/', 'doc_num': 24, 'doc_source': 'WWW', 'split_id': 220, '_id': 'a24bf751e34b413eb46b2dfd53cb40d3', '_collection_name': 'rag_tech_db'}), Document(page_content='among the largest language models today and we apply them on a wide range of language tasks,\\nincluding classiﬁcation, summarization, question-answering, creative writing, dialogue, and others.', metadata={'source': 'https://arxiv.org/pdf/2203.02155.pdf', 'file_path': 'https://arxiv.org/pdf/2203.02155.pdf', 'page': 16, 'total_pages': 68, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20220307013712Z', 'modDate': 'D:20220307013712Z', 'trapped': '', 'page_num': 16, 'doc_num': 6, 'doc_source': 'ArXiv', 'split_id': 1843, '_id': 'f15d88ff89c9446abf33462c2a733dcd', '_collection_name': 'rag_tech_db'}), Document(page_content='Many applications in natural language processing rely on adapt-\\ning one large-scale, pre-trained language model to multiple down-\\nstream applications. Such adaptation is usually done via ﬁne-tuning,', metadata={'source': 'https://arxiv.org/pdf/2106.09685.pdf', 'file_path': 'https://arxiv.org/pdf/2106.09685.pdf', 'page': 0, 'total_pages': 26, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20211019005713Z', 'modDate': 'D:20211019005713Z', 'trapped': '', 'page_num': 0, 'doc_num': 5, 'doc_source': 'ArXiv', 'split_id': 1151, '_id': '2ee87e17ecdc473e885aa83b443b1d41', '_collection_name': 'rag_tech_db'}), Document(page_content='substantial data volume can benefit the efficacy of\\ninstruction tuning.\\n2\\nRelated Work\\nLarge Language Models for Information Re-\\ntrieval\\nLLMs possess a remarkable capacity for\\nlanguage understanding, enabling them to be highly', metadata={'source': 'https://arxiv.org/pdf/2401.06532.pdf', 'file_path': 'https://arxiv.org/pdf/2401.06532.pdf', 'page': 1, 'total_pages': 28, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.25', 'creationDate': 'D:20240220022120Z', 'modDate': 'D:20240220022120Z', 'trapped': '', 'page_num': 1, 'doc_num': 19, 'doc_source': 'ArXiv', 'split_id': 5942, '_id': 'b59a7269cac144449a4f88bcf1dca2f1', '_collection_name': 'rag_tech_db'})]\n",
      "Before LLM: messages=[HumanMessage(content=\"[INST]\\n              Please provide an precise and concise answer to the engineer's question below based on the context information provided.\\n\\n\\n              Below is a context:\\nlimitations, and societal impact of large language models. arXiv preprint arXiv:2102.02503.\\nThoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng, H.-T., Jin, A., Bos,\\n\\nthem to do what a given set of humans want them to do. By default, language models optimize\\nthe next word prediction objective, which is only a proxy for what we want these models to do.\\n\\nBig language models have been pre-trained on a large collection of unsupervised textual corpus. Given enough parameters, these models are able to memorize some factual knowledge within parameter weights. Therefore, we can use these models to do\\n\\namong the largest language models today and we apply them on a wide range of language tasks,\\nincluding classiﬁcation, summarization, question-answering, creative writing, dialogue, and others.\\n\\nMany applications in natural language processing rely on adapt-\\ning one large-scale, pre-trained language model to multiple down-\\nstream applications. Such adaptation is usually done via ﬁne-tuning,\\n\\nsubstantial data volume can benefit the efficacy of\\ninstruction tuning.\\n2\\nRelated Work\\nLarge Language Models for Information Re-\\ntrieval\\nLLMs possess a remarkable capacity for\\nlanguage understanding, enabling them to be highly\\n\\n              Below is a question:\\nWhat purpose do large language models serve in the field of natural language processing?\\n\\n              Below are answer instructions in order of importance:\\n- Formatting: Provide a succint, single-paragraph answer. Do not use bullet points. Do not explicitly reference papers in your answer. \\n- Technical Detail: Include technical details and terminologies that relate to the question.\\n- Research Focus: Orient answers towards the research aspects of the questions. \\n- Objective Tone: Maintain an objective and informative tone, aiming to educate the reader without persuasive language.\\n[/INST]\\n\")]\n",
      "After Retriever: [Document(page_content='limitations, and societal impact of large language models. arXiv preprint arXiv:2102.02503.\\nThoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng, H.-T., Jin, A., Bos,', metadata={'source': 'https://arxiv.org/pdf/2203.02155.pdf', 'file_path': 'https://arxiv.org/pdf/2203.02155.pdf', 'page': 24, 'total_pages': 68, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20220307013712Z', 'modDate': 'D:20220307013712Z', 'trapped': '', 'page_num': 24, 'doc_num': 6, 'doc_source': 'ArXiv', 'split_id': 2014, '_id': '5b052b48b0654036bd513dfc447dcdbc', '_collection_name': 'rag_tech_db'}), Document(page_content='them to do what a given set of humans want them to do. By default, language models optimize\\nthe next word prediction objective, which is only a proxy for what we want these models to do.', metadata={'source': 'https://arxiv.org/pdf/2203.02155.pdf', 'file_path': 'https://arxiv.org/pdf/2203.02155.pdf', 'page': 19, 'total_pages': 68, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20220307013712Z', 'modDate': 'D:20220307013712Z', 'trapped': '', 'page_num': 19, 'doc_num': 6, 'doc_source': 'ArXiv', 'split_id': 1916, '_id': '57ef2c7e5e9c4b6ea434c0e698e6cc1c', '_collection_name': 'rag_tech_db'}), Document(page_content='Big language models have been pre-trained on a large collection of unsupervised textual corpus. Given enough parameters, these models are able to memorize some factual knowledge within parameter weights. Therefore, we can use these models to do', metadata={'source': 'https://lilianweng.github.io/posts/2020-10-29-odqa/', 'doc_num': 24, 'doc_source': 'WWW', 'split_id': 220, '_id': 'a24bf751e34b413eb46b2dfd53cb40d3', '_collection_name': 'rag_tech_db'}), Document(page_content='among the largest language models today and we apply them on a wide range of language tasks,\\nincluding classiﬁcation, summarization, question-answering, creative writing, dialogue, and others.', metadata={'source': 'https://arxiv.org/pdf/2203.02155.pdf', 'file_path': 'https://arxiv.org/pdf/2203.02155.pdf', 'page': 16, 'total_pages': 68, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20220307013712Z', 'modDate': 'D:20220307013712Z', 'trapped': '', 'page_num': 16, 'doc_num': 6, 'doc_source': 'ArXiv', 'split_id': 1843, '_id': 'f15d88ff89c9446abf33462c2a733dcd', '_collection_name': 'rag_tech_db'}), Document(page_content='Many applications in natural language processing rely on adapt-\\ning one large-scale, pre-trained language model to multiple down-\\nstream applications. Such adaptation is usually done via ﬁne-tuning,', metadata={'source': 'https://arxiv.org/pdf/2106.09685.pdf', 'file_path': 'https://arxiv.org/pdf/2106.09685.pdf', 'page': 0, 'total_pages': 26, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20211019005713Z', 'modDate': 'D:20211019005713Z', 'trapped': '', 'page_num': 0, 'doc_num': 5, 'doc_source': 'ArXiv', 'split_id': 1151, '_id': '2ee87e17ecdc473e885aa83b443b1d41', '_collection_name': 'rag_tech_db'}), Document(page_content='substantial data volume can benefit the efficacy of\\ninstruction tuning.\\n2\\nRelated Work\\nLarge Language Models for Information Re-\\ntrieval\\nLLMs possess a remarkable capacity for\\nlanguage understanding, enabling them to be highly', metadata={'source': 'https://arxiv.org/pdf/2401.06532.pdf', 'file_path': 'https://arxiv.org/pdf/2401.06532.pdf', 'page': 1, 'total_pages': 28, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.25', 'creationDate': 'D:20240220022120Z', 'modDate': 'D:20240220022120Z', 'trapped': '', 'page_num': 1, 'doc_num': 19, 'doc_source': 'ArXiv', 'split_id': 5942, '_id': 'b59a7269cac144449a4f88bcf1dca2f1', '_collection_name': 'rag_tech_db'})]\n",
      "Before LLM: messages=[HumanMessage(content=\"[INST]\\n              Please provide a precise and concise answer to the marketer's question below based on the context provided.\\n\\n\\n              Below is a context:\\nlimitations, and societal impact of large language models. arXiv preprint arXiv:2102.02503.\\nThoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng, H.-T., Jin, A., Bos,\\n\\nthem to do what a given set of humans want them to do. By default, language models optimize\\nthe next word prediction objective, which is only a proxy for what we want these models to do.\\n\\nBig language models have been pre-trained on a large collection of unsupervised textual corpus. Given enough parameters, these models are able to memorize some factual knowledge within parameter weights. Therefore, we can use these models to do\\n\\namong the largest language models today and we apply them on a wide range of language tasks,\\nincluding classiﬁcation, summarization, question-answering, creative writing, dialogue, and others.\\n\\nMany applications in natural language processing rely on adapt-\\ning one large-scale, pre-trained language model to multiple down-\\nstream applications. Such adaptation is usually done via ﬁne-tuning,\\n\\nsubstantial data volume can benefit the efficacy of\\ninstruction tuning.\\n2\\nRelated Work\\nLarge Language Models for Information Re-\\ntrieval\\nLLMs possess a remarkable capacity for\\nlanguage understanding, enabling them to be highly\\n\\n              Below is a question:\\nWhat purpose do large language models serve in the field of natural language processing?\\n\\n              Below are answer instructions in order of importance:\\n- Formatting: Provide a concise, single-paragraph answer that uses the fewest words necessary to fully address the question. Answer in a single sentence or phrase if you can. Do not use bullet points. Do not explicitly reference papers in your answer.\\n- Succinctness: Make sure your answer is concise and to the point. Provide only the essential information without delving into the technical depth.\\n- Broad Overview: Give a broad overview of the topic. Provide only the essential information without delving into the technical depth. \\n- Focus on Applications: EFocus on real-world uses and benefits and highlight how technology can solve problems or create opportunities.\\n[/INST]\\n\")]\n",
      "------------------------------------------------------------\n",
      "Question: What purpose do large language models serve in the field of natural language processing?\n",
      " \n",
      "Engineering Prediction: Large language models (LLMs) have become integral to natural language processing (NLP) due to their ability to understand and generate human-like language. With pre-training on vast textual corpora, LLMs can memorize factual knowledge within their parameters, enabling a wide range of language tasks such as classification, summarization, and question-answering. The adaptation of LLMs to specific downstream applications via fine-tuning has led to their widespread use in NLP research and applications. This process allows models to learn task-specific objectives beyond next-word prediction, making them versatile tools for language understanding and generation. The effectiveness of LLMs in NLP is further enhanced by their capacity for instruction tuning, which enables them to follow human instructions accurately.\n",
      "Engineering Answer: Large language models (LLMs) serve the purpose of enabling general-purpose language generation and other natural language processing tasks such as classification. They achieve this by learning statistical relationships from text documents during computationally intensive self-supervised and semi-supervised training. LLMs can be used for text generation by predicting the next token or word, making them valuable for tasks like speech recognition, machine translation, and information retrieval. Additionally, LLMs have superseded previous models like recurrent neural networks, showcasing their efficiency and effectiveness in NLP tasks.\n",
      "\n",
      "Marketing Prediction: Large language models are an essential tool in natural language processing, enabling a wide range of applications such as classification, summarization, and question-answering by leveraging their capacity for language understanding and ability to be fine-tuned for specific tasks with minimal additional data.\n",
      "Marketing Answer: Large language models serve the purpose of improving performance in various natural language processing tasks, such as speech recognition, machine translation, natural language generation, optical character recognition, handwriting recognition, grammar induction, and information retrieval.\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"question_0\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"Sample\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.37796447300922725,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eng_bleu\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3413579839813494,\n        \"min\": 0.09685166633897925,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.09685166633897925,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mk_bleu\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.37796447300922725,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eng_rouge\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.29684039099749066,\n        \"min\": 0.21463414634146344,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.21463414634146344,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mk_rouge\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2713591088271375,\n        \"min\": 0.2820512820512821,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.2820512820512821,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eng_f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.04424017536299967,\n        \"min\": 0.8829514980316162,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.8829514980316162,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mk_f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.04382747690450357,\n        \"min\": 0.8840433955192566,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.8840433955192566,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"composite_eng\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.1794438017770169,\n        \"min\": 0.525236326186043,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.525236326186043,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"composite_mk\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.17891436570223848,\n        \"min\": 0.5266370823750129,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.5266370823750129,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"composite_total\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.17923202734710558,\n        \"min\": 0.5257966286616309,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.5257966286616309,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-2fdd1dc9-2447-440d-ba9e-849bba521861\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample</th>\n",
       "      <th>eng_bleu</th>\n",
       "      <th>mk_bleu</th>\n",
       "      <th>eng_rouge</th>\n",
       "      <th>mk_rouge</th>\n",
       "      <th>eng_f1</th>\n",
       "      <th>mk_f1</th>\n",
       "      <th>composite_eng</th>\n",
       "      <th>composite_mk</th>\n",
       "      <th>composite_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.096852</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.214634</td>\n",
       "      <td>0.282051</td>\n",
       "      <td>0.882951</td>\n",
       "      <td>0.884043</td>\n",
       "      <td>0.525236</td>\n",
       "      <td>0.526637</td>\n",
       "      <td>0.525797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.096852</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.214634</td>\n",
       "      <td>0.282051</td>\n",
       "      <td>0.882951</td>\n",
       "      <td>0.884043</td>\n",
       "      <td>0.525236</td>\n",
       "      <td>0.526637</td>\n",
       "      <td>0.525797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.096852</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.214634</td>\n",
       "      <td>0.282051</td>\n",
       "      <td>0.882951</td>\n",
       "      <td>0.884043</td>\n",
       "      <td>0.525236</td>\n",
       "      <td>0.526637</td>\n",
       "      <td>0.525797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.096852</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.214634</td>\n",
       "      <td>0.282051</td>\n",
       "      <td>0.882951</td>\n",
       "      <td>0.884043</td>\n",
       "      <td>0.525236</td>\n",
       "      <td>0.526637</td>\n",
       "      <td>0.525797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.096852</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.214634</td>\n",
       "      <td>0.282051</td>\n",
       "      <td>0.882951</td>\n",
       "      <td>0.884043</td>\n",
       "      <td>0.525236</td>\n",
       "      <td>0.526637</td>\n",
       "      <td>0.525797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.096852</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.214634</td>\n",
       "      <td>0.282051</td>\n",
       "      <td>0.882951</td>\n",
       "      <td>0.884043</td>\n",
       "      <td>0.525236</td>\n",
       "      <td>0.526637</td>\n",
       "      <td>0.525797</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2fdd1dc9-2447-440d-ba9e-849bba521861')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-2fdd1dc9-2447-440d-ba9e-849bba521861 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-2fdd1dc9-2447-440d-ba9e-849bba521861');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-d5ce94e0-8088-4c78-ad25-df6ec673a347\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d5ce94e0-8088-4c78-ad25-df6ec673a347')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-d5ce94e0-8088-4c78-ad25-df6ec673a347 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "       Sample  eng_bleu  mk_bleu  eng_rouge  mk_rouge    eng_f1     mk_f1  \\\n",
       "count     1.0  1.000000      1.0   1.000000  1.000000  1.000000  1.000000   \n",
       "mean      0.0  0.096852      0.0   0.214634  0.282051  0.882951  0.884043   \n",
       "std       NaN       NaN      NaN        NaN       NaN       NaN       NaN   \n",
       "min       0.0  0.096852      0.0   0.214634  0.282051  0.882951  0.884043   \n",
       "25%       0.0  0.096852      0.0   0.214634  0.282051  0.882951  0.884043   \n",
       "50%       0.0  0.096852      0.0   0.214634  0.282051  0.882951  0.884043   \n",
       "75%       0.0  0.096852      0.0   0.214634  0.282051  0.882951  0.884043   \n",
       "max       0.0  0.096852      0.0   0.214634  0.282051  0.882951  0.884043   \n",
       "\n",
       "       composite_eng  composite_mk  composite_total  \n",
       "count       1.000000      1.000000         1.000000  \n",
       "mean        0.525236      0.526637         0.525797  \n",
       "std              NaN           NaN              NaN  \n",
       "min         0.525236      0.526637         0.525797  \n",
       "25%         0.525236      0.526637         0.525797  \n",
       "50%         0.525236      0.526637         0.525797  \n",
       "75%         0.525236      0.526637         0.525797  \n",
       "max         0.525236      0.526637         0.525797  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_rag_chain = build_RAG_prompt_chain(eng_rag_template, llm_model, retriever, format_docs)\n",
    "mk_rag_chain = build_RAG_prompt_chain(mk_rag_template, llm_model, retriever, format_docs)\n",
    "rag_chains = {\"engineering\": eng_rag_chain, \"marketing\": mk_rag_chain}\n",
    "metrics = ['rouge', 'bleu', 'bertscore']\n",
    "q0_dict = {\n",
    "    0: {\"question\": \"What purpose do large language models serve in the field of natural language processing?\",\n",
    "  \"gold_answer_research\": \"Large language models (LLMs) serve the purpose of enabling general-purpose language generation and other natural language processing tasks such as classification. They achieve this by learning statistical relationships from text documents during computationally intensive self-supervised and semi-supervised training. LLMs can be used for text generation by predicting the next token or word, making them valuable for tasks like speech recognition, machine translation, and information retrieval. Additionally, LLMs have superseded previous models like recurrent neural networks, showcasing their efficiency and effectiveness in NLP tasks.\",\n",
    "  \"gold_answer_marketing\": \"Large language models serve the purpose of improving performance in various natural language processing tasks, such as speech recognition, machine translation, natural language generation, optical character recognition, handwriting recognition, grammar induction, and information retrieval.\"}\n",
    "}\n",
    "question_0 = evaluate(metrics, q0_dict, rag_chains, iterations=0, verbose=False, dept_specific=True, print_results=True)\n",
    "question_0 = composite_evaluation(question_0)\n",
    "question_0.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6wrr5VUUJk1"
   },
   "source": [
    "a) RAG Result:\n",
    "\n",
    "Engineering Prediction:\n",
    "<blockquote>\n",
    "     Large language models (LLMs) have become integral to natural language processing (NLP) due to their ability to understand and generate human-like language. With pre-training on vast textual corpora, LLMs can memorize factual knowledge within their parameters, enabling a wide range of language tasks such as classification, summarization, and question-answering. The adaptation of LLMs to specific downstream applications via fine-tuning has led to their widespread use in NLP research and applications. This process allows models to learn task-specific objectives beyond next-word prediction, making them versatile tools for language understanding and generation. The effectiveness of LLMs in NLP is further enhanced by their capacity for instruction tuning, which enables them to follow human instructions accurately.\n",
    "</blockquote>\n",
    "\n",
    "Marketing Prediction:\n",
    "<blockquote>\n",
    "    Large language models are an essential tool in natural language processing, enabling a wide range of applications such as classification, summarization, and question-answering by leveraging their capacity for language understanding and ability to be fine-tuned for specific tasks with minimal additional data.\n",
    "</blockquote>\n",
    "\n",
    "b,c) Context Provide and Sources\n",
    "<blockquote>\n",
    "\n",
    "page_content='limitations, and societal impact of large language models. arXiv preprint arXiv:2102.02503.\\nThoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng, H.-T., Jin, A., Bos,',\n",
    "'source': 'https://arxiv.org/pdf/2203.02155.pdf'\n",
    "\n",
    "page_content='them to do what a given set of humans want them to do. By default, language models optimize\\nthe next word prediction objective, which is only a proxy for what we want these models to do.',\n",
    "'source': 'https://arxiv.org/pdf/2203.02155.pdf'\n",
    "\n",
    "page_content='Big language models have been pre-trained on a large collection of unsupervised textual corpus. Given enough parameters, these models are able to memorize some factual knowledge within parameter weights. Therefore, we can use these models to do',\n",
    "'source': 'https://lilianweng.github.io/posts/2020-10-29-odqa/'\n",
    "\n",
    "page_content='among the largest language models today and we apply them on a wide range of language tasks,\\nincluding classiﬁcation, summarization, question-answering, creative writing, dialogue, and others.',\n",
    "'source': 'https://arxiv.org/pdf/2203.02155.pdf'\n",
    "\n",
    "page_content='Many applications in natural language processing rely on adapt-\\ning one large-scale, pre-trained language model to multiple down-\\nstream applications. Such adaptation is usually done via ﬁne-tuning,',\n",
    "'source': 'https://arxiv.org/pdf/2106.09685.pdf',\n",
    "\n",
    "page_content='substantial data volume can benefit the efficacy of\\ninstruction tuning.\\n2\\nRelated Work\\nLarge Language Models for Information Re-\\ntrieval\\nLLMs possess a remarkable capacity for\\nlanguage understanding, enabling them to be highly',\n",
    "'source': 'https://arxiv.org/pdf/2401.06532.pdf'\n",
    "\n",
    "</blockquote>\n",
    "\n",
    "d) Metric(s)\n",
    "\n",
    " - Engineering Composite: 0.525236\n",
    " - Marketing Composite: 0.526637\n",
    " - Joint Composite: 0.525797"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZdkNySoUDy3"
   },
   "source": [
    "####5.2.2 Test Question 2\n",
    "\n",
    "Please run the query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "izSpKhLHT5Ky",
    "outputId": "368f6d1e-6351-423a-cd79-27822091440c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Retriever: [Document(page_content='datasets with instructions across diverse task cate-\\ngories and domains: Multitask Embeddings Data\\nwith Instructions (MEDI).\\nData\\nConstruction\\nWe\\nbuild\\nMEDI\\nby\\ncombining\\n300\\ndatasets\\nfrom\\nSuper-\\nNaturalInstructions\\n(super-NI;\\nWang\\net\\nal.,', metadata={'source': 'https://arxiv.org/pdf/2212.09741.pdf', 'file_path': 'https://arxiv.org/pdf/2212.09741.pdf', 'page': 2, 'total_pages': 18, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.25', 'creationDate': 'D:20230531004557Z', 'modDate': 'D:20230531004557Z', 'trapped': '', 'page_num': 2, 'doc_num': 9, 'doc_source': 'ArXiv', 'split_id': 3222, '_id': '65728727919742d1bfdfbf7d188c85e5', '_collection_name': 'rag_tech_db'}), Document(page_content='where the goal is to retrieve a few in-context learn-\\ning (i.e., demonstration) examples from annotated\\nexamples given a test instance. The embedding\\nmodel is used to encode all annotated examples\\nand to find the few most similar examples to the', metadata={'source': 'https://arxiv.org/pdf/2212.09741.pdf', 'file_path': 'https://arxiv.org/pdf/2212.09741.pdf', 'page': 13, 'total_pages': 18, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.25', 'creationDate': 'D:20230531004557Z', 'modDate': 'D:20230531004557Z', 'trapped': '', 'page_num': 13, 'doc_num': 9, 'doc_source': 'ArXiv', 'split_id': 3419, '_id': '1656fe859a594154bee83e871e180ea7', '_collection_name': 'rag_tech_db'}), Document(page_content='[43] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal,\\nK. Slama, A. Ray, et al. Training language models to follow instructions with human feedback.', metadata={'source': 'https://arxiv.org/pdf/2305.14314.pdf', 'file_path': 'https://arxiv.org/pdf/2305.14314.pdf', 'page': 18, 'total_pages': 26, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.25', 'creationDate': 'D:20230524004717Z', 'modDate': 'D:20230524004717Z', 'trapped': '', 'page_num': 18, 'doc_num': 10, 'doc_source': 'ArXiv', 'split_id': 3815, '_id': '8c945ba892ed476db19c8d3439eb4260', '_collection_name': 'rag_tech_db'}), Document(page_content='address this, the third paradigm trains customized\\nretrievers for each task using unlabeled corpora,\\nleveraging another model to automatically generate\\ntraining data (Wang et al., 2022a). Concurrent to\\nour work, Dai et al. (2022) use task-speciﬁc tem-', metadata={'source': 'https://arxiv.org/pdf/2211.09260.pdf', 'file_path': 'https://arxiv.org/pdf/2211.09260.pdf', 'page': 1, 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20221221015621Z', 'modDate': 'D:20221221015621Z', 'trapped': '', 'page_num': 1, 'doc_num': 7, 'doc_source': 'ArXiv', 'split_id': 2471, '_id': 'f6887047d2aa4680a642cb025c181055', '_collection_name': 'rag_tech_db'}), Document(page_content='Often we need to complete tasks that require latest knowledge after the model pretraining time cutoff or internal/private knowledge base. In that case, the model would not know the context if we don’t explicitly provide it in the prompt. Many methods for', metadata={'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'doc_num': 24, 'doc_source': 'WWW', 'split_id': 385, '_id': 'b2fce2b7b986426b94a3193d6a40b893', '_collection_name': 'rag_tech_db'}), Document(page_content='(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', '_id': '0f8d916e33784e3c9ded5615b8aa2244', '_collection_name': 'rag_tech_db'})]\n",
      "Before LLM: messages=[HumanMessage(content=\"[INST]\\n              Please provide an precise and concise answer to the engineer's question below based on the context information provided.\\n\\n\\n              Below is a context:\\ndatasets with instructions across diverse task cate-\\ngories and domains: Multitask Embeddings Data\\nwith Instructions (MEDI).\\nData\\nConstruction\\nWe\\nbuild\\nMEDI\\nby\\ncombining\\n300\\ndatasets\\nfrom\\nSuper-\\nNaturalInstructions\\n(super-NI;\\nWang\\net\\nal.,\\n\\nwhere the goal is to retrieve a few in-context learn-\\ning (i.e., demonstration) examples from annotated\\nexamples given a test instance. The embedding\\nmodel is used to encode all annotated examples\\nand to find the few most similar examples to the\\n\\n[43] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal,\\nK. Slama, A. Ray, et al. Training language models to follow instructions with human feedback.\\n\\naddress this, the third paradigm trains customized\\nretrievers for each task using unlabeled corpora,\\nleveraging another model to automatically generate\\ntraining data (Wang et al., 2022a). Concurrent to\\nour work, Dai et al. (2022) use task-speciﬁc tem-\\n\\nOften we need to complete tasks that require latest knowledge after the model pretraining time cutoff or internal/private knowledge base. In that case, the model would not know the context if we don’t explicitly provide it in the prompt. Many methods for\\n\\n(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\n              Below is a question:\\nWhat methods are typically employed to create training data for embedding models that use task-specific instructions?\\n\\n              Below are answer instructions in order of importance:\\n- Formatting: Provide a succint, single-paragraph answer. Do not use bullet points. Do not explicitly reference papers in your answer. \\n- Technical Detail: Include technical details and terminologies that relate to the question.\\n- Research Focus: Orient answers towards the research aspects of the questions. \\n- Objective Tone: Maintain an objective and informative tone, aiming to educate the reader without persuasive language.\\n[/INST]\\n\")]\n",
      "After Retriever: [Document(page_content='datasets with instructions across diverse task cate-\\ngories and domains: Multitask Embeddings Data\\nwith Instructions (MEDI).\\nData\\nConstruction\\nWe\\nbuild\\nMEDI\\nby\\ncombining\\n300\\ndatasets\\nfrom\\nSuper-\\nNaturalInstructions\\n(super-NI;\\nWang\\net\\nal.,', metadata={'source': 'https://arxiv.org/pdf/2212.09741.pdf', 'file_path': 'https://arxiv.org/pdf/2212.09741.pdf', 'page': 2, 'total_pages': 18, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.25', 'creationDate': 'D:20230531004557Z', 'modDate': 'D:20230531004557Z', 'trapped': '', 'page_num': 2, 'doc_num': 9, 'doc_source': 'ArXiv', 'split_id': 3222, '_id': '65728727919742d1bfdfbf7d188c85e5', '_collection_name': 'rag_tech_db'}), Document(page_content='where the goal is to retrieve a few in-context learn-\\ning (i.e., demonstration) examples from annotated\\nexamples given a test instance. The embedding\\nmodel is used to encode all annotated examples\\nand to find the few most similar examples to the', metadata={'source': 'https://arxiv.org/pdf/2212.09741.pdf', 'file_path': 'https://arxiv.org/pdf/2212.09741.pdf', 'page': 13, 'total_pages': 18, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.25', 'creationDate': 'D:20230531004557Z', 'modDate': 'D:20230531004557Z', 'trapped': '', 'page_num': 13, 'doc_num': 9, 'doc_source': 'ArXiv', 'split_id': 3419, '_id': '1656fe859a594154bee83e871e180ea7', '_collection_name': 'rag_tech_db'}), Document(page_content='[43] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal,\\nK. Slama, A. Ray, et al. Training language models to follow instructions with human feedback.', metadata={'source': 'https://arxiv.org/pdf/2305.14314.pdf', 'file_path': 'https://arxiv.org/pdf/2305.14314.pdf', 'page': 18, 'total_pages': 26, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.25', 'creationDate': 'D:20230524004717Z', 'modDate': 'D:20230524004717Z', 'trapped': '', 'page_num': 18, 'doc_num': 10, 'doc_source': 'ArXiv', 'split_id': 3815, '_id': '8c945ba892ed476db19c8d3439eb4260', '_collection_name': 'rag_tech_db'}), Document(page_content='address this, the third paradigm trains customized\\nretrievers for each task using unlabeled corpora,\\nleveraging another model to automatically generate\\ntraining data (Wang et al., 2022a). Concurrent to\\nour work, Dai et al. (2022) use task-speciﬁc tem-', metadata={'source': 'https://arxiv.org/pdf/2211.09260.pdf', 'file_path': 'https://arxiv.org/pdf/2211.09260.pdf', 'page': 1, 'total_pages': 25, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20221221015621Z', 'modDate': 'D:20221221015621Z', 'trapped': '', 'page_num': 1, 'doc_num': 7, 'doc_source': 'ArXiv', 'split_id': 2471, '_id': 'f6887047d2aa4680a642cb025c181055', '_collection_name': 'rag_tech_db'}), Document(page_content='Often we need to complete tasks that require latest knowledge after the model pretraining time cutoff or internal/private knowledge base. In that case, the model would not know the context if we don’t explicitly provide it in the prompt. Many methods for', metadata={'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'doc_num': 24, 'doc_source': 'WWW', 'split_id': 385, '_id': 'b2fce2b7b986426b94a3193d6a40b893', '_collection_name': 'rag_tech_db'}), Document(page_content='(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', '_id': '0f8d916e33784e3c9ded5615b8aa2244', '_collection_name': 'rag_tech_db'})]\n",
      "Before LLM: messages=[HumanMessage(content=\"[INST]\\n              Please provide a precise and concise answer to the marketer's question below based on the context provided.\\n\\n\\n              Below is a context:\\ndatasets with instructions across diverse task cate-\\ngories and domains: Multitask Embeddings Data\\nwith Instructions (MEDI).\\nData\\nConstruction\\nWe\\nbuild\\nMEDI\\nby\\ncombining\\n300\\ndatasets\\nfrom\\nSuper-\\nNaturalInstructions\\n(super-NI;\\nWang\\net\\nal.,\\n\\nwhere the goal is to retrieve a few in-context learn-\\ning (i.e., demonstration) examples from annotated\\nexamples given a test instance. The embedding\\nmodel is used to encode all annotated examples\\nand to find the few most similar examples to the\\n\\n[43] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal,\\nK. Slama, A. Ray, et al. Training language models to follow instructions with human feedback.\\n\\naddress this, the third paradigm trains customized\\nretrievers for each task using unlabeled corpora,\\nleveraging another model to automatically generate\\ntraining data (Wang et al., 2022a). Concurrent to\\nour work, Dai et al. (2022) use task-speciﬁc tem-\\n\\nOften we need to complete tasks that require latest knowledge after the model pretraining time cutoff or internal/private knowledge base. In that case, the model would not know the context if we don’t explicitly provide it in the prompt. Many methods for\\n\\n(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\n              Below is a question:\\nWhat methods are typically employed to create training data for embedding models that use task-specific instructions?\\n\\n              Below are answer instructions in order of importance:\\n- Formatting: Provide a concise, single-paragraph answer that uses the fewest words necessary to fully address the question. Answer in a single sentence or phrase if you can. Do not use bullet points. Do not explicitly reference papers in your answer.\\n- Succinctness: Make sure your answer is concise and to the point. Provide only the essential information without delving into the technical depth.\\n- Broad Overview: Give a broad overview of the topic. Provide only the essential information without delving into the technical depth. \\n- Focus on Applications: EFocus on real-world uses and benefits and highlight how technology can solve problems or create opportunities.\\n[/INST]\\n\")]\n",
      "------------------------------------------------------------\n",
      "Question: What methods are typically employed to create training data for embedding models that use task-specific instructions?\n",
      " \n",
      "Engineering Prediction: Training data for embedding models that employ task-specific instructions can be generated through a few approaches. One method involves combining large-scale datasets that contain diverse task instructions, such as the Super-NaturalInstructions dataset, to train models on a wide range of tasks and domains. Another approach is to utilize unlabeled corpora and generate training data automatically by training customized retrievers for each specific task. This can be done concurrently with the use of task-specific templates and prompts to fine-tune language models on instruction-following tasks. These methods aim to address the challenge of adapting to new knowledge and contexts that may not have been present during the model's pretraining phase.\n",
      "Engineering Answer: To create training data for embedding models that use task-specific instructions, a common method is to combine datasets from different sources, such as the SuperNaturalInstructions dataset with existing collections designed for embedding training. The SuperNaturalInstructions dataset provides natural language instructions, which can be paired with positive and negative examples to form training samples. Additionally, for tasks like classification or similarity, training samples can be constructed by selecting text sequences associated with different classes or similarities. This diverse training data is essential for instruction-based finetuning, which enables the embedding model to learn from a wide range of tasks and domains.\n",
      "\n",
      "Marketing Prediction: Training data for embedding models with task-specific instructions can be generated by leveraging another model to automatically create training examples for a customized retriever model, which then encodes and retrieves relevant in-context learning examples for specific tasks.\n",
      "Marketing Answer: Training data for embedding models that use task-specific instructions is typically created by formulating a wide variety of tasks as text-to-text problems, distinguishing good/bad candidate outputs given an input text. This is done by combining datasets with natural language instructions and constructing positive and negative pairs for training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"question_50\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"Sample\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 18.520259177452136,\n        \"min\": 1.0,\n        \"max\": 50.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          50.0,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eng_bleu\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.32019469452509025,\n        \"min\": 0.1528444671643164,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.1528444671643164,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mk_bleu\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3450017865880581,\n        \"min\": 0.08721107081501912,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.08721107081501912,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eng_rouge\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.28347335475692037,\n        \"min\": 0.25,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.25,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mk_rouge\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.27828153507272774,\n        \"min\": 0.2637362637362637,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.2637362637362637,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eng_f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.041479810893180924,\n        \"min\": 0.8902547359466553,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.8902547359466553,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mk_f1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.04448208573186976,\n        \"min\": 0.8823114633560181,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.8823114633560181,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"composite_eng\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.16982085077868464,\n        \"min\": 0.5506962614061909,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.5506962614061909,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"composite_mk\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.17472586070536486,\n        \"min\": 0.5377188249618919,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.5377188249618919,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"composite_total\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.17178285474935673,\n        \"min\": 0.5455052868284713,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.5455052868284713,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-8515787f-5f96-4982-8071-1503d1b20601\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample</th>\n",
       "      <th>eng_bleu</th>\n",
       "      <th>mk_bleu</th>\n",
       "      <th>eng_rouge</th>\n",
       "      <th>mk_rouge</th>\n",
       "      <th>eng_f1</th>\n",
       "      <th>mk_f1</th>\n",
       "      <th>composite_eng</th>\n",
       "      <th>composite_mk</th>\n",
       "      <th>composite_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>50.0</td>\n",
       "      <td>0.152844</td>\n",
       "      <td>0.087211</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.263736</td>\n",
       "      <td>0.890255</td>\n",
       "      <td>0.882311</td>\n",
       "      <td>0.550696</td>\n",
       "      <td>0.537719</td>\n",
       "      <td>0.545505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>50.0</td>\n",
       "      <td>0.152844</td>\n",
       "      <td>0.087211</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.263736</td>\n",
       "      <td>0.890255</td>\n",
       "      <td>0.882311</td>\n",
       "      <td>0.550696</td>\n",
       "      <td>0.537719</td>\n",
       "      <td>0.545505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>50.0</td>\n",
       "      <td>0.152844</td>\n",
       "      <td>0.087211</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.263736</td>\n",
       "      <td>0.890255</td>\n",
       "      <td>0.882311</td>\n",
       "      <td>0.550696</td>\n",
       "      <td>0.537719</td>\n",
       "      <td>0.545505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>50.0</td>\n",
       "      <td>0.152844</td>\n",
       "      <td>0.087211</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.263736</td>\n",
       "      <td>0.890255</td>\n",
       "      <td>0.882311</td>\n",
       "      <td>0.550696</td>\n",
       "      <td>0.537719</td>\n",
       "      <td>0.545505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>50.0</td>\n",
       "      <td>0.152844</td>\n",
       "      <td>0.087211</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.263736</td>\n",
       "      <td>0.890255</td>\n",
       "      <td>0.882311</td>\n",
       "      <td>0.550696</td>\n",
       "      <td>0.537719</td>\n",
       "      <td>0.545505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>50.0</td>\n",
       "      <td>0.152844</td>\n",
       "      <td>0.087211</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.263736</td>\n",
       "      <td>0.890255</td>\n",
       "      <td>0.882311</td>\n",
       "      <td>0.550696</td>\n",
       "      <td>0.537719</td>\n",
       "      <td>0.545505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8515787f-5f96-4982-8071-1503d1b20601')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-8515787f-5f96-4982-8071-1503d1b20601 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-8515787f-5f96-4982-8071-1503d1b20601');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-49cb5b90-8d9a-4fb0-9092-2865d2e71767\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-49cb5b90-8d9a-4fb0-9092-2865d2e71767')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-49cb5b90-8d9a-4fb0-9092-2865d2e71767 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "       Sample  eng_bleu   mk_bleu  eng_rouge  mk_rouge    eng_f1     mk_f1  \\\n",
       "count     1.0  1.000000  1.000000       1.00  1.000000  1.000000  1.000000   \n",
       "mean     50.0  0.152844  0.087211       0.25  0.263736  0.890255  0.882311   \n",
       "std       NaN       NaN       NaN        NaN       NaN       NaN       NaN   \n",
       "min      50.0  0.152844  0.087211       0.25  0.263736  0.890255  0.882311   \n",
       "25%      50.0  0.152844  0.087211       0.25  0.263736  0.890255  0.882311   \n",
       "50%      50.0  0.152844  0.087211       0.25  0.263736  0.890255  0.882311   \n",
       "75%      50.0  0.152844  0.087211       0.25  0.263736  0.890255  0.882311   \n",
       "max      50.0  0.152844  0.087211       0.25  0.263736  0.890255  0.882311   \n",
       "\n",
       "       composite_eng  composite_mk  composite_total  \n",
       "count       1.000000      1.000000         1.000000  \n",
       "mean        0.550696      0.537719         0.545505  \n",
       "std              NaN           NaN              NaN  \n",
       "min         0.550696      0.537719         0.545505  \n",
       "25%         0.550696      0.537719         0.545505  \n",
       "50%         0.550696      0.537719         0.545505  \n",
       "75%         0.550696      0.537719         0.545505  \n",
       "max         0.550696      0.537719         0.545505  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q50_dict = {\n",
    "  50: {\"question\": \"What methods are typically employed to create training data for embedding models that use task-specific instructions?\",\n",
    "    \"gold_answer_research\": \"To create training data for embedding models that use task-specific instructions, a common method is to combine datasets from different sources, such as the SuperNaturalInstructions dataset with existing collections designed for embedding training. The SuperNaturalInstructions dataset provides natural language instructions, which can be paired with positive and negative examples to form training samples. Additionally, for tasks like classification or similarity, training samples can be constructed by selecting text sequences associated with different classes or similarities. This diverse training data is essential for instruction-based finetuning, which enables the embedding model to learn from a wide range of tasks and domains.\",\n",
    "    \"gold_answer_marketing\": \"Training data for embedding models that use task-specific instructions is typically created by formulating a wide variety of tasks as text-to-text problems, distinguishing good/bad candidate outputs given an input text. This is done by combining datasets with natural language instructions and constructing positive and negative pairs for training.\"}\n",
    "}\n",
    "question_50 = evaluate(metrics, q50_dict, rag_chains, iterations=0, verbose=False, dept_specific=True, print_results=True)\n",
    "question_50 = composite_evaluation(question_50)\n",
    "question_50.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eDovfhEtUMMr"
   },
   "source": [
    "a) RAG Result:\n",
    "\n",
    "Engineering Prediction:\n",
    "<blockquote>\n",
    "     Training data for embedding models that employ task-specific instructions can be generated through a few approaches. One method involves combining large-scale datasets that contain diverse task instructions, such as the Super-NaturalInstructions dataset, to train models on a wide range of tasks and domains. Another approach is to utilize unlabeled corpora and generate training data automatically by training customized retrievers for each specific task. This can be done concurrently with the use of task-specific templates and prompts to fine-tune language models on instruction-following tasks. These methods aim to address the challenge of adapting to new knowledge and contexts that may not have been present during the model's pretraining phase.\n",
    "</blockquote>\n",
    "\n",
    "Marketing Prediction:\n",
    "<blockquote>\n",
    "    Training data for embedding models with task-specific instructions can be generated by leveraging another model to automatically create training examples for a customized retriever model, which then encodes and retrieves relevant in-context learning examples for specific tasks.\n",
    "</blockquote>\n",
    "\n",
    "b,c) Context Provide and Sources\n",
    "<blockquote>\n",
    "\n",
    "page_content='datasets with instructions across diverse task cate-\\ngories and domains: Multitask Embeddings Data\\nwith Instructions (MEDI).\\nData\\nConstruction\\nWe\\nbuild\\nMEDI\\nby\\ncombining\\n300\\ndatasets\\nfrom\\nSuper-\\nNaturalInstructions\\n(super-NI;\\nWang\\net\\nal.,',\n",
    "'source': 'https://arxiv.org/pdf/2212.09741.pdf'\n",
    "\n",
    "page_content='where the goal is to retrieve a few in-context learn-\\ning (i.e., demonstration) examples from annotated\\nexamples given a test instance. The embedding\\nmodel is used to encode all annotated examples\\nand to find the few most similar examples to the',\n",
    "'source': 'https://arxiv.org/pdf/2212.09741.pdf'\n",
    "\n",
    "page_content='[43] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal,\\nK. Slama, A. Ray, et al. Training language models to follow instructions with human feedback.',\n",
    "'source': 'https://arxiv.org/pdf/2305.14314.pdf'\n",
    "\n",
    "page_content='address this, the third paradigm trains customized\\nretrievers for each task using unlabeled corpora,\\nleveraging another model to automatically generate\\ntraining data (Wang et al., 2022a). Concurrent to\\nour work, Dai et al. (2022) use task-speciﬁc tem-',\n",
    "'source': 'https://arxiv.org/pdf/2211.09260.pdf'\n",
    "\n",
    "page_content='Often we need to complete tasks that require latest knowledge after the model pretraining time cutoff or internal/private knowledge base. In that case, the model would not know the context if we don’t explicitly provide it in the prompt. Many methods for',\n",
    "'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/'\n",
    "\n",
    "page_content='(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:',\n",
    "'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'\n",
    "\n",
    "</blockquote>\n",
    "\n",
    "d) Metric(s)\n",
    "\n",
    " - Engineering Composite: 0.550696\n",
    " - Marketing Composite: 0.537719\n",
    " - Joint Composite: 0.545505"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VxDhzMXsT48H"
   },
   "source": [
    "####5.2.3 Test Question 3\n",
    "\n",
    "Please run the query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9WXJX_x3T4we",
    "outputId": "33977c0f-590f-4c31-b945-74e51a3501f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Retriever: [Document(page_content='A model is able to answer novel questions which have answers not contained in the training dataset.', metadata={'source': 'https://lilianweng.github.io/posts/2020-10-29-odqa/', 'doc_num': 24, 'doc_source': 'WWW', 'split_id': 15, '_id': '22078dca273d4b3aa79c784e2c11a77e', '_collection_name': 'rag_tech_db'}), Document(page_content='as the associated general questions:\\n1. The overall quality of the question, such as its\\ndifficulty, clarity, and information needed for\\nanswering it.', metadata={'source': 'https://arxiv.org/pdf/2309.08872.pdf', 'file_path': 'https://arxiv.org/pdf/2309.08872.pdf', 'page': 5, 'total_pages': 17, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.25', 'creationDate': 'D:20231109011357Z', 'modDate': 'D:20231109011357Z', 'trapped': '', 'page_num': 5, 'doc_num': 13, 'doc_source': 'ArXiv', 'split_id': 4699, '_id': '93ac6c5433b54b81b43342059c2ab74e', '_collection_name': 'rag_tech_db'}), Document(page_content='on open Natural Questions [29], WebQuestions [3] and CuratedTrec [2] and strongly outperform\\nrecent approaches that use specialised pre-training objectives on TriviaQA [24]. Despite these being', metadata={'source': 'https://arxiv.org/pdf/2005.11401.pdf', 'file_path': 'https://arxiv.org/pdf/2005.11401.pdf', 'page': 1, 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210413004838Z', 'modDate': 'D:20210413004838Z', 'trapped': '', 'page_num': 1, 'doc_num': 1, 'doc_source': 'ArXiv', 'split_id': 28, '_id': '02c866ddf6c341858066b4de874b4e14', '_collection_name': 'rag_tech_db'}), Document(page_content='avenues for having the model sometimes prioritizing truthfulness and harmlessness over helpfulness\\nduring training, particularly through the use of refusals: having the model refuse to answer certain', metadata={'source': 'https://arxiv.org/pdf/2203.02155.pdf', 'file_path': 'https://arxiv.org/pdf/2203.02155.pdf', 'page': 35, 'total_pages': 68, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20220307013712Z', 'modDate': 'D:20220307013712Z', 'trapped': '', 'page_num': 35, 'doc_num': 6, 'doc_source': 'ArXiv', 'split_id': 2123, '_id': '62485b330f2c499a933d2d272a8350ce', '_collection_name': 'rag_tech_db'}), Document(page_content='0.71\\n0.34\\nHuman (lower bound)\\n-\\n71.62\\nTable 3: Model and lower-bound human performance\\non selecting evidence for questions in QASPER\\nFigure 2: Learning curves showing Answer-F1 and\\nEvidence-F1 on the dev. set while varying training data\\nsize.', metadata={'source': 'https://arxiv.org/pdf/2105.03011.pdf', 'file_path': 'https://arxiv.org/pdf/2105.03011.pdf', 'page': 6, 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210510005635Z', 'modDate': 'D:20210510005635Z', 'trapped': '', 'page_num': 6, 'doc_num': 4, 'doc_source': 'ArXiv', 'split_id': 1056, '_id': '3a004fb305fb41c5ad3f55c5fe3794bd', '_collection_name': 'rag_tech_db'}), Document(page_content='With the recommended system prompt, the model properly\\ndeclines to answer 100% of the harmful questions.\\nAs an illustration, we provide in Table 5 the answers of\\nboth Mistral 7B – Instruct and Llama 2 Chat 13B to the', metadata={'source': 'https://arxiv.org/pdf/2310.06825.pdf', 'file_path': 'https://arxiv.org/pdf/2310.06825.pdf', 'page': 4, 'total_pages': 9, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.25', 'creationDate': 'D:20231011004817Z', 'modDate': 'D:20231011004817Z', 'trapped': '', 'page_num': 4, 'doc_num': 15, 'doc_source': 'ArXiv', 'split_id': 4993, '_id': '01a535572b6147ab98e988a409633ba6', '_collection_name': 'rag_tech_db'})]\n",
      "Before LLM: messages=[HumanMessage(content=\"[INST]\\n              Please provide an precise and concise answer to the engineer's question below based on the context information provided.\\n\\n\\n              Below is a context:\\nA model is able to answer novel questions which have answers not contained in the training dataset.\\n\\nas the associated general questions:\\n1. The overall quality of the question, such as its\\ndifficulty, clarity, and information needed for\\nanswering it.\\n\\non open Natural Questions [29], WebQuestions [3] and CuratedTrec [2] and strongly outperform\\nrecent approaches that use specialised pre-training objectives on TriviaQA [24]. Despite these being\\n\\navenues for having the model sometimes prioritizing truthfulness and harmlessness over helpfulness\\nduring training, particularly through the use of refusals: having the model refuse to answer certain\\n\\n0.71\\n0.34\\nHuman (lower bound)\\n-\\n71.62\\nTable 3: Model and lower-bound human performance\\non selecting evidence for questions in QASPER\\nFigure 2: Learning curves showing Answer-F1 and\\nEvidence-F1 on the dev. set while varying training data\\nsize.\\n\\nWith the recommended system prompt, the model properly\\ndeclines to answer 100% of the harmful questions.\\nAs an illustration, we provide in Table 5 the answers of\\nboth Mistral 7B – Instruct and Llama 2 Chat 13B to the\\n\\n              Below is a question:\\nHow does a model's ability to answer questions relate to its exposure to specific types of questions during training?\\n\\n              Below are answer instructions in order of importance:\\n- Formatting: Provide a succint, single-paragraph answer. Do not use bullet points. Do not explicitly reference papers in your answer. \\n- Technical Detail: Include technical details and terminologies that relate to the question.\\n- Research Focus: Orient answers towards the research aspects of the questions. \\n- Objective Tone: Maintain an objective and informative tone, aiming to educate the reader without persuasive language.\\n[/INST]\\n\")]\n",
      "After Retriever: [Document(page_content='A model is able to answer novel questions which have answers not contained in the training dataset.', metadata={'source': 'https://lilianweng.github.io/posts/2020-10-29-odqa/', 'doc_num': 24, 'doc_source': 'WWW', 'split_id': 15, '_id': '22078dca273d4b3aa79c784e2c11a77e', '_collection_name': 'rag_tech_db'}), Document(page_content='as the associated general questions:\\n1. The overall quality of the question, such as its\\ndifficulty, clarity, and information needed for\\nanswering it.', metadata={'source': 'https://arxiv.org/pdf/2309.08872.pdf', 'file_path': 'https://arxiv.org/pdf/2309.08872.pdf', 'page': 5, 'total_pages': 17, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.25', 'creationDate': 'D:20231109011357Z', 'modDate': 'D:20231109011357Z', 'trapped': '', 'page_num': 5, 'doc_num': 13, 'doc_source': 'ArXiv', 'split_id': 4699, '_id': '93ac6c5433b54b81b43342059c2ab74e', '_collection_name': 'rag_tech_db'}), Document(page_content='on open Natural Questions [29], WebQuestions [3] and CuratedTrec [2] and strongly outperform\\nrecent approaches that use specialised pre-training objectives on TriviaQA [24]. Despite these being', metadata={'source': 'https://arxiv.org/pdf/2005.11401.pdf', 'file_path': 'https://arxiv.org/pdf/2005.11401.pdf', 'page': 1, 'total_pages': 19, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210413004838Z', 'modDate': 'D:20210413004838Z', 'trapped': '', 'page_num': 1, 'doc_num': 1, 'doc_source': 'ArXiv', 'split_id': 28, '_id': '02c866ddf6c341858066b4de874b4e14', '_collection_name': 'rag_tech_db'}), Document(page_content='avenues for having the model sometimes prioritizing truthfulness and harmlessness over helpfulness\\nduring training, particularly through the use of refusals: having the model refuse to answer certain', metadata={'source': 'https://arxiv.org/pdf/2203.02155.pdf', 'file_path': 'https://arxiv.org/pdf/2203.02155.pdf', 'page': 35, 'total_pages': 68, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20220307013712Z', 'modDate': 'D:20220307013712Z', 'trapped': '', 'page_num': 35, 'doc_num': 6, 'doc_source': 'ArXiv', 'split_id': 2123, '_id': '62485b330f2c499a933d2d272a8350ce', '_collection_name': 'rag_tech_db'}), Document(page_content='0.71\\n0.34\\nHuman (lower bound)\\n-\\n71.62\\nTable 3: Model and lower-bound human performance\\non selecting evidence for questions in QASPER\\nFigure 2: Learning curves showing Answer-F1 and\\nEvidence-F1 on the dev. set while varying training data\\nsize.', metadata={'source': 'https://arxiv.org/pdf/2105.03011.pdf', 'file_path': 'https://arxiv.org/pdf/2105.03011.pdf', 'page': 6, 'total_pages': 12, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20210510005635Z', 'modDate': 'D:20210510005635Z', 'trapped': '', 'page_num': 6, 'doc_num': 4, 'doc_source': 'ArXiv', 'split_id': 1056, '_id': '3a004fb305fb41c5ad3f55c5fe3794bd', '_collection_name': 'rag_tech_db'}), Document(page_content='With the recommended system prompt, the model properly\\ndeclines to answer 100% of the harmful questions.\\nAs an illustration, we provide in Table 5 the answers of\\nboth Mistral 7B – Instruct and Llama 2 Chat 13B to the', metadata={'source': 'https://arxiv.org/pdf/2310.06825.pdf', 'file_path': 'https://arxiv.org/pdf/2310.06825.pdf', 'page': 4, 'total_pages': 9, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'creator': 'LaTeX with hyperref', 'producer': 'pdfTeX-1.40.25', 'creationDate': 'D:20231011004817Z', 'modDate': 'D:20231011004817Z', 'trapped': '', 'page_num': 4, 'doc_num': 15, 'doc_source': 'ArXiv', 'split_id': 4993, '_id': '01a535572b6147ab98e988a409633ba6', '_collection_name': 'rag_tech_db'})]\n",
      "Before LLM: messages=[HumanMessage(content=\"[INST]\\n              Please provide a precise and concise answer to the marketer's question below based on the context provided.\\n\\n\\n              Below is a context:\\nA model is able to answer novel questions which have answers not contained in the training dataset.\\n\\nas the associated general questions:\\n1. The overall quality of the question, such as its\\ndifficulty, clarity, and information needed for\\nanswering it.\\n\\non open Natural Questions [29], WebQuestions [3] and CuratedTrec [2] and strongly outperform\\nrecent approaches that use specialised pre-training objectives on TriviaQA [24]. Despite these being\\n\\navenues for having the model sometimes prioritizing truthfulness and harmlessness over helpfulness\\nduring training, particularly through the use of refusals: having the model refuse to answer certain\\n\\n0.71\\n0.34\\nHuman (lower bound)\\n-\\n71.62\\nTable 3: Model and lower-bound human performance\\non selecting evidence for questions in QASPER\\nFigure 2: Learning curves showing Answer-F1 and\\nEvidence-F1 on the dev. set while varying training data\\nsize.\\n\\nWith the recommended system prompt, the model properly\\ndeclines to answer 100% of the harmful questions.\\nAs an illustration, we provide in Table 5 the answers of\\nboth Mistral 7B – Instruct and Llama 2 Chat 13B to the\\n\\n              Below is a question:\\nHow does a model's ability to answer questions relate to its exposure to specific types of questions during training?\\n\\n              Below are answer instructions in order of importance:\\n- Formatting: Provide a concise, single-paragraph answer that uses the fewest words necessary to fully address the question. Answer in a single sentence or phrase if you can. Do not use bullet points. Do not explicitly reference papers in your answer.\\n- Succinctness: Make sure your answer is concise and to the point. Provide only the essential information without delving into the technical depth.\\n- Broad Overview: Give a broad overview of the topic. Provide only the essential information without delving into the technical depth. \\n- Focus on Applications: EFocus on real-world uses and benefits and highlight how technology can solve problems or create opportunities.\\n[/INST]\\n\")]\n",
      "------------------------------------------------------------\n",
      "Question: How does a model's ability to answer questions relate to its exposure to specific types of questions during training?\n",
      " \n",
      "Engineering Prediction: A model's ability to answer questions is closely tied to the diversity and specificity of the training data it is exposed to. The more diverse and comprehensive the training dataset is, the better the model becomes at generalizing and providing accurate responses to novel questions. This is because the model learns patterns, contexts, and relationships between the questions and answers during training, which enables it to extrapolate this knowledge to new, unseen scenarios. However, if the training data lacks certain types of questions or covers a limited range of topics, the model may struggle with questions that deviate from its learned patterns, exhibiting lower performance in terms of accuracy and relevance of responses.\n",
      "\n",
      "Marketing Prediction: A model's ability to answer questions is directly related to its exposure to specific question types during training; the more diverse and comprehensive the training data, the better equipped the model is to handle novel questions and provide accurate responses.\n"
     ]
    }
   ],
   "source": [
    "question = \"How does a model's ability to answer questions relate to its exposure to specific types of questions during training?\"\n",
    "eng_prediction = rag_chains[\"engineering\"].invoke(question)\n",
    "mk_prediction = rag_chains[\"marketing\"].invoke(question)\n",
    "\n",
    "print(\"-\"*60)\n",
    "print(f\"Question: {question}\")\n",
    "print(\" \")\n",
    "print(f\"Engineering Prediction: {eng_prediction}\")\n",
    "print(\"\")\n",
    "print(f\"Marketing Prediction: {mk_prediction}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24UZvRmhUOWk"
   },
   "source": [
    "a) RAG Result:\n",
    "\n",
    "Engineering Prediction:\n",
    "<blockquote>\n",
    "     A model's ability to answer questions is closely tied to the diversity and specificity of the training data it is exposed to. The more diverse and comprehensive the training dataset is, the better the model becomes at generalizing and providing accurate responses to novel questions. This is because the model learns patterns, contexts, and relationships between the questions and answers during training, which enables it to extrapolate this knowledge to new, unseen scenarios. However, if the training data lacks certain types of questions or covers a limited range of topics, the model may struggle with questions that deviate from its learned patterns, exhibiting lower performance in terms of accuracy and relevance of responses.\n",
    "\n",
    "</blockquote>\n",
    "\n",
    "Marketing Prediction:\n",
    "<blockquote>\n",
    "    A model's ability to answer questions is directly related to its exposure to specific question types during training; the more diverse and comprehensive the training data, the better equipped the model is to handle novel questions and provide accurate responses.\n",
    "</blockquote>\n",
    "\n",
    "b,c) Context Provide and Sources\n",
    "<blockquote>\n",
    "\n",
    "page_content='A model is able to answer novel questions which have answers not contained in the training dataset.',\n",
    "'source': 'https://lilianweng.github.io/posts/2020-10-29-odqa/',\n",
    "\n",
    "page_content='as the associated general questions:\\n1. The overall quality of the question, such as its\\ndifficulty, clarity, and information needed for\\nanswering it.',\n",
    "'source': 'https://arxiv.org/pdf/2309.08872.pdf',\n",
    "\n",
    "page_content='on open Natural Questions [29], WebQuestions [3] and CuratedTrec [2] and strongly outperform\\nrecent approaches that use specialised pre-training objectives on TriviaQA [24]. Despite these being',\n",
    "'source': 'https://arxiv.org/pdf/2005.11401.pdf',\n",
    "\n",
    "page_content='avenues for having the model sometimes prioritizing truthfulness and harmlessness over helpfulness\\nduring training, particularly through the use of refusals: having the model refuse to answer certain',\n",
    "'source': 'https://arxiv.org/pdf/2203.02155.pdf',\n",
    "\n",
    "page_content='0.71\\n0.34\\nHuman (lower bound)\\n-\\n71.62\\nTable 3: Model and lower-bound human performance\\non selecting evidence for questions in QASPER\\nFigure 2: Learning curves showing Answer-F1 and\\nEvidence-F1 on the dev. set while varying training data\\nsize.',\n",
    "'source': 'https://arxiv.org/pdf/2105.03011.pdf',\n",
    "\n",
    "page_content='With the recommended system prompt, the model properly\\ndeclines to answer 100% of the harmful questions.\\nAs an illustration, we provide in Table 5 the answers of\\nboth Mistral 7B – Instruct and Llama 2 Chat 13B to the',\n",
    "'source': 'https://arxiv.org/pdf/2310.06825.pdf',\n",
    "\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "44h038A7AUdn"
   },
   "source": [
    "###5.3 Other Questions\n",
    "\n",
    "Below are a few questions that you should think about. Please answer them in the answer file directly (in a short paragraph) and also see whether they may be relevant for your final write-up.\n",
    "\n",
    "**QUESTION:**\n",
    "\n",
    "5.3.a. How would you expect your response quality to change if you had a chunk size of 50?\n",
    "\n",
    "5.3.b. How would you expect your response quality to change if you had a chunk size of 5000?\n",
    "\n",
    "5.3.c. If you had time, how do you think fine-tuning of the LLM could help?  What type of data would you want for that? And which training approach would you take?\n",
    "\n",
    "5.3.d. What was your design philosophy  of the prompts? How did they differ between engineering and marketing support?\n",
    "\n",
    "5.3.e. What are your average and peak load estimates for the system? Given that, would you suggest a pay-per-use deployment or one that reserves the LLM?\n",
    "\n",
    "5.3.f. What type of limitations/risks would you see in using this system?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6yhZpA9y1HyN"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "czxW4C_gf5Eo",
    "-T_P6z1xifzi",
    "Dk-gifg-4ui5",
    "fXge2i585b__",
    "m8XAqeMxDFzA",
    "hg__Qo7-g_Sr",
    "u7JPLwRIK7tu",
    "bPq6yhTXi7gj",
    "ckroOX6qc9cD",
    "f0Z3NUgEaJWL"
   ],
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00298c86bd5347a7aa8ebe3d2eae51b2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e9610898c8f34e0898c0c4f33d76e666",
      "placeholder": "​",
      "style": "IPY_MODEL_f2383551e8b74982aae2eca73542f635",
      "value": " 25.1k/25.1k [00:00&lt;00:00, 645kB/s]"
     }
    },
    "00400719a0904489aed145e287eedbee": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0091c32769f14b83b0ca4e5ba87f1d13": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "00ba5672234a4879b0463f784b832b1e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "011889a215454ebdace22dc8f02f457f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8194ebcecbf244688b70a771de7cf75b",
       "IPY_MODEL_19aa00338d3f4e78878dcffad74fafcf",
       "IPY_MODEL_45814158a33b46efb1c5b3e5dd55f73e"
      ],
      "layout": "IPY_MODEL_07d75d8cf7ad459bb93e3a14c4b873ef"
     }
    },
    "01fa84447757403d87609a97457097a9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "023aafd15019487f92276d8230f52e7d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0279b311601a4358b6dfca1e5e5e01c2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "02e4827845d7458b850f63b63be1bbdd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0b2072323ce840f88690bc72dcebd357",
      "max": 90868376,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2fa8f710890a4ea69d0b3806aa4346fc",
      "value": 90868376
     }
    },
    "03ac455559734c06ab934bdba9229262": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "03b1a1106cc8493b971c534c9413a010": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "044c1758656841c086ab105e5c2977d0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "04bf8b42a38c45268a65f8cb8cf223b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7a8139029aaf4eb49f249ac7c9d182cd",
      "max": 53,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_31346dbadbcd4eacae0c39017c72a2c5",
      "value": 53
     }
    },
    "04f229f8b5d847e9bfed5ffd6b78278d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "04f435a02ed0473cbb2c20286f7d69c0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "055fc71161734a2291d46496d34bd333": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6fe6c615e8804248b83a538c52d904c7",
      "placeholder": "​",
      "style": "IPY_MODEL_08ea40da30ee444d9aff24f284951e91",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "06ecaf2ff42241c6b232db150914078e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_544ce0bb6bdc405e998e0c2f19342a81",
       "IPY_MODEL_b7a0d02ad2f3466c90e54ee2de63d91b",
       "IPY_MODEL_20ebeeba9e42401c8db667bd6757c1d0"
      ],
      "layout": "IPY_MODEL_97db510554f04266835dedb6da450e08"
     }
    },
    "0742615e751f4e7b9278237ba28c4a80": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3bbe80f266ea43eeb6946c24242f6db6",
      "max": 466247,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e611a30d9fac4b10ae4e39c49c233512",
      "value": 466247
     }
    },
    "07d75d8cf7ad459bb93e3a14c4b873ef": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "084ec135b91a4b8285220d285809bbdc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "08a06ba0111f4c90820fa7da2aeade27": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "08ea40da30ee444d9aff24f284951e91": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0956b86c701f436e844f0c37aee45662": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_db8645b9f2d34d568699766a7b396281",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c2268477307043779d6c2070342774ad",
      "value": 231508
     }
    },
    "09803fd745254cb9901e94411bb71540": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0a0199431c1f4f07b1741468c9f1d114": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0a6f16dacd3042aa8cdbb9a1b151b643": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0aefb5ec218c4048b660662b54cec77f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0af94016632446c1a86e75f4f51a7f0f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_504401a87e23485784ccf9d3957a37da",
      "placeholder": "​",
      "style": "IPY_MODEL_10021f5d22a747688c8f8f58a2bd406a",
      "value": " 350/350 [00:00&lt;00:00, 24.0kB/s]"
     }
    },
    "0b2072323ce840f88690bc72dcebd357": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0cc2893ef4444bb296f114cf26c95aa4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0dc891cc2eec4d27b73774782bca2d6b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0dfa37c24f424776b3d15f8f0de1af0c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0e0be7f4054a4abe808accd1eb542e36": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a5c5577478b04951a6c3093b175c8b79",
      "max": 1795303,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a486fcff9502442e9f68619201b98673",
      "value": 1795303
     }
    },
    "0e108f1f1fe242748e5e99342436765a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0e2cd61b60974115ac8beae93ebd4287": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0e74588e049842a2a207d8aaad3b4c9b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_546d274303574956a1fa8f7b409e4347",
      "placeholder": "​",
      "style": "IPY_MODEL_7a5604969c9a481ab6e41079660042b0",
      "value": "modules.json: 100%"
     }
    },
    "0ef5557a48864cf390c9f63ab394636f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0f831c120bbd49ec9156d2e5f45def29": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4aa057719e8f484695edb384033c11af",
       "IPY_MODEL_721b98900e11438b8a13c98d8f442fc2",
       "IPY_MODEL_e0ebbf9c653a4b38a5983ff26b3cb122"
      ],
      "layout": "IPY_MODEL_a7c1dd526dc64b2899545a0e84e63014"
     }
    },
    "0fcdecd7f17d457ba197d7dade252823": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f6c77a3adb4e4093802838a15a583a9d",
       "IPY_MODEL_3b7ca4878ae5411ca8b6271919f06223",
       "IPY_MODEL_e76abefba7a144e3973ab74007867eee"
      ],
      "layout": "IPY_MODEL_564074528f84422b8015f8ef202c5bde"
     }
    },
    "10021f5d22a747688c8f8f58a2bd406a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "10de2dae49d94481946a8d20e800cff4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "10e64cd2844740ef9c104fd8adcff09e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c25257a378c04cffa2ce81a6664b87a8",
       "IPY_MODEL_882aef82a3fd42f3a6b310e63e5dfe50",
       "IPY_MODEL_8ad266de548b408face2edc4b1862d7b"
      ],
      "layout": "IPY_MODEL_292e98e2c2ce46f69e109aa9242431ce"
     }
    },
    "110532eccf6847df92563cf649efb47b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1108367e725c4b16970238fb2cd27ca1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3cc329c0639847238b57297d47d36a96",
      "max": 1355863,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fb2562fed9f34104a023df39610ece1c",
      "value": 1355863
     }
    },
    "1127b8dfd3cf46ba8219ba7227d2cd81": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1204cf7954ff4b638a5d1d25cba79c9e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "120547a0deb74a69bce43b08f7e453e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1a50ddf4eeff436f8dd87dccfbe497cf",
       "IPY_MODEL_161081ea350b41e1ab4228eb51cfcc4f",
       "IPY_MODEL_653af0f977154c2b8a5336893efac036"
      ],
      "layout": "IPY_MODEL_0091c32769f14b83b0ca4e5ba87f1d13"
     }
    },
    "12e9e208ec6e41beb10df311907ff5e3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "12ed138c2b6047788198c5983801a320": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_dcf0d88429104c798d3048da28035521",
       "IPY_MODEL_5fa0665b0f294c0dad16ab252c132809",
       "IPY_MODEL_3dc8124776044277a18cf2fcd5782403"
      ],
      "layout": "IPY_MODEL_e52558d760244c5ebb731b4af7878e45"
     }
    },
    "13f7f3bf21014a2685cde4f315b5c592": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1439a037803e4d9b90539a96226d6e7c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2c691a4e2b114d70880e7c81974d9223",
       "IPY_MODEL_cdd760a06b814bd187b40ea5ffea49f3",
       "IPY_MODEL_8a72a0ca0171424bab6fdd9e4a223bb7"
      ],
      "layout": "IPY_MODEL_573b6c54ff6c4ce6b9669b420c670c59"
     }
    },
    "14579babec1f4a478b990726bce224fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "145e84288178448dbb4fd3e4b0e45172": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "149d6095989145b5bd1eeebf47d65bb8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9cf15f64612346fa91c9479bc0881e3a",
      "placeholder": "​",
      "style": "IPY_MODEL_1914fb25557646a290c89cd3fcb740c8",
      "value": "merges.txt: 100%"
     }
    },
    "1554078f33ec4d45859c5dc6d2df1419": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1562e799dc8d46c8a49b9c00462d826e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ea3136dd585b4cd9888e191b7c1bd895",
       "IPY_MODEL_7dbcb5ce2143486495089ed89aa6900e",
       "IPY_MODEL_774e8740090f4438be20e43a9c9cf6e3"
      ],
      "layout": "IPY_MODEL_d298a4f7377344928a65ee4f9b556d39"
     }
    },
    "1579c889649a49b4989273662897fb8d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "159bf6a876f247bda3d8b6097739c1cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2276c89996004e8e99c1df0fecd69d89",
      "placeholder": "​",
      "style": "IPY_MODEL_2b667c4db1134a5e983696072aa8212e",
      "value": " 612/612 [00:00&lt;00:00, 38.8kB/s]"
     }
    },
    "159f751bdeb741c3bbdf96f8bf241463": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "161081ea350b41e1ab4228eb51cfcc4f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e269f2ae0ddd4265bfe01ae0b9c1c9ff",
      "max": 10659,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a021d0a6f40b4b53b8c81279ad657a0f",
      "value": 10659
     }
    },
    "16258087eda641e9ba243837d5974c8c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f67abb31961845d6ba6422aabd790ca3",
       "IPY_MODEL_22b23fcc2a774731b410521e5cb8de7e",
       "IPY_MODEL_c1f3804d62584cc182cd13f90c299f0f"
      ],
      "layout": "IPY_MODEL_97f5cba66e764e6e8a5dab9caf08c3bc"
     }
    },
    "18115621275a49148f81f9bb73f059d6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e2b409dff53441a6909f27c964f8b4bf",
      "placeholder": "​",
      "style": "IPY_MODEL_d3dcb0c70bb24821b1540a648d86c1da",
      "value": " 90.9M/90.9M [00:00&lt;00:00, 148MB/s]"
     }
    },
    "1914fb25557646a290c89cd3fcb740c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "19a8531b90114368bd0c6298f136d760": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "19aa00338d3f4e78878dcffad74fafcf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7ca5b258f67e46a185ca8fa7e647ca50",
      "max": 116,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1579c889649a49b4989273662897fb8d",
      "value": 116
     }
    },
    "1a50ddf4eeff436f8dd87dccfbe497cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eaa6409e563c436386c0257444bc2a5c",
      "placeholder": "​",
      "style": "IPY_MODEL_ba5fc8c18c5b4f47a2a7eaeae5278591",
      "value": "README.md: 100%"
     }
    },
    "1b23976143f643c08871d5c17f364036": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_53975da8ecb5463d873d4674629b180d",
      "placeholder": "​",
      "style": "IPY_MODEL_67b79b611a32439fb6c9e16436b8dd19",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "1d0c669af7e8433fa170fe61a91ae6bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1e01b6ba83b84354a783de89601a631f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_80ed477043a34061af1d0329def8c336",
       "IPY_MODEL_c38696dc6db64ee28c51ed5718866099",
       "IPY_MODEL_5d331d5b6973462f9883747f8b4c4ab8"
      ],
      "layout": "IPY_MODEL_8f3ce5ac92a84bb194fe4145ea108400"
     }
    },
    "1e15f4e6cbda4f8298540c7a4991de85": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1e225b5bdee34454ab860722c41cc286": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d86c4fc49c0744f6bc4c45778b561733",
      "placeholder": "​",
      "style": "IPY_MODEL_fe6118f3635a4a1dac2ced42046a7318",
      "value": "config_sentence_transformers.json: 100%"
     }
    },
    "1e7bc085b40e40a8aa334d8cbb8c9304": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1f5b84c9567e4d67b8dec40284c4a326": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2075d9e638e746ee9d02066ba50e2a2b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5a3c97d479184433b8bc063dfbbadc1b",
      "max": 456318,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bbf6a3dc755544909038770967ac7774",
      "value": 456318
     }
    },
    "20a10315b3134e868029987222642232": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cc39cd9f128f44c09efe00b47feccf80",
      "max": 116,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c4bfae8972374bb581d5d5abcc8a832d",
      "value": 116
     }
    },
    "20b6c8bf2505415ea451b2c2fdf28cbd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "20c1cb91da134ec7bf654f0528f08ae9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "20dfb400adc040bf85aa7b9d179ef46e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "20ebeeba9e42401c8db667bd6757c1d0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_159f751bdeb741c3bbdf96f8bf241463",
      "placeholder": "​",
      "style": "IPY_MODEL_71b202d1efef470a8a75892b30618c69",
      "value": " 190/190 [00:00&lt;00:00, 3.63kB/s]"
     }
    },
    "2236d0f93d464e50bed2231544fa7ba5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2b63721e767b4787ae474a7fd7515ebb",
      "placeholder": "​",
      "style": "IPY_MODEL_8f2b0ccefffe48868e1f15fd232de716",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "2276c89996004e8e99c1df0fecd69d89": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "22b23fcc2a774731b410521e5cb8de7e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2d89f99001d94c96a76a970b8dd6c4b5",
      "max": 90868376,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6ab40f4d10314413abb7a1435900c596",
      "value": 90868376
     }
    },
    "23b8aaecb2384c85a8d12a4cc39d513a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "247f3b37a349499fb88185e01f7ce359": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "24944ca627c746188d109df655eaa4de": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2503801bdc884ab1b924299ea6b32219": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ff77c48eacd74e7cbde24e77faf4112c",
      "placeholder": "​",
      "style": "IPY_MODEL_56b5cae7eb0d46cabfb122d5a599ce36",
      "value": " 232k/232k [00:00&lt;00:00, 5.16MB/s]"
     }
    },
    "25450062f6894e3487bf642c568f618b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "26419e544ac54982b4d6785fdcc3e806": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7f7a8e3453734b388f0e7380ace7e339",
      "placeholder": "​",
      "style": "IPY_MODEL_5b0ba54016b3462fb321037162eec2b6",
      "value": "config.json: 100%"
     }
    },
    "268267b9a9ac4a268e1aaff23f1da544": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7ec4029f051a48ce9e838d6f990b647e",
       "IPY_MODEL_0e0be7f4054a4abe808accd1eb542e36",
       "IPY_MODEL_2d7f4a1c4b7e40e497d8b383b464ec07"
      ],
      "layout": "IPY_MODEL_fdfcea700a644a8d9deb6cc679eaaf4d"
     }
    },
    "26c970f739694c7fb39d7265bcc1cd42": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "27f80615837649a68e938d7beecd3bf6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "288829dde7444ecea0c7590531881e1a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_811258b1e9d14978a045a17bfa0fac76",
      "placeholder": "​",
      "style": "IPY_MODEL_762db47a0b644102a206d50917cacd68",
      "value": "special_tokens_map.json: 100%"
     }
    },
    "292e98e2c2ce46f69e109aa9242431ce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "29f741fffd404c8c87df8fa7a62d8d36": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2b63721e767b4787ae474a7fd7515ebb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2b667c4db1134a5e983696072aa8212e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2ba5b9e374e348aeb8c3fbf36ba3bb88": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_49eac13e310440378d596881537beb66",
      "max": 90868376,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_20c1cb91da134ec7bf654f0528f08ae9",
      "value": 90868376
     }
    },
    "2c691a4e2b114d70880e7c81974d9223": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_633303b3635f48a58de1c4a4d0a8dd47",
      "placeholder": "​",
      "style": "IPY_MODEL_38b93318e17d42c1bac8eacba74b81d2",
      "value": "special_tokens_map.json: 100%"
     }
    },
    "2d3172fe38c6481fa3451b659007a843": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2d7f4a1c4b7e40e497d8b383b464ec07": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d72c50264df5470d92cc39c23ffbff04",
      "placeholder": "​",
      "style": "IPY_MODEL_5534bdabb6a749bfb9caec514e9ffa49",
      "value": " 1.80M/1.80M [00:00&lt;00:00, 6.89MB/s]"
     }
    },
    "2d89f99001d94c96a76a970b8dd6c4b5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2e26125a7aad4731828c3d05dae1db22": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e07ee354b4834417b0fdbb2a896e2e44",
      "placeholder": "​",
      "style": "IPY_MODEL_83f723e718214633a41be0b88f53ae4d",
      "value": " 456k/456k [00:00&lt;00:00, 3.92MB/s]"
     }
    },
    "2f0c47366b15435fa6a47666a76a405e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3b20c1f15f104fef9d35b0f765670edc",
       "IPY_MODEL_f1ec39a6c3d742d1b36735d3303ef7ca",
       "IPY_MODEL_bf17bd1fa8a943ed93c564f691edf5f3"
      ],
      "layout": "IPY_MODEL_3e8927492a8e4af7aba8339da1d9a454"
     }
    },
    "2f3b88aa761040c19eafa325ea877f10": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2f4505cdde3c4301bc814c1cc4534b12": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2f63831c1dee463f8e5f56fa516d48a8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2fa8f710890a4ea69d0b3806aa4346fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "31346dbadbcd4eacae0c39017c72a2c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "313af608a8e44338b4baee58091a5edd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "314278a71ef946988bb8f5fc97fadfa4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_10de2dae49d94481946a8d20e800cff4",
      "placeholder": "​",
      "style": "IPY_MODEL_3c49589aa8f04a91884ca44d20aba1c5",
      "value": "sentence_bert_config.json: 100%"
     }
    },
    "31931eb718934dc793f937ddacb2083b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "31ebf07a34694748ac87d3ca339259a7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "327563f522b047dda038e6290ca19ad5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_50bc4ec3d2a7486a896b7a78b9ed7c3a",
       "IPY_MODEL_c171b3d8dabe4347900df968e37d2fd8",
       "IPY_MODEL_6f05b02d95234ba69153d44c5a894835"
      ],
      "layout": "IPY_MODEL_0279b311601a4358b6dfca1e5e5e01c2"
     }
    },
    "32b3c3584eb74b698d4f891932f2f906": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6be7a10dc856492688d24802f8db60b5",
      "max": 1421700479,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_847743cff44e4829afa5ec81d8d9c237",
      "value": 1421700479
     }
    },
    "33288a3ef7e04f4c9fa43cfa376f942d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "335a3a8fed8740109e0c45867a2ca436": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3408be09aac6440dbbfea71f05802850": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8b42f1e530764f16bd1d969e44c63f23",
       "IPY_MODEL_04bf8b42a38c45268a65f8cb8cf223b0",
       "IPY_MODEL_45eb11d8aec44039bde98695099c3a4f"
      ],
      "layout": "IPY_MODEL_e97ab7e898984a4f85102646a59b5c11"
     }
    },
    "34373b5a19fb4efc8935c0917576ef77": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3481cf26144d495bb16512fb64f52abe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "34a59abe9953424aa9d8dd12e87a8a64": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "34c0130976f440a691a4b3a8362fc5b2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "357923f84865432d8f4a5c06b5311f09": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "36a4812c37204c98b257cc959a2b7278": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "372c77e8ca214939b29f7b1a61a60c48": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "37dbcc92e5a54e4f80b3b170296d8bb8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "38b93318e17d42c1bac8eacba74b81d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3940939e0d31433c8da8d355e1bee8c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_23b8aaecb2384c85a8d12a4cc39d513a",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b013050f923b45c3ab77eb7069cadf47",
      "value": 231508
     }
    },
    "39fe49065b394b85a79390ec03b0713e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3b20c1f15f104fef9d35b0f765670edc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9cb79e0763a54f1abce148fbdce93c59",
      "placeholder": "​",
      "style": "IPY_MODEL_ade46f9756bc415995865e9684681c7c",
      "value": "1_Pooling/config.json: 100%"
     }
    },
    "3b723473f56744339ac7fe2c84b2132d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3b7ca4878ae5411ca8b6271919f06223": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ae51e898667e48a9ad2da8ab65dac2e1",
      "max": 466247,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_08a06ba0111f4c90820fa7da2aeade27",
      "value": 466247
     }
    },
    "3bbe80f266ea43eeb6946c24242f6db6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3c1e8199c27d4024ba5e24e30d35e43c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_26419e544ac54982b4d6785fdcc3e806",
       "IPY_MODEL_596bd7c1d833426dab41ef79c4ca286d",
       "IPY_MODEL_159bf6a876f247bda3d8b6097739c1cb"
      ],
      "layout": "IPY_MODEL_084ec135b91a4b8285220d285809bbdc"
     }
    },
    "3c20237a88a443bf9ddb9d4436e0c513": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3c49589aa8f04a91884ca44d20aba1c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3c67e7effde948249b7f50f945de96fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3c9418cc42f44ea99ed22daeae979796": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6b97698f732247faa50e0b73c78e6ce4",
      "max": 898823,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_34373b5a19fb4efc8935c0917576ef77",
      "value": 898823
     }
    },
    "3cc329c0639847238b57297d47d36a96": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3ccdbbecfcb54492a15bc0e85f9f0d92": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_64bdfdf276a34818bca110ad18c96f02",
      "placeholder": "​",
      "style": "IPY_MODEL_c67a69bb0e0f4ac18792b71d1e1c5efd",
      "value": " 9.94G/9.94G [01:39&lt;00:00, 149MB/s]"
     }
    },
    "3d017f12eab847908ba278eae1898616": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3d0b077517804ee4876dc49c14e57aa0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3dc8124776044277a18cf2fcd5782403": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1e7bc085b40e40a8aa334d8cbb8c9304",
      "placeholder": "​",
      "style": "IPY_MODEL_4b64a670d8b54e7dabbf366be04be2a1",
      "value": " 10.7k/10.7k [00:00&lt;00:00, 241kB/s]"
     }
    },
    "3de167a1d20b4367a6bff8ea661edd7f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b26e7ad1b33a442c942bb1701c31ebd9",
      "placeholder": "​",
      "style": "IPY_MODEL_b28a3699532340498a50915bb860111a",
      "value": " 466k/466k [00:00&lt;00:00, 2.42MB/s]"
     }
    },
    "3e8927492a8e4af7aba8339da1d9a454": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3f0f2022b56a4853bf62f37d393ab5f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7426ddf30ae94afd92856cc87c31ea0c",
      "max": 53,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e503346ab4a347a39ba17ccb401bbebe",
      "value": 53
     }
    },
    "3fad7da98caf4aa090d820cbe6481af0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "40827ab62918499d987d6d2824b9ddd0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8a5bece63f1545c4ae5ac1d080596af4",
      "placeholder": "​",
      "style": "IPY_MODEL_12e9e208ec6e41beb10df311907ff5e3",
      "value": "vocab.txt: 100%"
     }
    },
    "4206b8c4e11445e180f91dee84f23a1a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "435e6838c65b4957953fc0ee88a9b882": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0dfa37c24f424776b3d15f8f0de1af0c",
      "placeholder": "​",
      "style": "IPY_MODEL_27f80615837649a68e938d7beecd3bf6",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "43a3872361cc497f8163cfb661265f73": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "43e8f6b20b1a422595b4cbf1103ee82c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6e59233f5b7c4d1682635be518b77d3b",
       "IPY_MODEL_74efa8e44d9e4c86a1f7758c3f62307d",
       "IPY_MODEL_2503801bdc884ab1b924299ea6b32219"
      ],
      "layout": "IPY_MODEL_c43e50e679ef44c59477e05523b6c1e0"
     }
    },
    "442b3a51bc49421f9a3598ac47a90711": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4483e4e4700a47399722045190e7d397": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4559eaab8b4e454f8611709bac60949d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "456e2cb3d1e44595b9cd298008811187": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_01fa84447757403d87609a97457097a9",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5c9bbc07ecb9430ea7e4a98ef70ab462",
      "value": 2
     }
    },
    "45814158a33b46efb1c5b3e5dd55f73e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f82d9ee7d5c041cb800769838a501c24",
      "placeholder": "​",
      "style": "IPY_MODEL_0dc891cc2eec4d27b73774782bca2d6b",
      "value": " 116/116 [00:00&lt;00:00, 7.68kB/s]"
     }
    },
    "45bc9824986747e7b4b9789a8166d9d1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "45eb11d8aec44039bde98695099c3a4f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_851e5131828742eca012c8bcd6a8e256",
      "placeholder": "​",
      "style": "IPY_MODEL_a5156a1f7ffb45b698488a70748779f2",
      "value": " 53.0/53.0 [00:00&lt;00:00, 1.51kB/s]"
     }
    },
    "46ebcdd0ed9d48c08952e6d137881455": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "48322e9a5bc540b4b66bd47cb608e848": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_51770412df7848478e714e41afa1ece7",
      "max": 112,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8400e9421545444d9c9aba39b976f0ab",
      "value": 112
     }
    },
    "4866cc1279194c27a916d1020d090f20": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cd7677fbbacd479f9048558a7da6e694",
      "placeholder": "​",
      "style": "IPY_MODEL_8bbe4457ad124a1ab7e93c3c21efd695",
      "value": " 4.54G/4.54G [00:53&lt;00:00, 38.0MB/s]"
     }
    },
    "48ee33c2012c42e8aaea3a558e808d8a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "48fd6a5a737d4832bc5349cab80abff6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4954ee9997f149dfa0c63dbb5fa36160": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8c7d23e2415241c984bc295fab2fab7d",
      "placeholder": "​",
      "style": "IPY_MODEL_33288a3ef7e04f4c9fa43cfa376f942d",
      "value": " 53.0/53.0 [00:00&lt;00:00, 1.27kB/s]"
     }
    },
    "49653cc5c85d4ded885e47ba760c0dc3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "499c89fd510f42e88ea5e63f484ec803": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_49653cc5c85d4ded885e47ba760c0dc3",
      "placeholder": "​",
      "style": "IPY_MODEL_04f229f8b5d847e9bfed5ffd6b78278d",
      "value": " 53.0/53.0 [00:00&lt;00:00, 3.28kB/s]"
     }
    },
    "49b9f8f3d38b4094bee6916d5318a445": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "49eac13e310440378d596881537beb66": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4aa057719e8f484695edb384033c11af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c4add13e56ec48dfb05f4f1a39fa4efb",
      "placeholder": "​",
      "style": "IPY_MODEL_8f1cb51519294dcd87652085ca49e697",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "4b64a670d8b54e7dabbf366be04be2a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4bd9def6f5ea4982984dfa177d864517": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4d2f67dc24794cd3ad8101a98c794c4b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4dbc82dae5c948aab3ec38d28b67d53e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4e6627df1f3e455d85e07c9a44f8ef52": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4fe1127f030146faa8640301730cbb76": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4fe39f51db8c4e65b41e0441f3e0a9b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "50434e7d42f04ca4a85ca72aeb632d9d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1554078f33ec4d45859c5dc6d2df1419",
      "placeholder": "​",
      "style": "IPY_MODEL_997e6e29b71c40a2a6fa2980fec6f6ca",
      "value": "vocab.json: 100%"
     }
    },
    "504401a87e23485784ccf9d3957a37da": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "506a2c798745430d8ab8d9eddac7c58b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "508290f007d84574b9f44bcff490bff4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "50bc4ec3d2a7486a896b7a78b9ed7c3a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0a6f16dacd3042aa8cdbb9a1b151b643",
      "placeholder": "​",
      "style": "IPY_MODEL_eeb7d672f16f452e98b8b9cf87100950",
      "value": "config.json: 100%"
     }
    },
    "516a5a39e2ca4799ac1866d67f3ab8de": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5cdf8ce4cede446db92df6a68e68ce52",
      "placeholder": "​",
      "style": "IPY_MODEL_5cb94b304b8d4e7cba498cda5b80417e",
      "value": " 612/612 [00:00&lt;00:00, 13.7kB/s]"
     }
    },
    "51770412df7848478e714e41afa1ece7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "53478135a2d847beb30b6ae61e2d12fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c6acb332e87942939a4b82c89e17e5fc",
       "IPY_MODEL_456e2cb3d1e44595b9cd298008811187",
       "IPY_MODEL_e6595872d82046229b0dbe70a9fd199e"
      ],
      "layout": "IPY_MODEL_970ae7b9ad144e6682bad2f38ec98cba"
     }
    },
    "53975da8ecb5463d873d4674629b180d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "544ce0bb6bdc405e998e0c2f19342a81": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6e85ce89f1a34beaa02dbd09087f516a",
      "placeholder": "​",
      "style": "IPY_MODEL_7a7c753cdfb3430bada12cb00b810819",
      "value": "1_Pooling/config.json: 100%"
     }
    },
    "546d274303574956a1fa8f7b409e4347": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "548f91976638484eac3db53e1390107e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "54e4009c06844525874494f303ba2b86": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5534bdabb6a749bfb9caec514e9ffa49": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "55b8ea09548641d49a1d50109dab779f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "55d3a2219935428a9b559e8b64ad223a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "564074528f84422b8015f8ef202c5bde": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "56b5cae7eb0d46cabfb122d5a599ce36": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "573b6c54ff6c4ce6b9669b420c670c59": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5904f6edf45e463981cdb2e64cd5eed0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "592a25432dba4625bfdb3ba0ee7054c1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "596bd7c1d833426dab41ef79c4ca286d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5a66cca774ad48a69b2364989926576b",
      "max": 612,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_20b6c8bf2505415ea451b2c2fdf28cbd",
      "value": 612
     }
    },
    "5a3c97d479184433b8bc063dfbbadc1b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5a410e134013469ba387648575d5b944": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5a66cca774ad48a69b2364989926576b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5b0ba54016b3462fb321037162eec2b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5b60e1c5ac11430da1f590750d8ec497": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5c415e2cf2494a07bd4385bc75327b66": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9cb6f875988b4d22a89250c75bea05a2",
       "IPY_MODEL_32b3c3584eb74b698d4f891932f2f906",
       "IPY_MODEL_a04dcb05cc21489d9e47a5cc6733ea75"
      ],
      "layout": "IPY_MODEL_9c23448a32d24449a55421e9b8d2419a"
     }
    },
    "5c9bbc07ecb9430ea7e4a98ef70ab462": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5cb94b304b8d4e7cba498cda5b80417e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5cd5d029725847119ee7ffbdaa6969ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_36a4812c37204c98b257cc959a2b7278",
      "max": 112,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f2d526cf04284a79862b3b454b0dc216",
      "value": 112
     }
    },
    "5cdf8ce4cede446db92df6a68e68ce52": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5d331d5b6973462f9883747f8b4c4ab8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a66e3f2c98c647daa227659755a11c4e",
      "placeholder": "​",
      "style": "IPY_MODEL_d55a9624c1984862b1d3a89a9b56574f",
      "value": " 571/571 [00:00&lt;00:00, 8.97kB/s]"
     }
    },
    "5e965613c8034827a9dbd2a4b8270ff4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5e9a6c88452c402f9afbf550d7670add": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5ee303ba267e4294a8d5b72a16a886c3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5fa0665b0f294c0dad16ab252c132809": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6529e68e94174966ad9ceaf3024b2825",
      "max": 10659,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b22856d9b37d43b1950a48f256afb037",
      "value": 10659
     }
    },
    "6015887cf6c5459080bde5b0c83442b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ff86ec4b2122413b96e913eb618abae2",
      "max": 4540516344,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f039047818de46e98962f3d9fb18611b",
      "value": 4540516344
     }
    },
    "6233e142295b4387ab8438288a94fa08": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "62781bbe27384f2197cbc935d5e8423d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_40827ab62918499d987d6d2824b9ddd0",
       "IPY_MODEL_3940939e0d31433c8da8d355e1bee8c5",
       "IPY_MODEL_f5f62b1b55fc4b8a9100015de19f22cc"
      ],
      "layout": "IPY_MODEL_8e2ed5154fd4415a8bd5f4437ddc2e55"
     }
    },
    "62bc8182837c4825b1ac3c7ea326cf88": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "633303b3635f48a58de1c4a4d0a8dd47": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "644b24f94f85412b8feef0f625ef8fa5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a6a38030c9bb41958543a91f72b4d6cc",
      "max": 350,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7a36340e31fd49beb920bad19aef01a6",
      "value": 350
     }
    },
    "64af9d28455e41cdb6164684ebf38d07": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d84cdde777c742249ddd605bc4587a21",
      "placeholder": "​",
      "style": "IPY_MODEL_680f4ae2881a47a98d3b0dca816dc3bf",
      "value": "generation_config.json: 100%"
     }
    },
    "64bd611054c54bc493a936dd87a316f0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "64bdfdf276a34818bca110ad18c96f02": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6529e68e94174966ad9ceaf3024b2825": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "653af0f977154c2b8a5336893efac036": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_25450062f6894e3487bf642c568f618b",
      "placeholder": "​",
      "style": "IPY_MODEL_4206b8c4e11445e180f91dee84f23a1a",
      "value": " 10.7k/10.7k [00:00&lt;00:00, 235kB/s]"
     }
    },
    "6631c98379f64b20927e621c5397b27c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f2c85271acf8462ebec8ce09fd057b2f",
      "placeholder": "​",
      "style": "IPY_MODEL_bc3ffd2d1e8b485f9c2111848f810a0a",
      "value": "vocab.txt: 100%"
     }
    },
    "664db7525bfb457a9fbffdd31908c52b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_592a25432dba4625bfdb3ba0ee7054c1",
      "placeholder": "​",
      "style": "IPY_MODEL_09803fd745254cb9901e94411bb71540",
      "value": " 349/349 [00:00&lt;00:00, 24.0kB/s]"
     }
    },
    "6688ecfa60ac43ba8ed96530e18d231f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "669fe02d06e64c7092110781e5f1e6f4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6740dbdd02fd4861af1b9c9410a29ecc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "67b79b611a32439fb6c9e16436b8dd19": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "680f4ae2881a47a98d3b0dca816dc3bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "68aa905c7ba24fcebe224084baaead28": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a5ebf730d7424f19989a6da524a25fbb",
       "IPY_MODEL_f29e5a964cb34c9b8c137f66eb19bbb1",
       "IPY_MODEL_c9865b9309be4d6b97faa80e91445254"
      ],
      "layout": "IPY_MODEL_0a0199431c1f4f07b1741468c9f1d114"
     }
    },
    "6926235886e64a29a4bd945c083ed89f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e66dae1012b54a4a98f223c0560b695f",
       "IPY_MODEL_833b764e721b4304aaaa97a3f5d0a399",
       "IPY_MODEL_664db7525bfb457a9fbffdd31908c52b"
      ],
      "layout": "IPY_MODEL_9d82de220e4f44ff8eff707bb4ab0691"
     }
    },
    "69bf13c4cece4f3eab1219b4788e058a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_993b525d8f304fb89faacabb5d3b4b53",
      "placeholder": "​",
      "style": "IPY_MODEL_357923f84865432d8f4a5c06b5311f09",
      "value": " 350/350 [00:00&lt;00:00, 12.5kB/s]"
     }
    },
    "69d74a46c49949e495aaba2a0da1d6e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_13f7f3bf21014a2685cde4f315b5c592",
      "placeholder": "​",
      "style": "IPY_MODEL_7fef0203903549ddb962c57057236b40",
      "value": " 1.36M/1.36M [00:00&lt;00:00, 7.61MB/s]"
     }
    },
    "6a07a9da13da4766a8543afaef5c2cbf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6ab40f4d10314413abb7a1435900c596": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6b97698f732247faa50e0b73c78e6ce4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6bb7bb647ade4cf28ae0dc9bd873dc32": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6be7a10dc856492688d24802f8db60b5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6c894160f44b4db09bec1e82dce4f325": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6ca465c5bcd746efb3d8d6db4defaa0d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6cb66d7257e94658a61eb5b0b92242f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_addf6ae6f9cd4227aa0fb77bfc43edad",
      "max": 612,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4fe39f51db8c4e65b41e0441f3e0a9b1",
      "value": 612
     }
    },
    "6e59233f5b7c4d1682635be518b77d3b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7a0306794ad046b3a1204c4b9bd23818",
      "placeholder": "​",
      "style": "IPY_MODEL_5904f6edf45e463981cdb2e64cd5eed0",
      "value": "vocab.txt: 100%"
     }
    },
    "6e85ce89f1a34beaa02dbd09087f516a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6f05b02d95234ba69153d44c5a894835": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0cc2893ef4444bb296f114cf26c95aa4",
      "placeholder": "​",
      "style": "IPY_MODEL_cbba1669ed414410afedd9052f71e8d2",
      "value": " 482/482 [00:00&lt;00:00, 4.91kB/s]"
     }
    },
    "6f961b89a2944740925b283f7dce55ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_92a499f69c5046b4be9e5795fccd44ce",
      "placeholder": "​",
      "style": "IPY_MODEL_4fe1127f030146faa8640301730cbb76",
      "value": "model-00001-of-00002.safetensors: 100%"
     }
    },
    "6fe6c615e8804248b83a538c52d904c7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "705eeadc71be452b9779623d08f0c0c6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_548f91976638484eac3db53e1390107e",
      "max": 612,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6ca465c5bcd746efb3d8d6db4defaa0d",
      "value": 612
     }
    },
    "712d125baed4441b93e607312ec83cc8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "71625362a863438e9dc3dcd5f71b952a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_03b1a1106cc8493b971c534c9413a010",
      "placeholder": "​",
      "style": "IPY_MODEL_741e8028c3b64415b01c0a658ea5d600",
      "value": " 112/112 [00:00&lt;00:00, 8.10kB/s]"
     }
    },
    "71b202d1efef470a8a75892b30618c69": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "71f57f5bf5d3449fb9dd0ecd97f4710d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9828a559d1b44511b106ed2a0f9fe1dd",
      "placeholder": "​",
      "style": "IPY_MODEL_5b60e1c5ac11430da1f590750d8ec497",
      "value": " 116/116 [00:00&lt;00:00, 6.25kB/s]"
     }
    },
    "721b98900e11438b8a13c98d8f442fc2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e1ef57d034024c739b00365e46eb3dfb",
      "max": 1467,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_eb23096a775e46bd8511af85da94bef2",
      "value": 1467
     }
    },
    "72c48a1476ab475785943437d1542911": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "736d57d7e889422194dab51dab7ab065": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "738d7abc107d4606a0cd84c67d63ab6e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "741e8028c3b64415b01c0a658ea5d600": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7426ddf30ae94afd92856cc87c31ea0c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "74efa8e44d9e4c86a1f7758c3f62307d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1e15f4e6cbda4f8298540c7a4991de85",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9983239b3a3c47da8599b0854201012b",
      "value": 231508
     }
    },
    "75120fbec53943e8a30e589a4b26c2ba": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7592f515d48346b380c03d1ebe979350": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "75e2eb26982f42ceb9de039e04b82a48": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5ee303ba267e4294a8d5b72a16a886c3",
      "placeholder": "​",
      "style": "IPY_MODEL_a264344c05d043feb64d4cabb7d03233",
      "value": " 232k/232k [00:00&lt;00:00, 3.01MB/s]"
     }
    },
    "75e56dce7ea34cfca43bb8aa44784900": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "760dfceff5894ab18c653725dccaf2e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "762db47a0b644102a206d50917cacd68": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7696d9f0266e4c10aba176e7e431681d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "772b3cc1599340d987c3e9f9d5bd00f0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "774e8740090f4438be20e43a9c9cf6e3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9dfd5c5b013340b2bc2ecaf0907a3e36",
      "placeholder": "​",
      "style": "IPY_MODEL_3c20237a88a443bf9ddb9d4436e0c513",
      "value": " 466k/466k [00:00&lt;00:00, 940kB/s]"
     }
    },
    "7894fd052aef4c0892a44854d60a3634": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "78b8805aa3a545d3bad8a432337bd554": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "793bd871a2374c2ebf658af2558c8b43": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7956b5f990954a7fa1d71aa0c42e8133": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d479615aabfb4aa1b0594625f0d1bf32",
      "max": 350,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a25a24976e8e4b7c857c29de4045e886",
      "value": 350
     }
    },
    "79b06fa4ee174dd283897d9bfce1d461": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_738d7abc107d4606a0cd84c67d63ab6e",
      "max": 190,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d2dfa246e4b64d13920220457de1a8d9",
      "value": 190
     }
    },
    "7a0306794ad046b3a1204c4b9bd23818": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7a0e619549a749c8b56fadc7c2b6bb77": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7a36340e31fd49beb920bad19aef01a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7a5604969c9a481ab6e41079660042b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7a7c753cdfb3430bada12cb00b810819": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7a8139029aaf4eb49f249ac7c9d182cd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7b37d6a232644f25b2ff23cc5dd7abc8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2236d0f93d464e50bed2231544fa7ba5",
       "IPY_MODEL_7956b5f990954a7fa1d71aa0c42e8133",
       "IPY_MODEL_b02339f3f5aa40a8858770995468e2a7"
      ],
      "layout": "IPY_MODEL_f1ba919e78e84da4b69a4d510126ff6b"
     }
    },
    "7c6c6f53d0c84d3ebf5ab6a3622becab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_313af608a8e44338b4baee58091a5edd",
      "placeholder": "​",
      "style": "IPY_MODEL_0e2cd61b60974115ac8beae93ebd4287",
      "value": " 10.7k/10.7k [00:00&lt;00:00, 770kB/s]"
     }
    },
    "7ca5b258f67e46a185ca8fa7e647ca50": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7cfefd424b304890a5894e83368e7bcb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7dbcb5ce2143486495089ed89aa6900e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fb95312965944faf8e57f7a0b700145d",
      "max": 466247,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cc403af917244162b84481df711795f6",
      "value": 466247
     }
    },
    "7dd4e6bed42b4fbe919fc565203b63a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_506a2c798745430d8ab8d9eddac7c58b",
      "placeholder": "​",
      "style": "IPY_MODEL_3d017f12eab847908ba278eae1898616",
      "value": " 493k/493k [00:00&lt;00:00, 16.3MB/s]"
     }
    },
    "7e3ae2ca80e54433b5bad299692f3788": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7e505515d41d459eb8d2341c0bf9ef34": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6c894160f44b4db09bec1e82dce4f325",
      "placeholder": "​",
      "style": "IPY_MODEL_34a59abe9953424aa9d8dd12e87a8a64",
      "value": " 116/116 [00:00&lt;00:00, 3.82kB/s]"
     }
    },
    "7eb36154990b4732b9897391cc156f96": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7ec4029f051a48ce9e838d6f990b647e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b2d5a7480bde43c28c756445bcd5cdfd",
      "placeholder": "​",
      "style": "IPY_MODEL_eab1a1b83de14827ae242fee5ff91540",
      "value": "tokenizer.json: 100%"
     }
    },
    "7f5bfb837c034badb4987642a441cafc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7f7a8e3453734b388f0e7380ace7e339": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7fef0203903549ddb962c57057236b40": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "80b08127f5b14a4e92dd07800924ad17": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "80b1613895124a84b9323077d1309946": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "80ed477043a34061af1d0329def8c336": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_772b3cc1599340d987c3e9f9d5bd00f0",
      "placeholder": "​",
      "style": "IPY_MODEL_a57a9ffe8d234ada849669fa408f8bb1",
      "value": "config.json: 100%"
     }
    },
    "811258b1e9d14978a045a17bfa0fac76": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8194ebcecbf244688b70a771de7cf75b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_75120fbec53943e8a30e589a4b26c2ba",
      "placeholder": "​",
      "style": "IPY_MODEL_876a8d99290a4e37b1c780f91fcfeda2",
      "value": "config_sentence_transformers.json: 100%"
     }
    },
    "8273312808344fe6bbcee1d2f5ca3943": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_64af9d28455e41cdb6164684ebf38d07",
       "IPY_MODEL_986baef623e3412d83f12636a77c3f73",
       "IPY_MODEL_71f57f5bf5d3449fb9dd0ecd97f4710d"
      ],
      "layout": "IPY_MODEL_78b8805aa3a545d3bad8a432337bd554"
     }
    },
    "82b57df2e40549b495e247e0602004eb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "833b764e721b4304aaaa97a3f5d0a399": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0ef5557a48864cf390c9f63ab394636f",
      "max": 349,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f8a9268a04034142b3a3ace39c75753f",
      "value": 349
     }
    },
    "83f723e718214633a41be0b88f53ae4d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8400e9421545444d9c9aba39b976f0ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "847743cff44e4829afa5ec81d8d9c237": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8486056394b1413cb6d0326acbb50667": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "851e5131828742eca012c8bcd6a8e256": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8556115ec56b40b8b3be5fb25921c6d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e6ecb61772d641ac9d0c20ddacd4a7d8",
       "IPY_MODEL_1108367e725c4b16970238fb2cd27ca1",
       "IPY_MODEL_69d74a46c49949e495aaba2a0da1d6e6"
      ],
      "layout": "IPY_MODEL_45bc9824986747e7b4b9789a8166d9d1"
     }
    },
    "85d5a23a50c548158a56b0fb2cd70ae1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "861d51b05a4f440b83239aeb60ca3f65": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "87027f1e6313419ca41f744414d0280b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8718e08801544720a85064cd4928a93c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "876a8d99290a4e37b1c780f91fcfeda2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "882aef82a3fd42f3a6b310e63e5dfe50": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9a934b589c8b4b1a8be18814fdbc11f8",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cd3adb62488d414b8b7f359786129f17",
      "value": 2
     }
    },
    "883d21a8ddf4436daef905e64998a513": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8a5bece63f1545c4ae5ac1d080596af4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8a72a0ca0171424bab6fdd9e4a223bb7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a15aa03eaf6745ca832eca411158e398",
      "placeholder": "​",
      "style": "IPY_MODEL_b0ae4b8fcc544579be40f15248bc326e",
      "value": " 112/112 [00:00&lt;00:00, 4.51kB/s]"
     }
    },
    "8ad266de548b408face2edc4b1862d7b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_39fe49065b394b85a79390ec03b0713e",
      "placeholder": "​",
      "style": "IPY_MODEL_f533e63c0dde4067899e4aeddda25c4c",
      "value": " 2/2 [02:33&lt;00:00, 72.90s/it]"
     }
    },
    "8b42f1e530764f16bd1d969e44c63f23": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_00400719a0904489aed145e287eedbee",
      "placeholder": "​",
      "style": "IPY_MODEL_cc7916202f89433196c32424d9c7ced7",
      "value": "sentence_bert_config.json: 100%"
     }
    },
    "8bbe4457ad124a1ab7e93c3c21efd695": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8c4dcb776cdf4426968093252bc5d631": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8c7d23e2415241c984bc295fab2fab7d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8d1010a5fd524dcfa2b4ebbfca3d758c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8e23598a3334441a82d0f7c4b3eff0f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8e2ed5154fd4415a8bd5f4437ddc2e55": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8f1cb51519294dcd87652085ca49e697": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8f2b0ccefffe48868e1f15fd232de716": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8f3a0ac9cd5e44508434e1aaa13c5ccd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3b723473f56744339ac7fe2c84b2132d",
      "placeholder": "​",
      "style": "IPY_MODEL_fe48ac8a18bd4042ab10a4ce2a19d75d",
      "value": "config_sentence_transformers.json: 100%"
     }
    },
    "8f3ce5ac92a84bb194fe4145ea108400": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9026567365b94e94b5f799cba90265ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7894fd052aef4c0892a44854d60a3634",
      "placeholder": "​",
      "style": "IPY_MODEL_1f5b84c9567e4d67b8dec40284c4a326",
      "value": " 349/349 [00:00&lt;00:00, 10.2kB/s]"
     }
    },
    "90f39fbec5774c24bca6094d96ab26ff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9145d0e2f8f34806b847a5d22066a8dd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "92a499f69c5046b4be9e5795fccd44ce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "934879e22b324c6e8d2a5fa50bafc891": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f2f75194a2ce4c78aaf09a2c6750ab93",
      "max": 53,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7e3ae2ca80e54433b5bad299692f3788",
      "value": 53
     }
    },
    "93d0aeb39303473391d4b3ececefaa99": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "94cd614cea4a4dd1b60080a69e2c3b83": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a8847a3075b34dc9b74fb17807b1e214",
      "max": 9942981696,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_55d3a2219935428a9b559e8b64ad223a",
      "value": 9942981696
     }
    },
    "9508ac23781042dbba6ece95f3eb3f3a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7a0e619549a749c8b56fadc7c2b6bb77",
      "placeholder": "​",
      "style": "IPY_MODEL_4559eaab8b4e454f8611709bac60949d",
      "value": "model.safetensors: 100%"
     }
    },
    "970ae7b9ad144e6682bad2f38ec98cba": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "97db510554f04266835dedb6da450e08": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "97f5cba66e764e6e8a5dab9caf08c3bc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9828a559d1b44511b106ed2a0f9fe1dd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "98664e3b3ae74ccea5b4d2be8d6fac80": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a5ea699b5e274823beaa1cc80be7274c",
       "IPY_MODEL_934879e22b324c6e8d2a5fa50bafc891",
       "IPY_MODEL_499c89fd510f42e88ea5e63f484ec803"
      ],
      "layout": "IPY_MODEL_e010fafde1f24938beb913566bcdcaae"
     }
    },
    "986baef623e3412d83f12636a77c3f73": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_87027f1e6313419ca41f744414d0280b",
      "max": 116,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_372c77e8ca214939b29f7b1a61a60c48",
      "value": 116
     }
    },
    "98b7173dcbec416299d39c5da6971ccf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_314278a71ef946988bb8f5fc97fadfa4",
       "IPY_MODEL_3f0f2022b56a4853bf62f37d393ab5f0",
       "IPY_MODEL_4954ee9997f149dfa0c63dbb5fa36160"
      ],
      "layout": "IPY_MODEL_b9cfdcf1172c467a9430fab02aebee93"
     }
    },
    "993b525d8f304fb89faacabb5d3b4b53": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "997e6e29b71c40a2a6fa2980fec6f6ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9983239b3a3c47da8599b0854201012b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9a934b589c8b4b1a8be18814fdbc11f8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9adb71f09145493d9cf75f15b4d66f36": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_288829dde7444ecea0c7590531881e1a",
       "IPY_MODEL_9fc629fd6f98499086f9ea106eeccaae",
       "IPY_MODEL_a91b290b13b74e7c8b5464bc07be0d69"
      ],
      "layout": "IPY_MODEL_2d3172fe38c6481fa3451b659007a843"
     }
    },
    "9b6461ed30224cf3a18ad36dae0c7023": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9bec41c835444f95aa27833c05bcdfea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_055fc71161734a2291d46496d34bd333",
       "IPY_MODEL_da726cc536d247c8865b82ad1cdf7b16",
       "IPY_MODEL_de1249f3361e4dc287ec7dbc055cd876"
      ],
      "layout": "IPY_MODEL_80b1613895124a84b9323077d1309946"
     }
    },
    "9c23448a32d24449a55421e9b8d2419a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9cb6f875988b4d22a89250c75bea05a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c081af5952044ee29799019f65e91678",
      "placeholder": "​",
      "style": "IPY_MODEL_14579babec1f4a478b990726bce224fd",
      "value": "model.safetensors: 100%"
     }
    },
    "9cb79e0763a54f1abce148fbdce93c59": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9cf15f64612346fa91c9479bc0881e3a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9d578c44f7e341c28c5125f8bdecf3b0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9d82de220e4f44ff8eff707bb4ab0691": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9dfd5c5b013340b2bc2ecaf0907a3e36": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9e63843a93f14446a02026c1d29167a4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9f5feea658c044ca8f725685d44c53e7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eaaa36fed39f4127a983fd6a0567babe",
      "max": 116,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7cfefd424b304890a5894e83368e7bcb",
      "value": 116
     }
    },
    "9f658dda4e71401db3ae921d8b978c4c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8718e08801544720a85064cd4928a93c",
      "placeholder": "​",
      "style": "IPY_MODEL_110532eccf6847df92563cf649efb47b",
      "value": "tokenizer.model: 100%"
     }
    },
    "9fc629fd6f98499086f9ea106eeccaae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_be7ccdd3bd5e4a7ab71df04aaebf18fb",
      "max": 72,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_df918b4f89ca496d9f2c5b22c6e096e7",
      "value": 72
     }
    },
    "9fc83b6769b0489a83ac6a5853cfe7a4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a021d0a6f40b4b53b8c81279ad657a0f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a04dcb05cc21489d9e47a5cc6733ea75": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8d1010a5fd524dcfa2b4ebbfca3d758c",
      "placeholder": "​",
      "style": "IPY_MODEL_5e965613c8034827a9dbd2a4b8270ff4",
      "value": " 1.42G/1.42G [00:15&lt;00:00, 132MB/s]"
     }
    },
    "a060449aff4d48cfb4c87293ac2813b0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a0add11803794b2eb1061baa6b15e77b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_54e4009c06844525874494f303ba2b86",
      "placeholder": "​",
      "style": "IPY_MODEL_ee175110fea7417f855dadef69e1a64e",
      "value": " 90.9M/90.9M [00:00&lt;00:00, 197MB/s]"
     }
    },
    "a15aa03eaf6745ca832eca411158e398": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a16af98067c14bfd8cca0058008806f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a1c5bc04620c440e9e12b2a5cd082e4d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b414adada5384d4880830339139d38e4",
       "IPY_MODEL_0742615e751f4e7b9278237ba28c4a80",
       "IPY_MODEL_3de167a1d20b4367a6bff8ea661edd7f"
      ],
      "layout": "IPY_MODEL_e29e81fb1be4460d8d277eb074a26a3c"
     }
    },
    "a1fd3c805831464ba8f5b175b293ea60": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c3ed1caeb4af4627a60201cce9420f07",
      "placeholder": "​",
      "style": "IPY_MODEL_2f3b88aa761040c19eafa325ea877f10",
      "value": "model.safetensors.index.json: 100%"
     }
    },
    "a25a24976e8e4b7c857c29de4045e886": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a264344c05d043feb64d4cabb7d03233": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a2eea0216f3a421c974cef798f7ffc4d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cf8eed43356247c0a8cf80abbc80ccf5",
      "max": 493443,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3fad7da98caf4aa090d820cbe6481af0",
      "value": 493443
     }
    },
    "a4348c77df0e421d81291ef10f1397d2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a486fcff9502442e9f68619201b98673": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a48ecf53832b404282c0ec146a665976": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1e225b5bdee34454ab860722c41cc286",
       "IPY_MODEL_20a10315b3134e868029987222642232",
       "IPY_MODEL_7e505515d41d459eb8d2341c0bf9ef34"
      ],
      "layout": "IPY_MODEL_736d57d7e889422194dab51dab7ab065"
     }
    },
    "a5156a1f7ffb45b698488a70748779f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a57a9ffe8d234ada849669fa408f8bb1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a5c5577478b04951a6c3093b175c8b79": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a5d12e37ee2e42bcabb296aa7642b15d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a5ea699b5e274823beaa1cc80be7274c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b6e2b14696f347e4b4082f4ab3039b0e",
      "placeholder": "​",
      "style": "IPY_MODEL_4483e4e4700a47399722045190e7d397",
      "value": "sentence_bert_config.json: 100%"
     }
    },
    "a5ebf730d7424f19989a6da524a25fbb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_04f435a02ed0473cbb2c20286f7d69c0",
      "placeholder": "​",
      "style": "IPY_MODEL_6740dbdd02fd4861af1b9c9410a29ecc",
      "value": "modules.json: 100%"
     }
    },
    "a61fb28b1c154e22895ce8b67a29f119": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1b23976143f643c08871d5c17f364036",
       "IPY_MODEL_f2257b285f4d4f4b83b602d7cc6eaaf8",
       "IPY_MODEL_69bf13c4cece4f3eab1219b4788e058a"
      ],
      "layout": "IPY_MODEL_861d51b05a4f440b83239aeb60ca3f65"
     }
    },
    "a66e3f2c98c647daa227659755a11c4e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a69d1966927b4d0db0040b3f1ff21d24": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0e74588e049842a2a207d8aaad3b4c9b",
       "IPY_MODEL_ab16f60256ae43f384d319af11e48d63",
       "IPY_MODEL_9026567365b94e94b5f799cba90265ae"
      ],
      "layout": "IPY_MODEL_20dfb400adc040bf85aa7b9d179ef46e"
     }
    },
    "a6a38030c9bb41958543a91f72b4d6cc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a78dbd71ec4f4146acb6561bdadcb9b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9508ac23781042dbba6ece95f3eb3f3a",
       "IPY_MODEL_2ba5b9e374e348aeb8c3fbf36ba3bb88",
       "IPY_MODEL_18115621275a49148f81f9bb73f059d6"
      ],
      "layout": "IPY_MODEL_eab95a3f9b5e49e0a05e36211cca9693"
     }
    },
    "a7c1dd526dc64b2899545a0e84e63014": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a83a1c079bf347a29481d080558c2ae6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a86c0daf8f554702a794e68160114cb5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_72c48a1476ab475785943437d1542911",
      "placeholder": "​",
      "style": "IPY_MODEL_508290f007d84574b9f44bcff490bff4",
      "value": " 116/116 [00:00&lt;00:00, 7.33kB/s]"
     }
    },
    "a8847a3075b34dc9b74fb17807b1e214": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a91b290b13b74e7c8b5464bc07be0d69": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8486056394b1413cb6d0326acbb50667",
      "placeholder": "​",
      "style": "IPY_MODEL_00ba5672234a4879b0463f784b832b1e",
      "value": " 72.0/72.0 [00:00&lt;00:00, 4.01kB/s]"
     }
    },
    "aa4b86f1504449b4956fd2155335937f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_442b3a51bc49421f9a3598ac47a90711",
      "placeholder": "​",
      "style": "IPY_MODEL_6a07a9da13da4766a8543afaef5c2cbf",
      "value": "model-00002-of-00002.safetensors: 100%"
     }
    },
    "aa71965ed74b4f609c95ba1a3e4b9163": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bf656aa894cf4e6f9ef12c69dbc9c881",
       "IPY_MODEL_79b06fa4ee174dd283897d9bfce1d461",
       "IPY_MODEL_ed01f31687df4240bb4c5ab0f122d827"
      ],
      "layout": "IPY_MODEL_a5d12e37ee2e42bcabb296aa7642b15d"
     }
    },
    "ab16f60256ae43f384d319af11e48d63": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a060449aff4d48cfb4c87293ac2813b0",
      "max": 349,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3c67e7effde948249b7f50f945de96fe",
      "value": 349
     }
    },
    "ab5338e1673044a7a535241202bfca1f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f71d0b1a2e53451696fe1abae9cd4bb6",
       "IPY_MODEL_5cd5d029725847119ee7ffbdaa6969ce",
       "IPY_MODEL_71625362a863438e9dc3dcd5f71b952a"
      ],
      "layout": "IPY_MODEL_e092acfe72b84945bafcfa53cdfe69b1"
     }
    },
    "ad6bfab680804b329345964c6f8bd8ba": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "addf6ae6f9cd4227aa0fb77bfc43edad": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ade46f9756bc415995865e9684681c7c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ae0cd7cd24af4b1baec8dff607c082d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f41dc475cde447efb6af54340a0f0fd7",
      "max": 10659,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a83a1c079bf347a29481d080558c2ae6",
      "value": 10659
     }
    },
    "ae4ab0cbd7dc45c9ad96163ab3d1dc91": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ed4fe0f8fd6145c7b0b8e1e6e7cd6206",
       "IPY_MODEL_02e4827845d7458b850f63b63be1bbdd",
       "IPY_MODEL_a0add11803794b2eb1061baa6b15e77b"
      ],
      "layout": "IPY_MODEL_85d5a23a50c548158a56b0fb2cd70ae1"
     }
    },
    "ae51e898667e48a9ad2da8ab65dac2e1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aea1b1fb4cf64fefbdc388ec74b22287": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f4f2719436674ce88d287701ebf9516c",
       "IPY_MODEL_6cb66d7257e94658a61eb5b0b92242f9",
       "IPY_MODEL_d88cc523ee5c43598ba2046e539cc8d3"
      ],
      "layout": "IPY_MODEL_5a410e134013469ba387648575d5b944"
     }
    },
    "aed3137980ba4d19937bc696ef6bc8b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e0756b4f857941699a84ccadd8ee5950",
      "placeholder": "​",
      "style": "IPY_MODEL_3d0b077517804ee4876dc49c14e57aa0",
      "value": " 899k/899k [00:00&lt;00:00, 3.81MB/s]"
     }
    },
    "af754c24722147dbbe40680118e19bcb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a1fd3c805831464ba8f5b175b293ea60",
       "IPY_MODEL_faefbaa58a5449fb8d308340131d4ec7",
       "IPY_MODEL_00298c86bd5347a7aa8ebe3d2eae51b2"
      ],
      "layout": "IPY_MODEL_31931eb718934dc793f937ddacb2083b"
     }
    },
    "af8f7de1b7154b17885f78b5b5574b7a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b013050f923b45c3ab77eb7069cadf47": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b02339f3f5aa40a8858770995468e2a7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f4493d99c0174cd2a0320dca66ccc921",
      "placeholder": "​",
      "style": "IPY_MODEL_55b8ea09548641d49a1d50109dab779f",
      "value": " 350/350 [00:00&lt;00:00, 7.55kB/s]"
     }
    },
    "b0ae4b8fcc544579be40f15248bc326e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b22856d9b37d43b1950a48f256afb037": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b26e7ad1b33a442c942bb1701c31ebd9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b28a3699532340498a50915bb860111a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b2d5a7480bde43c28c756445bcd5cdfd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b414adada5384d4880830339139d38e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_29f741fffd404c8c87df8fa7a62d8d36",
      "placeholder": "​",
      "style": "IPY_MODEL_4e6627df1f3e455d85e07c9a44f8ef52",
      "value": "tokenizer.json: 100%"
     }
    },
    "b4e962948e2e4c0b903529ccd579b9d5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b6e2b14696f347e4b4082f4ab3039b0e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b7a0d02ad2f3466c90e54ee2de63d91b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_34c0130976f440a691a4b3a8362fc5b2",
      "max": 190,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_31ebf07a34694748ac87d3ca339259a7",
      "value": 190
     }
    },
    "b83e0e5894694527915b0877105d68d5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b9af1242884d4a05a90261513d42b330": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6631c98379f64b20927e621c5397b27c",
       "IPY_MODEL_0956b86c701f436e844f0c37aee45662",
       "IPY_MODEL_75e2eb26982f42ceb9de039e04b82a48"
      ],
      "layout": "IPY_MODEL_0aefb5ec218c4048b660662b54cec77f"
     }
    },
    "b9cfdcf1172c467a9430fab02aebee93": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ba5fc8c18c5b4f47a2a7eaeae5278591": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bbf6a3dc755544909038770967ac7774": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "bc3ffd2d1e8b485f9c2111848f810a0a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "be7ccdd3bd5e4a7ab71df04aaebf18fb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bf17bd1fa8a943ed93c564f691edf5f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_19a8531b90114368bd0c6298f136d760",
      "placeholder": "​",
      "style": "IPY_MODEL_49b9f8f3d38b4094bee6916d5318a445",
      "value": " 190/190 [00:00&lt;00:00, 10.9kB/s]"
     }
    },
    "bf656aa894cf4e6f9ef12c69dbc9c881": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_03ac455559734c06ab934bdba9229262",
      "placeholder": "​",
      "style": "IPY_MODEL_760dfceff5894ab18c653725dccaf2e0",
      "value": "1_Pooling/config.json: 100%"
     }
    },
    "bfcb4345efe543aba62acbb07e6a3f7d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c081af5952044ee29799019f65e91678": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c171b3d8dabe4347900df968e37d2fd8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a4348c77df0e421d81291ef10f1397d2",
      "max": 482,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_62bc8182837c4825b1ac3c7ea326cf88",
      "value": 482
     }
    },
    "c1f3804d62584cc182cd13f90c299f0f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_24944ca627c746188d109df655eaa4de",
      "placeholder": "​",
      "style": "IPY_MODEL_9b6461ed30224cf3a18ad36dae0c7023",
      "value": " 90.9M/90.9M [00:00&lt;00:00, 251MB/s]"
     }
    },
    "c2268477307043779d6c2070342774ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c25257a378c04cffa2ce81a6664b87a8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_669fe02d06e64c7092110781e5f1e6f4",
      "placeholder": "​",
      "style": "IPY_MODEL_e46014f27c934682aba97fac7b431abf",
      "value": "Downloading shards: 100%"
     }
    },
    "c38696dc6db64ee28c51ed5718866099": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e76dc65a734b4fbcb02d886753ec3753",
      "max": 571,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1204cf7954ff4b638a5d1d25cba79c9e",
      "value": 571
     }
    },
    "c3ed1caeb4af4627a60201cce9420f07": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c43e50e679ef44c59477e05523b6c1e0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c4add13e56ec48dfb05f4f1a39fa4efb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c4bfae8972374bb581d5d5abcc8a832d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c5dfa6b08ba24299abac2f27bc68bdfd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c63a8af96239401e91d302b01f38b849": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c6720dd433aa48f3a3871dd8e45bd70b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c67a69bb0e0f4ac18792b71d1e1c5efd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c6acb332e87942939a4b82c89e17e5fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c63a8af96239401e91d302b01f38b849",
      "placeholder": "​",
      "style": "IPY_MODEL_ccac7e15cefa4cbe90f88577204b0697",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "c7c52a5f98654f9eafe6150ee4e894a7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c9865b9309be4d6b97faa80e91445254": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5e9a6c88452c402f9afbf550d7670add",
      "placeholder": "​",
      "style": "IPY_MODEL_26c970f739694c7fb39d7265bcc1cd42",
      "value": " 349/349 [00:00&lt;00:00, 24.7kB/s]"
     }
    },
    "cbba1669ed414410afedd9052f71e8d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cc39cd9f128f44c09efe00b47feccf80": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cc403af917244162b84481df711795f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cc7916202f89433196c32424d9c7ced7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ccac7e15cefa4cbe90f88577204b0697": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cd3adb62488d414b8b7f359786129f17": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cd7677fbbacd479f9048558a7da6e694": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cdd760a06b814bd187b40ea5ffea49f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1127b8dfd3cf46ba8219ba7227d2cd81",
      "max": 112,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c5dfa6b08ba24299abac2f27bc68bdfd",
      "value": 112
     }
    },
    "cf8eed43356247c0a8cf80abbc80ccf5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cfaad9a638e843a0b4581d2391cb437f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_aa4b86f1504449b4956fd2155335937f",
       "IPY_MODEL_6015887cf6c5459080bde5b0c83442b5",
       "IPY_MODEL_4866cc1279194c27a916d1020d090f20"
      ],
      "layout": "IPY_MODEL_793bd871a2374c2ebf658af2558c8b43"
     }
    },
    "d05ad18758a04f588084a8d0670abc06": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e2169ded590346b0856460ada5ad2d41",
       "IPY_MODEL_705eeadc71be452b9779623d08f0c0c6",
       "IPY_MODEL_516a5a39e2ca4799ac1866d67f3ab8de"
      ],
      "layout": "IPY_MODEL_4bd9def6f5ea4982984dfa177d864517"
     }
    },
    "d14529263b2b4d718fc2158b62942a2e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e1dfd20b730b4e648c931ca349124356",
      "placeholder": "​",
      "style": "IPY_MODEL_af8f7de1b7154b17885f78b5b5574b7a",
      "value": " 112/112 [00:00&lt;00:00, 1.90kB/s]"
     }
    },
    "d298a4f7377344928a65ee4f9b556d39": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d2dfa246e4b64d13920220457de1a8d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d3985b65b1ae415aa755bf19436dbd6f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d3dcb0c70bb24821b1540a648d86c1da": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d479615aabfb4aa1b0594625f0d1bf32": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d55a9624c1984862b1d3a89a9b56574f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d6558e58e4db4c04b9dc26cfad065696": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6f961b89a2944740925b283f7dce55ea",
       "IPY_MODEL_94cd614cea4a4dd1b60080a69e2c3b83",
       "IPY_MODEL_3ccdbbecfcb54492a15bc0e85f9f0d92"
      ],
      "layout": "IPY_MODEL_82b57df2e40549b495e247e0602004eb"
     }
    },
    "d72c50264df5470d92cc39c23ffbff04": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d84cdde777c742249ddd605bc4587a21": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d86c4fc49c0744f6bc4c45778b561733": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d88cc523ee5c43598ba2046e539cc8d3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_64bd611054c54bc493a936dd87a316f0",
      "placeholder": "​",
      "style": "IPY_MODEL_ef6dd12c633e4366a34a07a53de950d1",
      "value": " 612/612 [00:00&lt;00:00, 8.91kB/s]"
     }
    },
    "da726cc536d247c8865b82ad1cdf7b16": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7f5bfb837c034badb4987642a441cafc",
      "max": 25,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_80b08127f5b14a4e92dd07800924ad17",
      "value": 25
     }
    },
    "db8645b9f2d34d568699766a7b396281": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "db88d4b9f3084775a2a3f54561790630": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_edde1773123c4a6b8b4bb6d52866a143",
       "IPY_MODEL_48322e9a5bc540b4b66bd47cb608e848",
       "IPY_MODEL_d14529263b2b4d718fc2158b62942a2e"
      ],
      "layout": "IPY_MODEL_712d125baed4441b93e607312ec83cc8"
     }
    },
    "dc0a0a17eb9b40c182476575d85ff788": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dcf0d88429104c798d3048da28035521": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4d2f67dc24794cd3ad8101a98c794c4b",
      "placeholder": "​",
      "style": "IPY_MODEL_1d0c669af7e8433fa170fe61a91ae6bc",
      "value": "README.md: 100%"
     }
    },
    "de1249f3361e4dc287ec7dbc055cd876": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fe2bd04456d343a28cf913e7fb9592d1",
      "placeholder": "​",
      "style": "IPY_MODEL_883d21a8ddf4436daef905e64998a513",
      "value": " 25.0/25.0 [00:00&lt;00:00, 567B/s]"
     }
    },
    "deb04b39086d4b5fab3663bb251ac7c3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8f3a0ac9cd5e44508434e1aaa13c5ccd",
       "IPY_MODEL_9f5feea658c044ca8f725685d44c53e7",
       "IPY_MODEL_a86c0daf8f554702a794e68160114cb5"
      ],
      "layout": "IPY_MODEL_ad6bfab680804b329345964c6f8bd8ba"
     }
    },
    "df918b4f89ca496d9f2c5b22c6e096e7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e010fafde1f24938beb913566bcdcaae": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e0756b4f857941699a84ccadd8ee5950": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e07ee354b4834417b0fdbb2a896e2e44": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e092acfe72b84945bafcfa53cdfe69b1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e0ebbf9c653a4b38a5983ff26b3cb122": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6233e142295b4387ab8438288a94fa08",
      "placeholder": "​",
      "style": "IPY_MODEL_023aafd15019487f92276d8230f52e7d",
      "value": " 1.47k/1.47k [00:00&lt;00:00, 63.6kB/s]"
     }
    },
    "e1dfd20b730b4e648c931ca349124356": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e1ef57d034024c739b00365e46eb3dfb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e2169ded590346b0856460ada5ad2d41": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_43a3872361cc497f8163cfb661265f73",
      "placeholder": "​",
      "style": "IPY_MODEL_4dbc82dae5c948aab3ec38d28b67d53e",
      "value": "config.json: 100%"
     }
    },
    "e269f2ae0ddd4265bfe01ae0b9c1c9ff": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e29e81fb1be4460d8d277eb074a26a3c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e2b409dff53441a6909f27c964f8b4bf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e37f776845ca483eb9bf94e71abdc825": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e46014f27c934682aba97fac7b431abf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e503346ab4a347a39ba17ccb401bbebe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e52558d760244c5ebb731b4af7878e45": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e611a30d9fac4b10ae4e39c49c233512": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e6595872d82046229b0dbe70a9fd199e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9d578c44f7e341c28c5125f8bdecf3b0",
      "placeholder": "​",
      "style": "IPY_MODEL_6bb7bb647ade4cf28ae0dc9bd873dc32",
      "value": " 2/2 [01:07&lt;00:00, 31.47s/it]"
     }
    },
    "e66dae1012b54a4a98f223c0560b695f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_044c1758656841c086ab105e5c2977d0",
      "placeholder": "​",
      "style": "IPY_MODEL_8e23598a3334441a82d0f7c4b3eff0f1",
      "value": "modules.json: 100%"
     }
    },
    "e6ecb61772d641ac9d0c20ddacd4a7d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_75e56dce7ea34cfca43bb8aa44784900",
      "placeholder": "​",
      "style": "IPY_MODEL_a16af98067c14bfd8cca0058008806f9",
      "value": "tokenizer.json: 100%"
     }
    },
    "e6ee5c03dc0a4275a1e0f1f96ffb1904": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e76abefba7a144e3973ab74007867eee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2f4505cdde3c4301bc814c1cc4534b12",
      "placeholder": "​",
      "style": "IPY_MODEL_7696d9f0266e4c10aba176e7e431681d",
      "value": " 466k/466k [00:00&lt;00:00, 2.38MB/s]"
     }
    },
    "e76dc65a734b4fbcb02d886753ec3753": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e9610898c8f34e0898c0c4f33d76e666": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e97ab7e898984a4f85102646a59b5c11": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ea3136dd585b4cd9888e191b7c1bd895": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_48ee33c2012c42e8aaea3a558e808d8a",
      "placeholder": "​",
      "style": "IPY_MODEL_7eb36154990b4732b9897391cc156f96",
      "value": "tokenizer.json: 100%"
     }
    },
    "eaa6409e563c436386c0257444bc2a5c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eaaa36fed39f4127a983fd6a0567babe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eab1a1b83de14827ae242fee5ff91540": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "eab95a3f9b5e49e0a05e36211cca9693": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eb23096a775e46bd8511af85da94bef2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "eb8a4272c93d419abe5ae95b02e2f0b4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_435e6838c65b4957953fc0ee88a9b882",
       "IPY_MODEL_644b24f94f85412b8feef0f625ef8fa5",
       "IPY_MODEL_0af94016632446c1a86e75f4f51a7f0f"
      ],
      "layout": "IPY_MODEL_e37f776845ca483eb9bf94e71abdc825"
     }
    },
    "ed01f31687df4240bb4c5ab0f122d827": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_93d0aeb39303473391d4b3ececefaa99",
      "placeholder": "​",
      "style": "IPY_MODEL_90f39fbec5774c24bca6094d96ab26ff",
      "value": " 190/190 [00:00&lt;00:00, 12.8kB/s]"
     }
    },
    "ed4fe0f8fd6145c7b0b8e1e6e7cd6206": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b4e962948e2e4c0b903529ccd579b9d5",
      "placeholder": "​",
      "style": "IPY_MODEL_ff08870e519f43c8865b284ed065dd97",
      "value": "model.safetensors: 100%"
     }
    },
    "edde1773123c4a6b8b4bb6d52866a143": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dc0a0a17eb9b40c182476575d85ff788",
      "placeholder": "​",
      "style": "IPY_MODEL_48fd6a5a737d4832bc5349cab80abff6",
      "value": "special_tokens_map.json: 100%"
     }
    },
    "ee175110fea7417f855dadef69e1a64e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "eeb7d672f16f452e98b8b9cf87100950": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "eebde3ca3de740758b01bf5d929279d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fc165979f4c94c288f6529a1c092b58a",
       "IPY_MODEL_ae0cd7cd24af4b1baec8dff607c082d1",
       "IPY_MODEL_7c6c6f53d0c84d3ebf5ab6a3622becab"
      ],
      "layout": "IPY_MODEL_f71582a8837b4758bbf167e96edd02d6"
     }
    },
    "eed117f4dc2041adae06a13c8ed27776": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_50434e7d42f04ca4a85ca72aeb632d9d",
       "IPY_MODEL_3c9418cc42f44ea99ed22daeae979796",
       "IPY_MODEL_aed3137980ba4d19937bc696ef6bc8b1"
      ],
      "layout": "IPY_MODEL_9e63843a93f14446a02026c1d29167a4"
     }
    },
    "ef2fcee2a6354921bced437f08abae5f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ef6dd12c633e4366a34a07a53de950d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f039047818de46e98962f3d9fb18611b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f1ba919e78e84da4b69a4d510126ff6b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f1ec39a6c3d742d1b36735d3303ef7ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_46ebcdd0ed9d48c08952e6d137881455",
      "max": 190,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0e108f1f1fe242748e5e99342436765a",
      "value": 190
     }
    },
    "f2257b285f4d4f4b83b602d7cc6eaaf8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c7c52a5f98654f9eafe6150ee4e894a7",
      "max": 350,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9145d0e2f8f34806b847a5d22066a8dd",
      "value": 350
     }
    },
    "f2383551e8b74982aae2eca73542f635": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f29e5a964cb34c9b8c137f66eb19bbb1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b83e0e5894694527915b0877105d68d5",
      "max": 349,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ef2fcee2a6354921bced437f08abae5f",
      "value": 349
     }
    },
    "f2c85271acf8462ebec8ce09fd057b2f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f2d526cf04284a79862b3b454b0dc216": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f2f75194a2ce4c78aaf09a2c6750ab93": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f41dc475cde447efb6af54340a0f0fd7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f4493d99c0174cd2a0320dca66ccc921": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f4f2719436674ce88d287701ebf9516c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9fc83b6769b0489a83ac6a5853cfe7a4",
      "placeholder": "​",
      "style": "IPY_MODEL_8c4dcb776cdf4426968093252bc5d631",
      "value": "config.json: 100%"
     }
    },
    "f533e63c0dde4067899e4aeddda25c4c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f5f62b1b55fc4b8a9100015de19f22cc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7592f515d48346b380c03d1ebe979350",
      "placeholder": "​",
      "style": "IPY_MODEL_247f3b37a349499fb88185e01f7ce359",
      "value": " 232k/232k [00:00&lt;00:00, 3.26MB/s]"
     }
    },
    "f67abb31961845d6ba6422aabd790ca3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f85faf7c29dc4c26bf2c4fb0663eb1ff",
      "placeholder": "​",
      "style": "IPY_MODEL_3481cf26144d495bb16512fb64f52abe",
      "value": "model.safetensors: 100%"
     }
    },
    "f6c77a3adb4e4093802838a15a583a9d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_37dbcc92e5a54e4f80b3b170296d8bb8",
      "placeholder": "​",
      "style": "IPY_MODEL_bfcb4345efe543aba62acbb07e6a3f7d",
      "value": "tokenizer.json: 100%"
     }
    },
    "f6e7c316f2b746fda925c279be258bfe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f71582a8837b4758bbf167e96edd02d6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f71d0b1a2e53451696fe1abae9cd4bb6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e6ee5c03dc0a4275a1e0f1f96ffb1904",
      "placeholder": "​",
      "style": "IPY_MODEL_2f63831c1dee463f8e5f56fa516d48a8",
      "value": "special_tokens_map.json: 100%"
     }
    },
    "f767773861bd4a30ad2c11c33f4c079e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_149d6095989145b5bd1eeebf47d65bb8",
       "IPY_MODEL_2075d9e638e746ee9d02066ba50e2a2b",
       "IPY_MODEL_2e26125a7aad4731828c3d05dae1db22"
      ],
      "layout": "IPY_MODEL_335a3a8fed8740109e0c45867a2ca436"
     }
    },
    "f82d9ee7d5c041cb800769838a501c24": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f85faf7c29dc4c26bf2c4fb0663eb1ff": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f8a9268a04034142b3a3ace39c75753f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f9866af4b7c9478e8a8afa2336d8c8a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9f658dda4e71401db3ae921d8b978c4c",
       "IPY_MODEL_a2eea0216f3a421c974cef798f7ffc4d",
       "IPY_MODEL_7dd4e6bed42b4fbe919fc565203b63a4"
      ],
      "layout": "IPY_MODEL_d3985b65b1ae415aa755bf19436dbd6f"
     }
    },
    "faefbaa58a5449fb8d308340131d4ec7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f6e7c316f2b746fda925c279be258bfe",
      "max": 25125,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_145e84288178448dbb4fd3e4b0e45172",
      "value": 25125
     }
    },
    "fb2562fed9f34104a023df39610ece1c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "fb95312965944faf8e57f7a0b700145d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fc165979f4c94c288f6529a1c092b58a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6688ecfa60ac43ba8ed96530e18d231f",
      "placeholder": "​",
      "style": "IPY_MODEL_c6720dd433aa48f3a3871dd8e45bd70b",
      "value": "README.md: 100%"
     }
    },
    "fdfcea700a644a8d9deb6cc679eaaf4d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fe2bd04456d343a28cf913e7fb9592d1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fe48ac8a18bd4042ab10a4ce2a19d75d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fe6118f3635a4a1dac2ced42046a7318": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ff08870e519f43c8865b284ed065dd97": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ff77c48eacd74e7cbde24e77faf4112c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ff86ec4b2122413b96e913eb618abae2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
